* 推荐阅读：<br>
  * [ICCV2021/2019/2017 论文/代码/解读/直播合集](https://github.com/extreme-assistant/ICCV2021-Paper-Code-Interpretation)
  * [2020-2021年计算机视觉综述论文汇总](https://github.com/extreme-assistant/survey-computer-vision)
  * [国内外优秀的计算机视觉团队汇总](https://github.com/extreme-assistant/Awesome-CV-Team)
------

# CVPR2021最新信息及论文下载（Papers/Codes/Project/PaperReading／Demos/直播分享／论文分享会等）

官网链接：http://cvpr2021.thecvf.com<br>
时间：2021年6月19日-6月25日<br>
论文接收公布时间：2021年2月28日<br>

相关问题：<br>

* [如何评价CVPR 2021的论文接收结果？](https://www.zhihu.com/question/446299297)<br>
* [CVPR 2021接收结果出炉！录用1663篇，接受率提升，你的论文中了吗？（附论文下载）](https://mp.weixin.qq.com/s/4UQ2W1V-eLnL02L8BDOtMg)

<br><br>

# 目录

[1. CVPR2021接受论文/代码分方向汇总（更新中）](#1)<br>
[2. CVPR2021 Oral（更新中）](#2)<br>
[3. CVPR2021论文解读汇总（更新中）](#3)<br>
[4. CVPR2021 Workshop](#4)<br>
[5. To do list](#4)<br>


<br>

<a name="1"/> 

# 1.CVPR2021接受论文/代码分方向整理(持续更新)


## 分类目录：

### [1. 检测](#detection)
* [2D目标检测(2D Object Detection)](#IOD)
  * [一文看尽CVPR2021 2D 目标检测论文（27篇）](https://bbs.cvmart.net/articles/5120)
* [视频目标检测(Video Object Detection)](#VOD)
* [3D目标检测(3D Object Detection)](#3DOD)
* [人物交互检测(HOI Detection)](#HOI)
* [伪装目标检测(Camouflaged Object Detection)](#COD)
* [旋转目标检测(Rotation Object Detection)](#ROD)
* [显著性目标检测(Saliency Object Detection)](#SOD)
* [图像异常检测(Anomally Detection in Image)](#ADI)
  * [一文看尽CVPR2021 图像异常检测论文（6篇）](https://bbs.cvmart.net/articles/5127)
* [关键点检测(Keypoint Detection)](#KeypointDetection)

### [2. 分割(Segmentation)](#Segmentation)
* [图像分割(Image Segmentation)](#ImageSegmentation)
* [全景分割(Panoptic Segmentation)](#PanopticSegmentation)
* [语义分割(Semantic Segmentation)](#SemanticSegmentation)
* [实例分割(Instance Segmentation)](#InstanceSegmentation)
* [超像素(Superpixel)](#Superpixel)
* [视频目标分割(Video Object Segmentation)](#VOS)
* [抠图(Matting)](#Matting)
* [密集预测(Dense Prediction)](#DensePrediction)

### [3. 图像处理(Image Processing)](#ImageProcessing)

* [超分辨率(Super Resolution)](#SuperResolution)
* [图像复原/图像增强(Image Restoration)](#ImageRestoration)
* [图像去阴影/去反射(Image Shadow Removal/Image Reflection Removal)](#ISR)
* [图像去噪/去模糊/去雨去雾(Image Denoising)](#ImageDenoising)
* [图像编辑/修复(Image Edit/Image Inpainting)](#ImageEdit)
* [图像翻译(Image Translation)](#ImageTranslation)
* [图像质量评估(Image Quality Assessment)](#IQA)
* [风格迁移(Style Transfer)](#StyleTransfer)

### [4. 估计(Estimation)](#Estimation)
* [姿态估计(Pose Estimation)](#HumanPoseEstimation)
* [手势估计(Gesture Estimation)](#GestureEstimation)
* [光流/位姿/运动估计(Flow/Pose/Motion Estimation)](#Flow/Pose/MotionEstimation)
* [深度估计(Depth Estimation)](#DepthEstimation)

### [5. 图像&视频检索/理解(Image&Video Retrieval/Video Understanding)](#ImageRetrieval)
* [行为识别/行为识别/动作识别/检测/分割(Action/Activity Recognition)](#ActionRecognition)
* [行人重识别/检测(Re-Identification/Detection)](#Re-Identification)
* [图像/视频字幕(Image/Video Caption)](#VideoCaption)

### [6. 人脸(Face)](#Face)
* [人脸识别/检测(Facial Recognition/Detection)](#FacialRecognition)
* [人脸生成/合成/重建/编辑(Face Generation/Face Synthesis/Face Reconstruction/Face Editing)](#FaceSynthesis)
* [人脸伪造/反欺骗(Face Forgery/Face Anti-Spoofing)](#FaceAnti-Spoofing)

### [7. 三维视觉(3D Vision)](#3DVision)
* [点云(Point Cloud)](#3DPC)
* [三维重建(3D Reconstruction)](#3DReconstruction)

### [8. 目标跟踪(Object Tracking)](#ObjectTracking)

### [9. 医学影像(Medical Imaging)](#MedicalImaging)

### [10. 文本检测/识别(Text Detection/Recognition)](#TDR)

### [11. 遥感图像(Remote Sensing Image)](#RSI)

### [12. GAN/生成式/对抗式(GAN/Generative/Adversarial)](#GAN)

### [13. 图像生成/合成(Image Generation/Image Synthesis)](#IGIS)
* [视图合成(View Synthesis)](#ViewSynthesis)

### [14. 场景图(Scene Graph](#SG)
* [场景图生成(Scene Graph Generation)](#SGG)
* [场景图预测(Scene Graph Prediction)](#SGP)
* [场景图理解(Scene Graph Understanding)](#SGU)

### [15. 视觉定位(Visual Localization)](#VisualLocalization)

### [16. 视觉推理/视觉问答(Visual Reasoning/VQA)](#VisualReasoning)

### [17. 图像分类(Image Classification)](#ImageClassification)

### [18. 神经网络结构设计(Neural Network Structure Design)](#NNS)
* [Transformer](#Transformer)
* [图神经网络(GNN)](#GNN)
* [神经网络架构搜索(NAS)](#NAS)

### [19. 模型压缩(Model Compression)](#ModelCompression)
* [知识蒸馏(Knowledge Distillation)](#KnowledgeDistillation)
* [剪枝(Pruning)](#Pruning)
* [量化(Quantization)](#Quantization)

### [20. 模型训练/泛化(Model Training/Generalization)](#ModelTraining)
* [噪声标签(Noisy Label)](#NoisyLabel)
* [长尾分布(Long-Tailed Distribution)](#Long-Tailed)

### [21. 模型评估(Model Evaluation)](#ModelEvaluation)

### [22. 数据处理(Data Processing)](#DataProcessing)
* [数据增广(Data Augmentation)](#DataAugmentation)
* [表征学习(Representation Learning)](#RepresentationLearning)
* [归一化/正则化(Batch Normalization)](#BatchNormalization)
* [图像聚类(Image Clustering)](#ImageClustering)
* [图像压缩(Image Compression)](#ImageCompression)
* [异常检测(Anomaly Detection)](#AnomalyDetection)

### [23. 主动学习(Active Learning)](#ActiveLearning)

### [24. 小样本学习/零样本学习(Few-shot/Zero-shot Learning)](#Few-shotLearning)

### [25. 持续学习(Continual Learning/Life-long Learning)](#ContinualLearning)

### [26. 迁移学习/domain/自适应(Transfer Learning/Domain Adaptation)](#domain)

### [27. 度量学习(Metric Learning)](#MetricLearning)

### [28. 对比学习(Contrastive Learning)](#ContrastiveLearning)

### [29. 增量学习(Incremental Learning)](#IncrementalLearning)

### [30. 强化学习(Reinforcement Learning)](#RL)

### [31. 元学习(Meta Learning)](#MetaLearning)

### [32. 多模态学习(Multi-Modal Learning)](#MMLearning)
* [视听学习(Audio-visual Learning)](#Audio-VisualLearning)

### [33. 视觉预测(Vision-based Prediction)](#Vision-basedPrediction)

### [34. 数据集(Dataset)](#Dataset)

### [暂无分类](#100)



<br><br>

<a name="detection"/> 

## 检测


<a name="IOD"/> 

### 图像目标检测(2D Object Detection)

[28] Open-Vocabulary Object Detection Using Captions(使用字幕的开放词汇对象检测)<br>
[paper](https://arxiv.org/abs/2011.10678)<br><br>

[27] Improved Handling of Motion Blur in Online Object Detection(改进在线对象检测中运动模糊的处理)<br>
[paper](https://arxiv.org/abs/2011.14448)<br><br>

[26] PSRR-MaxpoolNMS: Pyramid Shifted MaxpoolNMS with Relationship Recovery(PSRR MaxpoolNMS：具有关系恢复的金字塔移位MaxpoolNMS)<br>
[paper](https://arxiv.org/abs/2105.12990)<br><br>

[25] Domain-Specific Suppression for Adaptive Object Detection(领域特定的自适应对象检测抑制)<br>
[paper](https://arxiv.org/abs/2105.03570)<br><br>

[24] Line Segment Detection Using Transformers without Edges(【线段检测】使用没有边缘的Transformer进行线段检测)<br>
[paper](https://arxiv.org/abs/2101.01909)<br><br>

[23] IQDet: Instance-wise Quality Distribution Sampling for Object Detection(IQDet：用于对象检测的按实例进行质量分布采样)<br>
[paper](https://arxiv.org/abs/2104.06936)<br><br>

[22] Adaptive Class Suppression Loss for Long-Tail Object Detection(长尾目标检测的自适应类抑制损失)<br>
[paper](https://arxiv.org/abs/2104.00885) | [code](https://github.com/CASIA-IVA-Lab/ACSL)<br><br>

[21] DAP: Detection-Aware Pre-training with Weak Supervision(具有弱监督的可感知检测的预训练)<br>
[paper](https://arxiv.org/abs/2103.16651)<br><br>

[20] Dense Relation Distillation with Context-aware Aggregation for Few-Shot Object Detection(稠密关系蒸馏与上下文感知聚合用于小样本对象检测)<br>
[paper](https://arxiv.org/abs/2103.17115) ｜ [code](https://github.com/hzhupku/DCNet)<br><br>

[19] Scale-aware Automatic Augmentation for Object Detection(用于物体检测的可感知规模的自动增强)<br>
[paper](https://arxiv.org/abs/2103.17220) | [code](https://github.com/Jia-Research-Lab/SA-AutoAug)<br><br>

[18] Data-Uncertainty Guided Multi-Phase Learning for Semi-Supervised Object Detection(数据不确定性指导的多阶段学习，用于半监督对象检测)<br>
[paper](https://arxiv.org/abs/2103.16368)<br><br>

[17] OTA: Optimal Transport Assignment for Object Detection(OTA：用于对象检测的最佳传输分配)<br>
[paper](https://arxiv.org/abs/2103.14259) | [code](https://github.com/Megvii-BaseDetection/OTA)<br><br>

[16] Distilling Object Detectors via Decoupled Features(通过解耦功能蒸馏物体检测器)<br>
[paper](https://arxiv.org/abs/2103.14475) | [code](https://github.com/ggjy/DeFeat.pytorch)<br><br>

[15] I^3Net: Implicit Instance-Invariant Network for Adapting One-Stage Object Detectors(I ^ 3Net：用于适应一阶段对象检测器的隐式实例不变网络)<br>
[paper](https://arxiv.org/abs/2103.13757)<br><br>

[14] Robust and Accurate Object Detection via Adversarial Learning(通过对抗学习进行稳健而准确的目标检测)<br>
[paper](https://arxiv.org/abs/2103.13886) | [model](https://github.com/google/automl/tree/master/efficientdet/Det-AdvProp.md)<br><br>

[13] You Only Look One-level Feature<br>
[paper](https://arxiv.org/pdf/2103.09460.pdf) | [code](https://github.com/megvii-model/YOLOF)<br>
解读：[我扔掉FPN来做目标检测，效果竟然这么强！YOLOF开源：你只需要看一层特征](https://mp.weixin.qq.com/s/GMHx3oMZr0qlP9Iy8tWGMw)

[12] End-to-End Object Detection with Fully Convolutional Network(使用全卷积网络进行端到端目标检测)<br>
[paper](https://arxiv.org/abs/2012.03544) | [code](https://github.com/Megvii-BaseDetection/DeFCN)<br>
解读：[丢弃Transformer，FCN也可以实现E2E检测](https://zhuanlan.zhihu.com/p/332281368)<br><br>

[11] FSCE: Few-Shot Object Detection via Contrastive Proposal Encoding(通过对比提案编码进行的小样本目标检测)<br>
[paper](https://arxiv.org/abs/2103.05950v2) ｜ [code](https://github.com/MegviiDetection/FSCE)<br><br>

[10] Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection(学习可靠的定位质量估计用于密集目标检测)<br>
[paper](https://arxiv.org/pdf/2011.12885.pdf) | [code](https://github.com/implus/GFocalV2) <br>
解读:[大白话 Generalized Focal Loss V2](https://zhuanlan.zhihu.com/p/313684358)<br><br>

[9] MeGA-CDA: Memory Guided Attention for Category-Aware Unsupervised Domain Adaptive Object Detection(用于类别识别无监督域自适应对象检测)<br>
[paper](https://arxiv.org/pdf/2103.04224.pdf)<br><br>

[8] OPANAS: One-Shot Path Aggregation Network Architecture Search for Object Detection(一键式路径聚合网络体系结构搜索对象)<br>
[paper](https://arxiv.org/abs/2103.04507) | [code](https://github.com/VDIGPKU/OPANAS)<br><br>

[7] Semantic Relation Reasoning for Shot-Stable Few-Shot Object Detection(小样本目标检测的语义关系推理)<br>
[paper](https://arxiv.org/abs/2103.01903)<br><br>

[6] General Instance Distillation for Object Detection(通用实例蒸馏技术在目标检测中的应用)<br>
[paper](https://arxiv.org/abs/2103.02340)<br><br>

[5] Instance Localization for Self-supervised Detection Pretraining(自监督检测预训练的实例定位)<br>
[paper](https://arxiv.org/pdf/2102.08318.pdf)｜[code](https://github.com/limbo0000/InstanceLoc)<br><br>

[4] Multiple Instance Active Learning for Object Detection（用于对象检测的多实例主动学习）<br>
[paper](https://github.com/yuantn/MIAL/raw/master/paper.pdf) | [code](https://github.com/yuantn/MIAL)<br>
解读:[MI-AOD: 少量样本实现高检测性能](https://zhuanlan.zhihu.com/p/362764637)<br>

[3] Towards Open World Object Detection(开放世界中的目标检测)<br>
[paper](https://arxiv.org/abs/2103.02603) | [code](https://github.com/JosephKJ/OWOD)<br>
解读:[目标检测一卷到底之后，终于有人为它挖了个新坑｜CVPR2021 Oral](https://mp.weixin.qq.com/s/1_WC_hTTc9fgSJUqBsfTYQ)<br><br>

[2] Positive-Unlabeled Data Purification in the Wild for Object Detection(野外检测对象的阳性无标签数据提纯)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_Positive-Unlabeled_Data_Purification_in_the_Wild_for_Object_Detection_CVPR_2021_paper.pdf)<br><br>

[1] UP-DETR: Unsupervised Pre-training for Object Detection with Transformers<br>
[paper](https://arxiv.org/pdf/2011.09094.pdf) | [code](https://github.com/dddzg/up-detr)<br>
解读：[无监督预训练检测器](https://www.zhihu.com/question/432321109/answer/1606004872)<br><br>


<a name="VOD"/> 

### 视频目标检测(Video Object Detection)

[3] Dogfight: Detecting Drones from Drones Videos(从无人机视频中检测无人机)<br>
[paper](https://arxiv.org/abs/2103.17242)<br><br>

[2] Depth from Camera Motion and Object Detection(相机运动和物体检测的深度)<br>
[paper](https://arxiv.org/abs/2103.01468)<br><br>

[1] There is More than Meets the Eye: Self-Supervised Multi-Object Detection  and Tracking with Sound by Distilling Multimodal Knowledge(多模态知识提取的自监督多目标检测与有声跟踪)<br>
[paper](https://arxiv.org/abs/2103.01353) | [video](https://www.youtube.com/channel/UCRpM8k1GY3kD2TqCo_yKN3g) | [project](http://rl.uni-freiburg.de/research/multimodal-distill)<br><br>


<a name="3DOD"/> 

### 三维目标检测(3D object detection)

[15] Exploring intermediate representation for monocular vehicle pose estimation(探索单目车辆姿态估计的中间表示)<br>
[paper](https://arxiv.org/abs/2011.08464) ｜ [code](https://github.com/Nicholasli1995/EgoNet)<br><br>

[14] SE-SSD: Self-Ensembling Single-Stage Object Detector From Point Cloud(SE-SSD：来自点云的自集成单级目标检测器)<br>
[paper](https://arxiv.org/abs/2104.09804) | [code](https://github.com/Vegeta2020/SE-SSD)<br><br>

[13] Back-tracing Representative Points for Voting-based 3D Object Detection in Point Clouds(点云中基于投票的3D对象检测的回溯代表点)<br>
[paper](https://arxiv.org/abs/2104.06114) | [code](https://github.com/cheng052/BRNet)<br><br>

[12] Objects are Different: Flexible Monocular 3D Object Detection(对象不同：灵活的单眼3D对象检测)<br>
[paper](https://arxiv.org/abs/2104.02323) | [code](https://github.com/zhangyp15/MonoFlex)<br><br>

[11] HVPR: Hybrid Voxel-Point Representation for Single-stage 3D Object Detection(HVPR：用于单阶段3D对象检测的混合体素点表示)<br>
[paper](https://arxiv.org/abs/2104.00902)<br><br>

[10] GrooMeD-NMS: Grouped Mathematically Differentiable NMS for Monocular 3D Object Detection(用于单眼3D对象检测的数学可微分的分组NMS)<br>
[paper](https://arxiv.org/abs/2103.17202) | [code](https://github.com/abhi1kumar/groomed_nms)<br><br>

[9] Delving into Localization Errors for Monocular 3D Object Detection(深入研究单目3D对象检测的定位错误)<br>
[paper](https://arxiv.org/abs/2103.16237) | [code](https://github.com/xinzhuma/monodle)<br><br>

[8] Depth-conditioned Dynamic Message Propagation for Monocular 3D Object Detection(用于单眼3D对象检测的深度条件动态消息传播)<br>
[paper](https://arxiv.org/abs/2103.16470) | [code](https://github.com/fudan-zvg/DDMP)<br><br>

[7] LiDAR R-CNN: An Efficient and Universal 3D Object Detector(高效且通用的3D对象检测器)<br>
[paper](https://arxiv.org/abs/2103.15297) | [code](https://github.com/tusimple/LiDAR_RCNN)<br><br>

[6] M3DSSD: Monocular 3D Single Stage Object Detector(单眼3D单级目标检测器)<br>
[paper](https://arxiv.org/abs/2103.13164)<br><br>

[5] MonoRUn: Monocular 3D Object Detection by Self-Supervised Reconstruction and Uncertainty Propagation(通过自我监督的重构和不确定性传播进行单眼3D目标检测)<br>
[paper](https://arxiv.org/abs/2103.12605)<br><br>

[4] ST3D: Self-training for Unsupervised Domain Adaptation on 3D Object Detection(ST3D：在三维目标检测上进行无监督域自适应的自训练)<br>
[paper](https://arxiv.org/pdf/2103.05346.pdf) | [code](https://github.com/CVMI-Lab/ST3D)<br><br>

[3] Center-based 3D Object Detection and Tracking(基于中心的3D目标检测和跟踪)<br>
[paper](https://arxiv.org/abs/2006.11275) | [code](https://github.com/tianweiy/CenterPoint)<br><br>

[2] 3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection(利用IoU预测进行半监督3D对象检测)<br>
[paper](https://arxiv.org/pdf/2012.04355.pdf) | [code](https://github.com/THU17cyz/3DIoUMatch) | [project](https://thu17cyz.github.io/3DIoUMatch/) | [video](https://youtu.be/nuARjhkQN2U)<br><br>

[1] Categorical Depth Distribution Network for Monocular 3D Object Detection(用于单目三维目标检测的分类深度分布网络)<br>
[paper](https://arxiv.org/abs/2103.01100)<br><br>

<a name="HOI"/> 

### 人物交互检测(HOI Detection)

[7] HOTR: End-to-End Human-Object Interaction Detection with Transformers(HOTR：使用变压器进行端到端的人与对象交互检测)<br>
[paper](https://arxiv.org/abs/2104.13682)<br><br>

[6] Glance and Gaze: Inferring Action-aware Points for One-Stage Human-Object Interaction Detection(凝视与凝视：推断行动感知点，用于一阶段的人物交互检测)<br>
[paper](https://arxiv.org/abs/2104.05269)<br><br>

[5] Affordance Transfer Learning for Human-Object Interaction Detection(物价转移学习用于人物交互检测)<br>
[paper](https://arxiv.org/abs/2104.02867) | [code](https://github.com/zhihou7/HOI-CL)<br><br>

[4] Detecting Human-Object Interaction via Fabricated Compositional Learning(通过人为构图学习检测人与物体的相互作用)<br>
[paper](https://arxiv.org/abs/2103.08214) | [code](https://github.com/zhihou7/FCL)<br><br>

[3] Reformulating HOI Detection as Adaptive Set Prediction(将人物交互检测重新配置为自适应集预测)<br>
[paper](https://arxiv.org/abs/2103.05983) | [code](https://arxiv.org/abs/2103.05983)<br><br>

[2] QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information(具有图像范围的上下文信息的基于查询的成对人物交互检测)<br>
[paper](https://arxiv.org/abs/2103.05399) | [code](https://github.com/hitachi-rd-cv/qpic)<br><br>

[1] End-to-End Human Object Interaction Detection with HOI Transformer(使用HOI Transformer进行端到端的人类对象交互检测)<br>
[paper](https://arxiv.org/pdf/2103.04503.pdf) | [code](https://github.com/bbepoch/HoiTransformer)<br><br>

<a name="COD"/> 

### 伪装目标检测(Camouflaged Object Detection)

[4] Camouflaged Object Segmentation with Distraction Mining(带有干扰挖掘的伪装对象分割)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Mei_Camouflaged_Object_Segmentation_With_Distraction_Mining_CVPR_2021_paper.pdf) | [code](https://mhaiyang.github.io/CVPR2021_PFNet/index)<br><br>

[3] Mutual Graph Learning for Camouflaged Object Detection(用于伪装目标检测的互图学习)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhai_Mutual_Graph_Learning_for_Camouflaged_Object_Detection_CVPR_2021_paper.pdf)<br><br>

[2] Uncertainty-aware Joint Salient Object and Camouflaged Object Detection(不确定度联合显着物体和伪装物体检测)<br>
[paper](https://arxiv.org/abs/2104.02628)<br><br>

[1] Simultaneously Localize, Segment and Rank the Camouflaged Objects(同时定位，分割和排序伪装的对象)<br>
[paper](https://arxiv.org/abs/2103.04011) | [code](https://github.com/JingZhang617/COD-Rank-Localize-and-Segment)<br><br>


<a name="ROD"/> 

### 旋转目标检测(Rotation Object Detection)

[2] ReDet: A Rotation-equivariant Detector for Aerial Object Detection(ReDet：用于航空物体检测的等速旋转检测器)<br>
[paper](https://arxiv.org/abs/2103.07733) | [code](https://github.com/csuhan/ReDet)<br><br>

[1] Dense Label Encoding for Boundary Discontinuity Free Rotation Detection(密集标签编码，用于边界不连续自由旋转检测)<br>
[paper](https://arxiv.org/abs/2011.09670) | [code](https://github.com/yangxue0827/RotationDetection) | [解读-DCL：旋转目标检测新方法](https://zhuanlan.zhihu.com/p/354373013)<br><br>

<a name="SOD"/> 

### 显著性检测(Saliency Object Detection)

[4] Calibrated RGB-D Salient Object Detection(校准的 RGB-D 显著物体检测)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Calibrated_RGB-D_Salient_Object_Detection_CVPR_2021_paper.pdf) | [code](https://github.com/jiwei0921/DCF)<br><br>

[3] Weakly Supervised Video Salient Object Detection(弱监督视频显著性目标检测)<br>
[paper](https://arxiv.org/abs/2104.02391)<br><br>

[2] Group Collaborative Learning for Co-Salient Object Detection(协同显著性目标检测的小组协作学习)<br>
[paper](https://arxiv.org/abs/2104.01108) | [project](https://github.com/fanq15/GCoNet)<br><br>

[1] Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion(具有深度敏感注意力和自动多模态融合的深度RGB-D显著性检测)<br>
[paper](https://arxiv.org/abs/2103.11832)<br><br>

<a name="ADI"/> 

### 图像异常检测(Anomally Detection in Image)

[7] Anomaly Detection in Video via Self-Supervised and Multi-Task Learning(通过自我监督和多任务学习进行视频异常检测)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Georgescu_Anomaly_Detection_in_Video_via_Self-Supervised_and_Multi-Task_Learning_CVPR_2021_paper.pdf)<br><br>

[6] MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection(用于视频异常检测的多实例自训练框架)<br>
[paper](https://arxiv.org/abs/2104.01633)<br><br>

[5] CutPaste: Self-Supervised Learning for Anomaly Detection and Localization(CutPaste：用于异常检测和定位的自我监督学习)<br>
[paper](https://arxiv.org/abs/2104.04015)<br><br>

[4] CutPaste: Self-Supervised Learning for Anomaly Detection and Localization(CutPaste：用于异常检测和定位的自监督学习)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_CutPaste_Self-Supervised_Learning_for_Anomaly_Detection_and_Localization_CVPR_2021_paper.pdf)<br><br>

[3] Pixel-wise Anomaly Detection in Complex Driving Scenes(复杂驾驶场景中的逐像素异常检测)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Di_Biase_Pixel-Wise_Anomaly_Detection_in_Complex_Driving_Scenes_CVPR_2021_paper.pdf)<br><br>

[2] PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation(PANDA：调整用于异常检测和分割的预训练特征)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Reiss_PANDA_Adapting_Pretrained_Features_for_Anomaly_Detection_and_Segmentation_CVPR_2021_paper.pdf) | [code](https://github.com/talreiss/PANDA)<br><br>

[1] Multiresolution Knowledge Distillation for Anomaly Detection(用于异常检测的多分辨率知识蒸馏)<br>
[paper](https://arxiv.org/abs/2011.11108)<br><br>

<a name="KeypointDetection"/> 

### 关键点检测(Keypoint Detection)

[1] Skeleton Merger: an Unsupervised Aligned Keypoint Detector(骨架合并：无监督的对准关键点检测器)<br>
[paper](https://arxiv.org/pdf/2103.10814.pdf) | [code](https://github.com/eliphatfs/SkeletonMerger)<br><br>


<br>
<a name="Segmentation"/> 

## 分割(Segmentation)

<a name="ImageSegmentation"/> 

## 图像分割(Image Segmentation)

[14] ATSO: Asynchronous Teacher-Student Optimization for Semi-Supervised Image Segmentation(ATSO：半监督图像分割的异步师生优化)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Huo_ATSO_Asynchronous_Teacher-Student_Optimization_for_Semi-Supervised_Image_Segmentation_CVPR_2021_paper.pdf)<br><br>

[13] Encoder Fusion Network with Co-Attention Embedding for Referring Image Segmentation(用于【基于文本的图像分割】的具有协同注意力嵌入的编码器融合网络)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Feng_Encoder_Fusion_Network_With_Co-Attention_Embedding_for_Referring_Image_Segmentation_CVPR_2021_paper.pdf)<br><br>

[12] Bottom-Up Shift and Reasoning for Referring Image Segmentation(【基于文本的图像分割】的自底向上移位和推理)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Bottom-Up_Shift_and_Reasoning_for_Referring_Image_Segmentation_CVPR_2021_paper.pdf) | [code](https://github.com/incredibleXM/BUSNet)<br><br>

[11] Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation(每种注释都至关重要：【医学图像分割】的多标签深度监管)<br>
[paper](https://arxiv.org/abs/2104.13243)<br><br>

[10] Camouflaged Object Segmentation with Distraction Mining(【伪装目标分割】基于分心挖掘的伪装目标分割)<br>
[paper](https://arxiv.org/abs/2104.10475)<br><br>

[9] Adaptive Prototype Learning and Allocation for Few-Shot Segmentation(【小样本分割】的自适应原型学习和分配)<br>
[paper](https://arxiv.org/abs/2104.01893)<br><br>

[8] DiNTS: Differentiable Neural Network Topology Search for 3D Medical Image Segmentation(DiNTS：用于3D【医学图像分割】的可区分神经网络拓扑搜索)<br>
[paper](https://arxiv.org/abs/2103.15954)<br><br>

[7] Self-Guided and Cross-Guided Learning for Few-Shot Segmentation(自我指导和交叉指导学习，用于【小样本分割】)<br>
[paper](https://arxiv.org/abs/2103.16129)<br><br>

[6] Locate then Segment: A Strong Pipeline for Referring Image Segmentation(找到然后分割：用于【基于文本的图像分割】的强大管道)<br>
[paper](https://arxiv.org/abs/2103.16284)<br><br>

[5] Boundary IoU: Improving Object-Centric Image Segmentation Evaluation(边界IoU：改进以对象为中心的图像分割评估)<br>
[paper](https://arxiv.org/abs/2103.16562) | [code](https://bowenc0221.github.io/boundary-iou)<br><br>

[4] PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation(语义流经点以进行【航空图像分割】)<br>
[paper](https://arxiv.org/pdf/2103.06564.pdf)<br><br>

[3] FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space(在连续频率空间中通过情景学习进行【医学图像分割】的联合域泛化)<br>
[paper](https://arxiv.org/abs/2103.06030) | [code](https://github.com/liuquande/FedDG-ELCFS)<br><br>

[2] Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?(【小样本分割】没有元学习的小样本分割：你只需要一个好的转换推论？)<br>
[paper](https://arxiv.org/abs/2012.06166) | [code](https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation)<br><br>

[1] Learning Calibrated Medical Image Segmentation via Multi-rater Agreement Modeling(通过多评分者协议建模学习校准的【医学图像分割】)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Learning_Calibrated_Medical_Image_Segmentation_via_Multi-Rater_Agreement_Modeling_CVPR_2021_paper.pdf) ｜ [code](https://github.com/jiwei0921/MRNet/)<br><br>


<a name="PanopticSegmentation"/> 

### 全景分割(Panoptic Segmentation)

[15] Fully Convolutional Networks for Panoptic Segmentation(Oral | 用于全景分割的全卷积网络)<br>
[paper](https://arxiv.org/abs/2012.00720) | [code](https://github.com/Jia-Research-Lab/PanopticFCN)<br><br>

[14] Part-aware Panoptic Segmentation(部分感知全景分割)<br>
[paper](https://arxiv.org/abs/2106.06351) | [code](https://github.com/tue-mps/panoptic_parts)<br><br>

[13] LiDAR-based Panoptic Segmentation via Dynamic Shifting Network(通过动态移动网络基于 LiDAR 的全景分割)<br>
[paper](https://arxiv.org/abs/2011.11964) | [code](https://github.com/hongfz16/DS-Net)<br><br>

[12] Learning to Associate Every Segment for Video Panoptic Segmentation(学习关联视频全景分割的每个片段)<br>
[paper](https://arxiv.org/abs/2106.09453)<br><br>

[11] Hierarchical Lovász Embeddings for Proposal-free Panoptic Segmentation(用于无提议全景分割的分层 Lova ́sz 嵌入)<br>
[paper](https://arxiv.org/abs/2106.04555)<br><br>

[10] Exemplar-Based Open-Set Panoptic Segmentation Network(基于范例的开放集全景分割网络)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Hwang_Exemplar-Based_Open-Set_Panoptic_Segmentation_Network_CVPR_2021_paper.pdf) | [project](https://cv.snu.ac.kr/research/EOPSN)<br><br>

[9] ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation()<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Qiao_VIP-DeepLab_Learning_Visual_Perception_With_Depth-Aware_Video_Panoptic_Segmentation_CVPR_2021_paper.pdf) | [code]( https://github.com/joe-siyuan-qiao/ViP-DeepLab)<br><br>

[8] LPSNet: A lightweight solution for fast panoptic segmentation(LPSNet：快速全景分割的轻量级解决方案)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_LPSNet_A_Lightweight_Solution_for_Fast_Panoptic_Segmentation_CVPR_2021_paper.pdf)<br><br>

[7] Improving Panoptic Segmentation at All Scales(改进所有尺度的全景分割)<br>
[paper](http://arxiv.org/abs/2012.07717)<br><br>

[6] Toward Joint Thing-and-Stuff Mining for Weakly Supervised Panoptic Segmentation(面向弱监督全景分割的联合物物挖掘)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_Toward_Joint_Thing-and-Stuff_Mining_for_Weakly_Supervised_Panoptic_Segmentation_CVPR_2021_paper.pdf)<br><br>

[5] MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers(MaX-DeepLab：使用掩模转换器进行端到端全景分割)<br>
[paper](https://arxiv.org/abs/2012.00759) ｜ [code](https://github.com/google-research/deeplab2)<br><br>

[4] Panoptic Segmentation Forecasting(全景分割预测)<br>
[paper](https://arxiv.org/abs/2104.03962)<br><br>

[3] Panoptic-PolarNet: Proposal-free LiDAR Point Cloud Panoptic Segmentation(无提案的LiDAR点云全景分割)<br>
[paper](https://arxiv.org/pdf/2103.14962.pdf)<br><br>

[2] Cross-View Regularization for Domain Adaptive Panoptic Segmentation(用于域自适应全景分割的跨视图正则化)<br>
[paper](https://arxiv.org/abs/2103.02584)<br><br>

[1] 4D Panoptic LiDAR Segmentation（4D全景LiDAR分割）<br>
[paper](https://arxiv.org/abs/2102.12472)<br><br>

<a name="SemanticSegmentation"/> 

### 语义分割(Semantic Segmentation)

[49] DCNAS: Densely Connected Neural Architecture Search for Semantic Image Segmentation(DCNAS：语义图像分割的密集连接神经架构搜索)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_DCNAS_Densely_Connected_Neural_Architecture_Search_for_Semantic_Image_Segmentation_CVPR_2021_paper.pdf)<br><br>

[48] Semi-supervised Semantic Segmentation with Directional Context-aware Consistency(具有定向上下文感知一致性的半监督语义分割)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Lai_Semi-Supervised_Semantic_Segmentation_With_Directional_Context-Aware_Consistency_CVPR_2021_paper.pdf)<br><br>

[47] Scale-Aware Graph Neural Network for Few-Shot Semantic Segmentation(用于小样本语义分割的尺度感知图神经网络)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Xie_Scale-Aware_Graph_Neural_Network_for_Few-Shot_Semantic_Segmentation_CVPR_2021_paper.pdf)<br><br>

[46] Uncertainty Reduction for Model Adaptation in Semantic Segmentation(语义分割中模型自适应的不确定性降低)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/S_Uncertainty_Reduction_for_Model_Adaptation_in_Semantic_Segmentation_CVPR_2021_paper.pdf)<br><br>

[45] Rethinking BiSeNet For Real-time Semantic Segmentation(重新思考用于实时语义分割的 BiSeNet)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Fan_Rethinking_BiSeNet_for_Real-Time_Semantic_Segmentation_CVPR_2021_paper.pdf) | [code](https://github.com/ MichaelFan01/STDC-Seg)<br><br>

[44] HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation(HyperSeg：用于实时语义分割的 Patch-wise Hypernetwork)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Nirkin_HyperSeg_Patch-Wise_Hypernetwork_for_Real-Time_Semantic_Segmentation_CVPR_2021_paper.pdf) | [code](https://nirkin.com/hyperseg)<br><br>

[43] Complete & Label: A Domain Adaptation Approach to Semantic Segmentation of LiDAR Point Clouds(完整和标签：激光雷达点云语义分割的域适应方法)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Yi_Complete__Label_A_Domain_Adaptation_Approach_to_Semantic_Segmentation_CVPR_2021_paper.pdf)<br><br>

[42] Non-Salient Region Object Mining for Weakly Supervised Semantic Segmentation(弱监督语义分割的非显著区域对象挖掘)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Yao_Non-Salient_Region_Object_Mining_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2021_paper.pdf) ｜ [code](https://github.com/NUST-Machine-Intelligence-Laboratory/nsrom)<br><br>

[41] Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation(域自适应语义分割的原型伪标签去噪和目标结构学习)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Prototypical_Pseudo_Label_Denoising_and_Target_Structure_Learning_for_Domain_CVPR_2021_paper.pdf) | [code](https://github.com/microsoft/ProDA)<br><br>

[40] Few-shot 3D Point Cloud Semantic Segmentation(小样本 3D 点云语义分割)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhao_Few-Shot_3D_Point_Cloud_Semantic_Segmentation_CVPR_2021_paper.pdf)<br><br>

[39] Embedded Discriminative Attention Mechanism for Weakly Supervised Semantic Segmentation(弱监督语义分割的嵌入式判别注意机制)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Embedded_Discriminative_Attention_Mechanism_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2021_paper.pdf)<br><br>

[38] Anti-aliasing Semantic Reconstruction for Few-Shot Semantic Segmentation(针对小样本语义分割的抗锯齿语义重建)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Anti-Aliasing_Semantic_Reconstruction_for_Few-Shot_Semantic_Segmentation_CVPR_2021_paper.pdf) ｜ [code](https://github.com/Bibkiller/ASR)<br><br>

[37] Three Ways to Improve Semantic Segmentation with Self-Supervised Depth Estimation(使用自监督深度估计改进语义分割的三种方法)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Hoyer_Three_Ways_To_Improve_Semantic_Segmentation_With_Self-Supervised_Depth_Estimation_CVPR_2021_paper.pdf) | [code](https://github.com/lhoyer/improving_segmentation_ with_selfsupervised_depth)<br><br>

[36] (AF) -S3Net: Attentive Feature Fusion with Adaptive Feature Selection for Sparse Semantic Segmentation Network(用于稀疏语义分割网络的具有自适应特征选择的注意力特征融合)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Cheng_AF2-S3Net_Attentive_Feature_Fusion_With_Adaptive_Feature_Selection_for_Sparse_CVPR_2021_paper.pdf)<br><br>

[35] Exploit Visual Dependency Relations for Semantic Segmentation(利用视觉依赖关系进行语义分割)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Exploit_Visual_Dependency_Relations_for_Semantic_Segmentation_CVPR_2021_paper.pdf)<br><br>

[34] Revisiting Superpixels for Active Learning in Semantic Segmentation with Realistic Annotation Costs(在具有现实注释成本的语义分割中重新审视用于主动学习的超像素)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Cai_Revisiting_Superpixels_for_Active_Learning_in_Semantic_Segmentation_With_Realistic_CVPR_2021_paper.pdf)<br><br>

[33] ABMDRNet: Adaptive-weighted Bi-directional Modality Difference Reduction Network for RGB-T Semantic Segmentation(ABMDRNet：用于 RGB-T 语义分割的自适应加权双向模态差异减少网络)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_ABMDRNet_Adaptive-Weighted_Bi-Directional_Modality_Difference_Reduction_Network_for_RGB-T_Semantic_CVPR_2021_paper.pdf)<br><br>

[32] CGA-Net: Category Guided Aggregation for Point Cloud Semantic Segmentation(CGA-Net：点云语义分割的类别引导聚合)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Lu_CGA-Net_Category_Guided_Aggregation_for_Point_Cloud_Semantic_Segmentation_CVPR_2021_paper.pdf)<br><br>

[31] Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers(使用 Transformer 从序列到序列的角度重新思考语义分割)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.pdf) | [project](https://fudan-zvg.github.io/SETR)<br><br>

[30] Anti-Adversarially Manipulated Attributions for Weakly and Semi-Supervised Semantic Segmentation(弱和半监督语义分割的反对抗操纵属性)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Anti-Adversarially_Manipulated_Attributions_for_Weakly_and_Semi-Supervised_Semantic_Segmentation_CVPR_2021_paper.pdf) | [code](https://github.com/jbeomlee93/AdvCAM)<br><br>

[29] Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision(具有交叉伪监督的半监督语义分割)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Semi-Supervised_Semantic_Segmentation_With_Cross_Pseudo_Supervision_CVPR_2021_paper.pdf)<br><br>

[28] Railroad is not a Train: Saliency as Pseudo-pixel Supervision for Weakly Supervised Semantic Segmentation(铁路不是火车：作为弱监督语义分割的伪像素监督的显著性)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Railroad_Is_Not_a_Train_Saliency_As_Pseudo-Pixel_Supervision_for_CVPR_2021_paper.pdf) ｜ [code](https://github.com/halbielee/EPS)<br><br>

[27] Cluster, Split, Fuse, and Update: Meta-Learning for Open Compound Domain Adaptive Semantic Segmentation(集群、拆分、融合和更新：开放复合域自适应语义分割的元学习)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Gong_Cluster_Split_Fuse_and_Update_Meta-Learning_for_Open_Compound_Domain_CVPR_2021_paper.pdf)<br><br>

[26] Omni-supervised Point Cloud Segmentation via Gradual Receptive Field Component Reasoning(通过渐进感受野分量推理的全方位监督点云分割)<br>
[paper](https://arxiv.org/abs/2105.10203) | [code](https://github.com/azuki-miho/RFCR)<br><br>

[25] Unsupervised Part Segmentation through Disentangling Appearance and Shape(基于外观和形状分离的无监督零件分割)<br>
[paper](https://arxiv.org/abs/2105.12405)<br><br>

[24] Self-supervised Augmentation Consistency for Adapting Semantic Segmentation(自适应语义分割的自我监督增强一致性)<br>
[paper](https://arxiv.org/abs/2105.00097) | [code](https://github.com/visinf/da-sac)<br><br>

[23] DANNet: A One-Stage Domain Adaptation Network for Unsupervised Nighttime Semantic Segmentation(DANNet：一种用于无监督夜间语义切分的单阶段域自适应网络)<br>
[paper](https://arxiv.org/abs/2104.10834) | [code](https://github. com/W-zx-Y/DANNet)<br><br>

[22] Improving Online Performance Prediction for Semantic Segmentation(改进用于语义分割的在线性能预测)<br>
[paper](https://arxiv.org/abs/2104.05255)<br><br>

[21] Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization(生成模型的语义分割：半监督学习和强大的域外泛化)<br>
[paper](https://arxiv.org/abs/2104.05833) ｜ [code](https://nv-tlabs.github.io/semanticGAN/)<br><br>

[20] Progressive Semantic Segmentation(渐进式语义分割)<br>
[paper](https://arxiv.org/abs/2104.03778) | [code](https://github.com/VinAIResearch/MagNet)<br><br>

[19] InverseForm: A Loss Function for Structured Boundary-Aware Segmentation(结构化边界感知分割的损失函数)<br>
[paper](https://arxiv.org/abs/2104.02745)<br><br>

[18] 3D-to-2D Distillation for Indoor Scene Parsing(用于室内场景解析的3D到2D蒸馏)<br>
[paper](https://arxiv.org/abs/2104.02243)<br><br>

[17] One Thing One Click: A Self-Training Approach for Weakly Supervised 3D Semantic Segmentation(一键式点击：一种用于弱监督3D语义分割的自训练方法)<br>
[paper](https://arxiv.org/abs/2104.02246)<br><br>

[16] Background-Aware Pooling and Noise-Aware Loss for Weakly-Supervised Semantic Segmentation(弱监督语义分割的背景感知池和噪声感知损失)<br>
[paper](https://arxiv.org/abs/2104.00905)<br><br>

[15] PiCIE: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering(PiCIE：在聚类中使用不变性和等方差的无监督语义分割)<br>
[paper](https://arxiv.org/abs/2103.17070) | [code](https://github.com/janghyuncho/PiCIE)<br><br>

[14] Source-Free Domain Adaptation for Semantic Segmentation(用于语义分割的无源域自适应)<br>
[paper](https://arxiv.org/abs/2103.16372)<br><br>

[13] RobustNet: Improving Domain Generalization in Urban-Scene Segmentation via Instance Selective Whitening(通过实例选择性增白提高城市场景分割中的域泛化)<br>
[paper](https://arxiv.org/abs/2103.15597) | [code](https://github.com/shachoi/RobustNet)<br><br>

[12] Coarse-to-Fine Domain Adaptive Semantic Segmentation with Photometric Alignment and Category-Center Regularization(具有光度对齐和类别中心正则化的粗到细域自适应语义分割)<br>
[paper](https://arxiv.org/abs/2103.13041)<br><br>

[11] Cross-Dataset Collaborative Learning for Semantic Segmentation(跨数据集协同学习的语义分割)<br>
[paper](https://arxiv.org/abs/2103.11351)<br><br>

[10] BBAM: Bounding Box Attribution Map for Weakly Supervised Semantic and Instance Segmentation(用于弱监督语义和实例细分的边界框归因图)<br>
[paper](https://arxiv.org/abs/2103.08907)<br><br>

[9] Continual Semantic Segmentation via Repulsion-Attraction of Sparse and Disentangled Latent Representations(通过稀疏和纠缠的潜在表示的排斥力进行连续语义分割)<br>
[paper](https://arxiv.org/abs/2103.06342)<br><br>

[8] Semantic Segmentation for Real Point Cloud Scenes via Bilateral Augmentation and Adaptive Fusion(通过双边扩充和自适应融合对实点云场景进行语义分割)<br>
[paper](https://arxiv.org/abs/2103.07074)<br><br>

[7] Capturing Omni-Range Context for Omnidirectional Segmentation(捕获全方位上下文进行全方位分割)<br>
[paper](https://arxiv.org/abs/2103.05687)<br><br>

[6] MetaCorrection: Domain-aware Meta Loss Correction for Unsupervised Domain Adaptation in Semantic Segmentation(MetaCorrection：语义分割中无监督域自适应的域感知元丢失校正)<br>
[paper](https://arxiv.org/abs/2103.05254)<br><br>

[5] Learning Statistical Texture for Semantic Segmentation(学习用于语义分割的统计纹理)<br>
[paper](https://arxiv.org/abs/2103.04133)<br><br>

[4] Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation(基于双层域混合的半监督域自适应语义分割)<br>
[paper](https://arxiv.org/pdf/2103.04705.pdf)<br><br>

[3] Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation(多源领域自适应与协作学习的语义分割)<br>
[paper](https://arxiv.org/abs/2103.04717)<br><br>

[2] Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset, Benchmarks and Challenges(走向城市规模3D点云的语义分割：数据集，基准和挑战)<br>
[paper](https://arxiv.org/abs/2009.03137) | [code](https://github.com/QingyongHu/SensatUrban)<br><br>

[1] PLOP: Learning without Forgetting for Continual Semantic Segmentation（PLOP：学习而不会忘记连续的语义分割）<br>
[paper](https://arxiv.org/abs/2011.11390) ｜ [code](https://github.com/arthurdouillard/CVPR2021_PLOP)<br><br>

<a name="InstanceSegmentation"/> 

### 实例分割(Instance Segmentation)

[24] BoxInst: High-Performance Instance Segmentation with Box Annotations(BoxInst：带框注释的高性能实例分割)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Tian_BoxInst_High-Performance_Instance_Segmentation_With_Box_Annotations_CVPR_2021_paper.pdf) | [code](https://git.io/AdelaiDet)<br><br>

[23] Unsupervised Discovery of the Long-Tail in Instance Segmentation Using Hierarchical Self-Supervision(使用分层自监督在实例分割中无监督地发现长尾)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Weng_Unsupervised_Discovery_of_the_Long-Tail_in_Instance_Segmentation_Using_Hierarchical_CVPR_2021_paper.pdf)<br><br>

[22] Seesaw Loss for Long-Tailed Instance Segmentation(长尾实例分割的跷跷板损失)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Seesaw_Loss_for_Long-Tailed_Instance_Segmentation_CVPR_2021_paper.pdf) | [code](https://github.com/open-mmlab/mmdetection)<br><br>

[21] ColorRL: Reinforced Coloring for End-to-End Instance Segmentation(ColorRL：端到端实例分割的强化着色)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Tuan_ColorRL_Reinforced_Coloring_for_End-to-End_Instance_Segmentation_CVPR_2021_paper.pdf)<br><br>

[20] SG-Net: Spatial Granularity Network for One-Stage Video Instance Segmentation(SG-Net：用于单阶段视频实例分割的空间粒度网络)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_SG-Net_Spatial_Granularity_Network_for_One-Stage_Video_Instance_Segmentation_CVPR_2021_paper.pdf)<br><br>

[19] Deeply Shape-guided Cascade for Instance Segmentation(用于实例分割的深度形状引导级联)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Ding_Deeply_Shape-Guided_Cascade_for_Instance_Segmentation_CVPR_2021_paper.pdf) | [code](https://github.com/hding2455/DSC)<br><br>

[18] DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation(DCT-Mask：用于实例分割的离散余弦变换掩码表示)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Shen_DCT-Mask_Discrete_Cosine_Transform_Mask_Representation_for_Instance_Segmentation_CVPR_2021_paper.pdf)<br><br>

[17] Point Cloud Instance Segmentation using Probabilistic Embeddings(使用概率嵌入的点云实例分割)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Point_Cloud_Instance_Segmentation_Using_Probabilistic_Embeddings_CVPR_2021_paper.pdf)<br><br>

[16] Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation(简单的复制粘贴是一种用于实例分割的强数据增强方法)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Ghiasi_Simple_Copy-Paste_Is_a_Strong_Data_Augmentation_Method_for_Instance_CVPR_2021_paper.pdf)  | [code](https://github.com/tensorflow/tpu/tree/master/models/ official/detection/projects/copy_paste)<br><br>

[15] Zero-Shot Instance Segmentation(零样本实例分割)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Zero-Shot_Instance_Segmentation_CVPR_2021_paper.pdf)<br><br>

[14] DyCo3D: Robust Instance Segmentation of 3D Point Clouds through Dynamic Convolution(DyCo3D：通过动态卷积对 3D 点云进行稳健的实例分割)<br>
[paper](https://arxiv.org/abs/2011.13328) | [code](https://git.io/DyCo3D)<br><br>

[13] Incremental Few-Shot Instance Segmentation(增量小样本实例分割)<br>
[paper](https://arxiv.org/abs/2105.05312) ｜ [code](https://github.com/danganea/iMTFA)<br><br>

[12] Robust Instance Segmentation through Reasoning about Multi-Object Occlusion(通过推理多对象遮挡进行鲁棒的实例分割)<br>
[paper](https://arxiv.org/abs/2012.02107) | [code](https://github.com/XD7479/Multi-Object-Occlusion)<br><br>

[11] A^2-FPN: Attention Aggregation based Feature Pyramid Network for Instance Segmentation(A ^ 2-FPN：基于注意力聚合的特征金字塔网络，用于实例分割)<br>
[paper](https://arxiv.org/abs/2105.03186)<br><br>

[10] RefineMask: Towards High-Quality Instance Segmentation with Fine-Grained Features(RefineMask：通过细粒度功能实现高质量实例分割)<br>
[paper](https://arxiv.org/abs/2104.08569) | [code](https://github.com/zhanggang001/RefineMask)<br><br>

[9] Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation(看起来更接近以更好地分割：用于实例分割的边界补丁优化)<br>
[paper](https://arxiv.org/abs/2104.05239) | [code](https://github.com/tinyalpha/BPR)<br><br>

[8] Spatial Feature Calibration and Temporal Fusion for Effective One-stage Video Instance Segmentation(空间特征校准和时间融合，以实现有效的一级视频实例分割)<br>
[paper](https://arxiv.org/abs/2104.05606) | [code](https://github.com/MinghanLi/STMask)<br><br>

[7] DARCNN: Domain Adaptive Region-based Convolutional Neural Network for Unsupervised Instance Segmentation in Biomedical Images(DARCNN：用于生物医学图像中无监督实例分割的基于域自适应区域的卷积神经网络)<br>
[paper](https://arxiv.org/abs/2104.01325)<br><br>

[6] Weakly-supervised Instance Segmentation via Class-agnostic Learning with Salient Images(通过带有显着图像的类不可知学习进行弱监督实例分割)<br>
[paper](https://arxiv.org/abs/2104.01526) | [code](https://github.com/hustvl/BoxCaseg)<br><br>

[5] FAPIS: A Few-shot Anchor-free Part-based Instance Segmenter(FAPIS：少量基于无锚的基于实例分割器)<br>
[paper](https://arxiv.org/abs/2104.00073)<br><br>

[4] Weakly Supervised Instance Segmentation for Videos with Temporal Mask Consistency(具有时间掩码一致性的视频的弱监督实例分割)<br>
[paper](https://arxiv.org/abs/2103.12886)<br><br>

[3] Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers(具有重叠BiLayer的深度遮挡感知实例分割)<br>
[paper](https://arxiv.org/abs/2103.12340) | [code](https://github.com/lkeab/BCNet)<br><br>

[2] BBAM: Bounding Box Attribution Map for Weakly Supervised Semantic and Instance Segmentation(用于弱监督语义和实例细分的边界框归因图)<br>
[paper](https://arxiv.org/abs/2103.08907) | [code](https://github.com/jbeomlee93/BBAM)<br><br>

[1] End-to-End Video Instance Segmentation with Transformers(使用Transformer的端到端视频实例分割) <br>
[paper](https://arxiv.org/abs/2011.14503) | [code](https://github.com/Epiphqny/VisTR)
<br><br>

<a name="Superpixel"/> 

## 超像素(Superpixel)

[2] Revisiting Superpixels for Active Learning in Semantic Segmentation with Realistic Annotation Costs(在具有现实注释成本的语义分割中重新审视用于主动学习的超像素)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Cai_Revisiting_Superpixels_for_Active_Learning_in_Semantic_Segmentation_With_Realistic_CVPR_2021_paper.pdf)<br><br>

[1] Learning the Superpixel in a Non-iterative and Lifelong Manner(以非迭代和终身的方式学习超像素)<br>
[paper](https://arxiv.org/pdf/2103.10681.pdf)<br><br>

<a name="VOS"/> 

### 视频目标分割(Video Object Segmentation)

[12] Learning Dynamic Network Using a Reuse Gate Function in Semi-supervised Video Object Segmentation(在半监督视频对象分割中使用重用门函数学习动态网络)<br>
[paper](https://arxiv.org/abs/2012.11655) ｜ [code](https://github.com/HYOJINPARK/Reuse_VOS)<br><br>

[11] SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation(Oral|SSTVOS：用于视频对象分割的稀疏时空变换器)<br>
[paper](https://arxiv.org/abs/2101.08833) ｜ [code](https://github.com/dukebw/SSTVOS)<br><br>

[10] SwiftNet: Real-time Video Object Segmentation(SwiftNet：实时视频对象分割)<br>
[paper](https://arxiv.org/abs/2102.04604) | [code](https://github.com/haochenheheda/SwiftNet)<br><br>

[9] Video Object Segmentation Using Global and Instance Embedding Learning(使用全局和实例嵌入学习的视频对象分割)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_Video_Object_Segmentation_Using_Global_and_Instance_Embedding_Learning_CVPR_2021_paper.pdf)<br><br>

[8] Delving Deep into Many-to-many Attention for Few-shot Video Object Segmentation(深入研究小样本视频对象分割的多对多注意力)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Delving_Deep_Into_Many-to-Many_Attention_for_Few-Shot_Video_Object_Segmentation_CVPR_2021_paper.pdf) ｜ [code](https://github.com/scutpaul/DANet)<br><br>

[7] Reciprocal Transformations for Unsupervised Video Object Segmentation(无监督视频对象分割的互易变换)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Reciprocal_Transformations_for_Unsupervised_Video_Object_Segmentation_CVPR_2021_paper.pdf) | [code](https://github.com/OliverRensu/RTNet)<br><br>

[6] Learning Position and Target Consistency for Memory-based Video Object Segmentation(基于内存的视频对象分割的学习位置和目标一致性)<br>
[paper](https://arxiv.org/abs/2104.04329)<br><br>

[5] Guided Interactive Video Object Segmentation Using Reliability-Based Attention Maps(基于可靠性的注意映射引导交互式视频对象分割)<br>
[paper](https://arxiv.org/abs/2104.10386) | [code](https://github.com/yuk6heo/GIS-RAmap)<br><br>

[4] Target-Aware Object Discovery and Association for Unsupervised Video Multi-Object Segmentation(无监督视频多对象分割的目标感知对象发现和关联)<br>
[paper](https://arxiv.org/abs/2104.04782)<br><br>

[3] Efficient Regional Memory Network for Video Object Segmentation(用于视频对象分割的高效区域存储网络)<br>
[paper](https://arxiv.org/abs/2103.12934) | [code](https://haozhexie.com/project/rmnet)<br><br>

[2] Learning to Recommend Frame for Interactive Video Object Segmentation in the Wild(学习推荐帧用于交互式野外视频对象分割)<br>
[paper](https://arxiv.org/pdf/2103.10391.pdf) | [code](https://github.com/svip-lab/IVOS-W)<br><br>

[1] Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion(模块化交互式视频对象分割：面具交互，传播和差异感知融合)<br>
[paper](https://arxiv.org/pdf/2103.07941.pdf) | [project](https://hkchengrex.github.io/MiVOS/)<br><br>

<a name="Matting"/> 

### 抠图(Matting)

[7] Deep Video Matting via Spatio-Temporal Alignment and Aggregation(通过时空对齐和聚合的深度视频抠图)<br>
[paper](https://arxiv.org/abs/2104.11208)<br><br>

[6] Learning Affinity-Aware Upsampling for Deep Image Matting(学习用于深度图像抠图的亲和感知上采样)<br>
[paper](https://arxiv.org/abs/2011.14288)<br><br>

[5] Semantic Image Matting(语义图像抠图)<br>
[paper](https://arxiv.org/abs/2104.08201) ｜ [code](https://github.com/nowsyn/SIM)<br><br>

[4] Mask Guided Matting via Progressive Refinement Network(通过渐进式细化网络的掩码引导抠图)<br>
[paper](https://arxiv.org/abs/2012.06722）| [code](https://github.com/yucornetto/MGMatting)<br><br>

[3] Omnimatte: Associating Objects and Their Effects in Video(Omnimatte：在视频中关联对象及其效果)<br>
[paper](https://arxiv.org/abs/2105.06993) | [project](https://omnimatte.github.io/)<br><br>

[2] Improved Image Matting via Real-time User Clicks and Uncertainty Estimation(通过实时用户点击和不确定性估计改善图像抠图)<br>
[paper](https://arxiv.org/abs/2012.08323)<br><br>

[1] Real-Time High Resolution Background Matting<br>
[paper](https://arxiv.org/abs/2012.07810) | [code](https://github.com/PeterL1n/BackgroundMattingV2) | [project](https://grail.cs.washington.edu/projects/background-matting-v2/) | [video](https://youtu.be/oMfPTeYDF9g)<br><br>

<a name="DensePrediction"/> 

### 密集预测(Dense Prediction)

[3] Generic Perceptual Loss for Modeling Structured Output Dependencies(用于建模结构化输出依存关系的一般感知损失)<br>
[paper](https://arxiv.org/pdf/2103.10571.pdf)<br><br>

[2]Densely connected multidilated convolutional networks for dense prediction tasks（用于密集预测任务的多重卷积连接网络）<br>
[paper](https://arxiv.org/abs/2011.11844)<br><br>

[1] Dense Contrastive Learning for Self-Supervised Visual Pre-Training(自监督视觉预训练的密集对比学习)<br>
[paper](https://arxiv.org/abs/2011.09157) | [code](https://github.com/WXinlong/DenseCL)<br><br>

<a name="Estimation"/> 

## 估计(Estimation)

<a name="HumanPoseEstimation"/> 

### 姿态估计(Human Pose Estimation)

[23] Beyond Static Features for Temporally Consistent 3D Human Pose and Shape from a Video(超越视频中时间一致的 3D 人体姿势和形状的静态特征)<br>
[paper](https://arxiv.org/abs/2011.08627) | [code](https://github.com/hongsukchoi/TCMR_RELEASE) | [video](https://youtu.be/WB3nTnSQDII)<br><br>

[22] ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search(ViPNAS：通过神经架构搜索进行高效的视频姿态估计)<br>
[paper](https://arxiv.org/abs/2105.10154)<br><br>

[21] When Human Pose Estimation Meets Robustness: Adversarial Algorithms and Benchmarks(当人体姿势估计达到稳健性时：对抗算法和基准)<br>
[paper](https://arxiv.org/abs/2105.06152)<br><br>

[20] Monocular Real-time Full Body Capture with Inter-part Correlations(具有部分间相关性的单眼实时全身捕获)<br>
[paper](https://arxiv.org/abs/2012.06087)<br><br>

[19] Unsupervised Human Pose Estimation through Transforming Shape Templates(通过变换形状模板的无监督人体姿势估计)<br>
[paper](https://arxiv.org/abs/2105.04154) | [project](https://infantmotion.github.io/)<br><br>

[18] Body Meshes as Points(身体网格物体为点)<br>
[paper](https://arxiv.org/abs/2105.02467) | [code](https://github.com/jfzhang95/BMP)<br><br>

[17] PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation(PoseAug：用于3D人类姿势估计的可微分姿势增强框架)<br>
[paper](https://arxiv.org/abs/2105.02465) | [code](https://github.com/jfzhang95/PoseAug)<br><br>

[16] AGORA: Avatars in Geography Optimized for Regression Analysis(AGORA：针对回归分析进行了优化的地理头像)<br>
[paper](https://arxiv.org/abs/2104.14643) | [project](https://agora.is.tue.mpg.de)<br><br>

[15] Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration(用于3D人体网格配准的局部感知分段变换字段)<br>
[paper](https://arxiv.org/abs/2104.08160) | [code](https://taconite.github.io/PTF/website/PTF.html)<br><br>

[14] Pose Recognition with Cascade Transformers(级联Transformer的姿势识别)<br>
[paper](https://arxiv.org/abs/2104.06976) | [code](https://github.com/mlpc-ucsd/PRTR)<br><br>

[13] Lite-HRNet: A Lightweight High-Resolution Network(Lite-HRNet：轻巧的高分辨率网络)<br>
[paper](https://arxiv.org/abs/2104.06403) | [code](https://github.com/HRNet/Lite-HRNet)<br><br>

[12] Multi-View Multi-Person 3D Pose Estimation with Plane Sweep Stereo(具有平面扫描立体声的多视图多人3D姿势估计)<br>
[paper](https://arxiv.org/abs/2104.02273) | [code](https://github.com/jiahaoLjh/PlaneSweepPose)<br><br>

[11] Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression(通过解聚的关键点自下而上的人体姿势估计)<br>
[paper](https://arxiv.org/abs/2104.02300) | [code](https://github.com/HRNet/DEKR)<br><br>

[10] Monocular 3D Multi-Person Pose Estimation by Integrating Top-Down and Bottom-Up Networks(通过集成自上而下和自下而上的网络进行单眼3D多人姿势估计)<br>
[paper](https://arxiv.org/abs/2104.01797) | [code](https://github.com/3dpose/3D-Multi-Person-Pose)<br><br>

[9] Reconstructing 3D Human Pose by Watching Humans in the Mirror(通过照镜子中的人来重建3D人的姿势)<br>
[paper](https://arxiv.org/abs/2104.00340) | [project](https://zju3dv.github.io/Mirrored-Human/)<br><br>

[8] SimPoE: Simulated Character Control for 3D Human Pose Estimation(用于3D人体姿势估计的模拟角色控制)<br>
[paper](https://arxiv.org/abs/2104.00683) | [project](https://www.ye-yuan.com/simpoe/)<br><br>

[7] Human POSEitioning System (HPS): 3D Human Pose Estimation and Self-localization in Large Scenes from Body-Mounted Sensors(人体姿势系统（HPS）：人体安装传感器在大场景中的3D人体姿势估计和自定位)<br>
[paper](https://arxiv.org/abs/2103.17265) | [project](http://virtualhumans.mpi-inf.mpg.de/hps/)<br><br>

[6] Graph Stacked Hourglass Networks for 3D Human Pose Estimation(用于3D人体姿势估计的图形堆叠沙漏网络)<br>
[paper](https://arxiv.org/abs/2103.16385)<br><br>

[5] From Synthetic to Real: Unsupervised Domain Adaptation for Animal Pose Estimation(【动物姿态估计】从合成到真实：用于动物姿势估计的无监督域自适应)<br>
[paper](https://arxiv.org/abs/2103.14843) | [code](https://github.com/chaneyddtt/UDA-Animal-Pose)<br><br>

[4] DCPose: Deep Dual Consecutive Network for Human Pose Estimation(用于人体姿态估计的深度双重连续网络)<br>
[paper](https://arxiv.org/abs/2103.07254) | [code](https://github.com/Pose-Group/DCPose)<br><br>

[3] Differentiable Multi-Granularity Human Representation Learning for Instance-Aware Human Semantic Parsing(用于实例感知人类语义解析的可微分多粒度人类表示学习)<br>
[paper](https://arxiv.org/pdf/2103.04570.pdf) | [code](https://github.com/tfzhou/MG-HumanParsing)<br><br>

[2] CanonPose: Self-supervised Monocular 3D Human Pose Estimation in the Wild（野外自监督的单眼3D人类姿态估计）<br><br>

[1] PCLs: Geometry-aware Neural Reconstruction of 3D Pose with Perspective Crop Layers（具有透视作物层的3D姿势的几何感知神经重建）<br>
[paper](https://arxiv.org/abs/2011.13607)<br><br>

<a name="GestureEstimation"/> 


### 手势估计(Gesture Estimation)

[6] Improving Sign Language Translation with Monolingual Data by Sign Back-Translation(【手势翻译】基于单语数据的手语翻译研究)<br>
[paper](https://arxiv.org/abs/2105.12397)<br><br>

[5] ContactOpt: Optimizing Contact to Improve Grasps(ContactOpt：优化联系人以提高抓地力)<br>
[paper](https://arxiv.org/abs/2104.07267)<br><br>

[4] Fingerspelling Detection in American Sign Language(美国手语中的手指拼写检测)<br>
[paper](https://arxiv.org/abs/2104.01291)<br><br>

[3] Read and Attend: Temporal Localisation in Sign Language Videos(阅读和参加：手语视频中的时间本地化)<br>
[paper](https://arxiv.org/abs/2103.16481) | [project](https://www.robots.ox.ac.uk/ ̃vgg/research/bslattend/)<br><br>

[2] Skeleton Based Sign Language Recognition Using Whole-body Keypoints(基于全身关键点的基于骨架的手语识别)<br>
[paper](https://arxiv.org/abs/2103.08833) | [code](https://github.com/jackyjsy/CVPR21Chal-SLR)<br><br>

[1] Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive  2D-1D Registration(基于语义聚合和自适应2D-1D配准的相机空间手部网格恢复)<br>
[paper](https://arxiv.org/pdf/2103.02845.pdf) | [code](https://github.com/SeanChenxy/HandMesh)<br><br>

<a name="Flow/Pose/MotionEstimation"/> 

### 光流/位姿/运动估计(Optical Flow/Pose/Motion Estimation)

[17] Exploring intermediate representation for monocular vehicle pose estimation(探索单目车辆姿态估计的中间表示)<br>
[paper](https://arxiv.org/abs/2011.08464) ｜ [code](https://github.com/Nicholasli1995/EgoNet)<br><br>

[16] Extreme Rotation Estimation using Dense Correlation Volumes(使用密集相关体积的极端旋转估计)<br>
[paper](https://arxiv.org/abs/2104.13530) | [project](https://ruojincai.github.io/ExtremeRotation/)<br><br>

[15] Motion Representations for Articulated Animation(【运动估计&表示】关节动画的运动表示)<br>
[paper](https://arxiv.org/abs/2104.11280) | [code](https://github.com/snap-research/articulated-animation)<br><br>

[14] Self-Supervised Pillar Motion Learning for Autonomous Driving(【运动估计】用于自动驾驶的自我监督支柱运动学习)<br>
[paper](https://arxiv.org/abs/2104.08683)<br><br>

[13] Single-view robot pose and joint angle estimation via render & compare(通过渲染和比较进行单视图机器人姿态和关节角度估计)<br>
[paper](https://arxiv.org/abs/2104.09359) | [code](https://www.di.ens.fr/willow/research/robopose/)<br><br>

[12] Fusing the Old with the New: Learning Relative Camera Pose with Geometry-Guided Uncertainty(新旧融合：通过几何引导的不确定性学习相对相机姿势)<br>
[paper](https://arxiv.org/abs/2104.08278)<br><br>

[11] VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals(【视觉测距】VOLDOR：来自对数逻辑密集光流残差的视觉里程表)<br>
[paper](https://arxiv.org/abs/2104.06789)<br><br>

[10] DSC-PoseNet: Learning 6DoF Object Pose Estimation via Dual-scale Consistency(【6D位姿估计】通过双尺度一致性学习6DoF对象姿势估计)<br>
[paper](https://arxiv.org/abs/2104.03658)<br><br>

[9] Learning optical flow from still images(【光流估计】从静止图像中学习光流)<br>
[paper](https://arxiv.org/abs/2104.03965) | [project](https://mattpoggi.github.io/projects/cvpr2021aleotti/)<br><br>

[8] Learning Optical Flow from a Few Matches(【光流估计】通过少量匹配学习光流)<br>
[paper](https://arxiv.org/abs/2104.02166) | [code](https://github.com/zacjiang/scv)<br><br>

[7] FESTA: Flow Estimation via Spatial-Temporal Attention for Scene Point Clouds(【光流估计】FESTA：场景点云通过时空注意进行光流估计)<br>
[paper](https://arxiv.org/abs/2104.00798)<br><br>

[6] Wide-Depth-Range 6D Object Pose Estimation in Space(【6D位姿估计】空间中的深度范围6D对象姿态估计)<br>
[paper](https://arxiv.org/abs/2104.00337)<br><br>

[5] Deep Two-View Structure-from-Motion Revisited(重新审视运动的深层两视图结构)<br>
[paper](https://arxiv.org/abs/2104.00556)<br><br>

[4] FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism(【6D位姿估计】具有分离旋转机制的类别级6D对象姿势估计的快速基于形状的网络)<br>
[paper](https://arxiv.org/abs/2103.07054) | [code](https://github.com/DC1991/FS_Net)<br><br>

[3] GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation(【6D位姿估计】用于单眼6D对象姿态估计的几何引导直接回归网络)<br>
[paper](https://arxiv.org/abs/2102.12145) | [code](https://github.com/THU-DA-6D-Pose-Group/GDR-Net)<br><br>

[2] Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments(在动态室内环境中，通过空间划分的鲁棒神经路由可实现摄像机的重新定位)<br>
[paper](https://arxiv.org/abs/2012.04746) | [project](https://ai.stanford.edu/~hewang/)<br><br>

[1] MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan Synchronization(通过3D扫描同步进行多主体分割和运动估计)<br>
[paper](https://arxiv.org/pdf/2101.06605.pdf) | [code](https://github.com/huangjh-pub/multibody-sync)<br><br>

<a name="DepthEstimation"/> 

### 深度估计(Depth Estimation)

[18] MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera(MonoRec：在动态环境中从单个移动相机进行半监督密集重建)<br>
[paper](https://arxiv.org/abs/2011.11814) | [project](https://vision.in.tum.de/research/monorec)<br><br>

[17] Single Image Depth Estimation using Wavelet Decomposition(使用小波分解的单幅图像深度估计)<br>
[paper](https://arxiv.org/abs/2106.02022) | [code](https://github.com/nianticlabs/wavelet-monodepth)<br><br>

[16] Self-Supervised Multi-Frame Monocular Scene Flow(自监督多帧单眼场景流)<br>
[paper](https://arxiv.org/abs/2105.02216) | [code](https://github.com/visinf/multi-mono-sf)<br><br>

[15] Binary TTC: A Temporal Geofence for Autonomous Navigation(【接触时间估计】二进制TTC：自主导航的时空地理围栏)<br>
[paper](https://arxiv.org/abs/2101.04777)<br><br>

[14] The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth(时间机会主义者：自我监督的多帧单眼深度)<br>
[paper](https://arxiv.org/abs/2104.14540)<br><br>

[13] Lighting, Reflectance and Geometry Estimation from 360∘ Panoramic Stereo(360∘全景立体的光照、反射率和几何估计)<br>
[paper](https://arxiv.org/abs/2104.09886) | [code](https://github.com/junxuan-li/LRG_360Panoramic)<br><br>

[12] Depth Completion using Plane-Residual Representation(使用平面残差表示法的深度补全)<br>
[paper](https://arxiv.org/abs/2104.07350)<br><br>

[11] StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision(通过立体视觉进行深度感知的布景人类数字化)<br>
[paper](https://arxiv.org/abs/2104.05289) | [projec](http://crishy1995.github.io/StereoPIFuProject)<br><br>

[10] Self-supervised Learning of Depth Inference for Multi-view Stereo(多视图立体声深度推理的自我监督学习)<br>
[paper](https://arxiv.org/abs/2104.02972) | [code](https://github.com/JiayuYANG/Self-supervised-CVP-MVSNet)<br><br>

[9] Depth Completion with Twin Surface Extrapolation at Occlusion Boundaries(遮挡边界处的深度补全和双曲面外推)<br>
[paper](https://arxiv.org/abs/2104.02253)<br><br>

[8] S2R-DepthNet: Learning a Generalizable Depth-specific Structural Representation(学习通用的深度特定的结构表示)<br>
[paper](https://arxiv.org/abs/2104.00877) | [code](https://github.com/microsoft/S2R-DepthNet)<br><br>

[7] RGB-D Local Implicit Function for Depth Completion of Transparent Objects(RGB-D局部隐式函数用于透明对象的深度补全)<br>
[paper](https://arxiv.org/abs/2104.00622) | [code](https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit)<br><br>

[6] LED2-Net: Monocular 360 Layout Estimation via Differentiable Depth Rendering(通过可分辨深度渲染进行单眼360布局估算)<br>
[paper](https://arxiv.org/abs/2104.00568) | [project](https://fuenwang.ml/project/led2net)<br><br>

[5] Deep Two-View Structure-from-Motion Revisited(重新审视运动的深层两视图结构)<br>
[paper](https://arxiv.org/abs/2104.00556)<br><br>

[4] Mask-ToF: Learning Microlens Masks for Flying Pixel Correction in Time-of-Flight Imaging(学习微透镜掩模以在飞行时间成像中进行飞行像素校正)<br>
[paper](https://arxiv.org/abs/2103.16693) | [project](https://light.princeton.edu/publication/mask-tof)<br><br>

[3] Generalizing to the Open World: Deep Visual Odometry with Online Adaptation(推广到开放世界：具有在线适应功能的深度视觉里程表)<br>
[paper](https://arxiv.org/abs/2103.15279)<br><br>

[2] Beyond Image to Depth: Improving Depth Prediction using Echoes(超越图像深度：使用回声改善深度预测)<br>
[paper](https://arxiv.org/pdf/2103.08468.pdf) | [code](https://krantiparida.github.io/projects/bimgdepth.html)<br><br>


[1] PLADE-Net: Towards Pixel-Level Accuracy for Self-Supervised Single-View Depth Estimation with Neural Positional Encoding and Distilled Matting Loss(具有神经位置编码和蒸馏消光损耗的自我监督单视图深度估计的像素级精度)<br>
[paper](https://arxiv.org/abs/2103.07362)<br><br>

<br>

<a name="ImageProcessing"/> 


## 图像处理(Image Processing)

[1] Invertible Image Signal Processing(可逆图像信号处理)<br>
[paper](https://arxiv.org/abs/2103.15061) | [code](https://github.com/yzxing87/Invertible-ISP)<br><br>

<a name="SuperResolution"/> 

### 超分辨率(Super Resolution)

[10] KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local Adjustment(KOALAnet：使用面向内核的自适应局部调整的盲超分辨率)<br>
[paper](https://arxiv.org/abs/2012.08103)<br><br>

[9] BasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond()<br>
[paper](https://arxiv.org/abs/2012.02181)<br>

[8] Temporal Modulation Network for Controllable Space-Time Video Super-Resolution(可控时空视频超分辨率的时间调制网络)<br>
[paper](https://arxiv.org/abs/2104.10642) | [code](https://github.com/CS-GangXu/TMNet)<br><br>

[7] SRWarp: Generalized Image Super-Resolution under Arbitrary Transformation(SRWarp：任意变换下的广义图像超分辨率)<br>
[paper](https://arxiv.org/abs/2104.10325)<br><br>

[6] Unsupervised Degradation Representation Learning for Blind Super-Resolution(盲超分辨率的无监督退化表示学习)<br>
[paper](https://arxiv.org/abs/2104.00416) | [code](https://github.com/LongguangWang/DASR)<br><br>

[5] Flow-based Kernel Prior with Application to Blind Super-Resolution(基于流的内核先于盲超分辨率的应用)<br>
[paper](https://arxiv.org/abs/2103.15977) | [code](https://github.com/JingyunLiang/FKP)<br><br>

[4] ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic(通过数据特征加速超分辨率网络的通用框架)<br>
[paper](https://arxiv.org/abs/2103.04039) | [解读-超分性能不降低，计算量降低50%：加速图像超分的ClassSR](https://zhuanlan.zhihu.com/p/355873199)<br><br>

[3] Learning Continuous Image Representation with Local Implicit Image Function(通过局部隐含图像功能学习连续图像表示)<br>
[paepr](https://arxiv.org/abs/2012.09161) | [code](https://github.com/yinboc/liif) | [video](https://youtu.be/6f2roieSY_8) | [project](https://yinboc.github.io/liif/)<br><br>

[2] Data-Free Knowledge Distillation For Image Super-Resolution(DAFL算法的SR版本)<br><br>

[1] AdderSR: Towards Energy Efficient Image Super-Resolution(将加法网路应用到图像超分辨率中)<br>
[paper](https://arxiv.org/pdf/2009.08891.pdf) | [code](https://github.com/huawei-noah/AdderNet)<br>
解读：[华为开源加法神经网络](https://zhuanlan.zhihu.com/p/113536045)<br><br>

<a name="ImageRestoration"/> 

###  图像复原/图像增强(Image Restoration)

[3] Removing Diffraction Image Artifacts in Under-Display Camera via Dynamic Skip Connection Network(利用动态跳跃连接网络消除显示下摄像机衍射图像伪影)<br>
[paper](https://arxiv.org/abs/2104.09556)<br><br>

[2] NeX: Real-time View Synthesis with Neural Basis Expansion(NeX：具有神经基础扩展的实时视图合成)<br>
[paper](https://arxiv.org/abs/2103.05606) | [code](https://nex-mpi.github.io/)<br><br>

[1] Multi-Stage Progressive Image Restoration(多阶段渐进式图像复原)<br>
[paper](https://arxiv.org/abs/2102.02808) | [code](https://github.com/swz30/MPRNet)<br><br>


<a name="ISR"/> 

### 图像去阴影/去反射(Image Shadow Removal/Image Reflection Removal)

[3] From Shadow Generation to Shadow Removal(从阴影生成到阴影去除)<br>
[paper](https://arxiv.org/abs/2103.12997)<br><br>

[2] Robust Reflection Removal with Reflection-free Flash-only Cues(通过无反射的仅含Flash线索进行鲁棒的反射去除)<br>
[paper](https://arxiv.org/pdf/2103.04273.pdf) | [code](https://github.com/ChenyangLEI/flash-reflection-removal)<br><br>

[1] Auto-Exposure Fusion for Single-Image Shadow Removal(用于单幅图像阴影去除的自动曝光融合)<br>
[paper](https://arxiv.org/abs/2103.01255) | [code](https://github.com/tsingqguo/exposure-fusion-shadow-removal)<br><br>



<a name="ImageDenoising"/> 

### 图像去噪/去模糊/去雨去雾(Image Denoising)

[8] FBI-Denoiser: Fast Blind Image Denoiser for Poisson-Gaussian Noise(FBI-Denoiser：泊松-高斯噪声的快速盲图像降噪器)<br>
[paper](https://arxiv.org/abs/2105.10967) | [code](https://github.com/csm9493/FBI-Denoiser)<br><br>

[7] Deep Denoising of Flash and No-Flash Pairs for Photography in Low-Light Environments(弱光环境下用于摄影的闪光灯和非闪光灯对的深度降噪)<br>
[paper](https://arxiv.org/abs/2012.05116) | [project](https://www.cse.wustl.edu/~zhihao.xia/deepfnf/)<br><br>

[6] Digital Gimbal: End-to-end Deep Image Stabilization with Learnable Exposure Times(数码云台：具有可学习的曝光时间的端到端深度图像稳定)<br>
[paper](https://arxiv.org/abs/2012.04515)<br><br>

[5] Contrastive Learning for Compact Single Image Dehazing(紧凑型单图像去雾的对比学习)<br>
[paper](https://arxiv.org/abs/2104.09367) | [code](https://github.com/GlassyWu/AECR-Net)<br><br>

[4] Explore Image Deblurring via Blur Kernel Space(通过模糊内核空间探索图像去模糊)<br>
[paper](https://arxiv.org/abs/2104.00317)<br><br>

[3] Semi-Supervised Video Deraining with Dynamic Rain Generator(带动态雨水产生器的半监督视频去雨)<br>
[paper](https://arxiv.org/abs/2103.07939)<br><br>

[2] ARVo: Learning All-Range Volumetric Correspondence for Video Deblurring(学习用于视频去模糊的全范围体积对应)<br>
[paper](https://arxiv.org/pdf/2103.04260.pdf)<br><br>

[1] DeFMO: Deblurring and Shape Recovery of Fast Moving Objects(快速移动物体的去模糊和形状恢复)<br>
[paper](https://arxiv.org/abs/2012.00595) | [code](https://github.com/rozumden/DeFMO) | [video](https://www.youtube.com/watch?v=pmAynZvaaQ4)<br><br>

<a name="ImageEdit"/> 

### 图像编辑/图像修复(Image Edit/Inpainting)

[11] PD-GAN: Probabilistic Diverse GAN for Image Inpainting(PD-GAN：用于图像修复的概率多样GAN)<br>
[paper](https://arxiv.org/abs/2105.02201)<br><br>

[10] StyleMapGAN: Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing(StyleMapGAN：利用GAN中潜在的空间维度进行实时图像编辑)<br>
[paper](https://arxiv.org/abs/2104.14754) | [code](https://github.com/naver-ai/StyleMapGAN)<br><br>

[9] Image Inpainting with External-internal Learning and Monochromic Bottleneck(具有内在内在学习和单色瓶颈的图像修复)<br>
[paper](https://arxiv.org/abs/2104.09068)<br><br>

[8] TransFill: Reference-guided Image Inpainting by Merging Multiple Color and Spatial Transformations(通过合并多个颜色和空间变换进行参考引导的图像修复)
[paper](https://arxiv.org/abs/2103.15982)<br><br>

[7] DeFLOCNet: Deep Image Editing via Flexible Low-level Controls(通过灵活的低级控件进行深度图像编辑)<br>
[paper](https://arxiv.org/abs/2103.12723)<br><br>

[6] Generating Diverse Structure for Image Inpainting With Hierarchical VQ-VAE(使用分层VQ-VAE生成图像修复的多样结构)<br>
[paper](https://arxiv.org/pdf/2103.10022) | [code](https://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting)<br><br>

[5] PISE: Person Image Synthesis and Editing with Decoupled GAN(使用分离的GAN进行人像合成和编辑)<br>
[paper](https://arxiv.org/abs/2103.04023) | [code](https://github.com/Zhangjinso/PISE)<br><br>

[4] DeFLOCNet: Deep Image Editing via Flexible Low level Controls(通过灵活的低级控件进行深度图像编辑)<br><br>

[3] PD-GAN: Probabilistic Diverse GAN for Image Inpainting(用于图像修复的概率多样GAN)<br><br>

[2] Anycost GANs for Interactive Image Synthesis and Editing(用于交互式图像合成和编辑的AnyCost Gans)<br>
[paper](https://arxiv.org/abs/2103.03243) | [code](https://github.com/mit-han-lab/anycost-gan)<br><br>

[1] Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing（利用GAN中潜在的空间维度进行实时图像编辑）<br><br>

<a name="ImageTranslation"/> 

### 图像翻译(Image Translation)

[9] CoCosNet v2: Full-Resolution Correspondence Learning for Image Translation(CoCosNet v2：用于图像翻译的全分辨率函授学习)<br>
[paper](https://arxiv.org/abs/2012.02047)<br><br>

[8] Visualizing Adapted Knowledge in Domain Transfer(领域转移中适应性知识的可视化)<br>
[paper](https://arxiv.org/abs/2104.10602) | [code](https://github.com/hou-yz/DA_visualization)<br><br>

[7] Memory-guided Unsupervised Image-to-image Translation(内存引导的无监督图像到图像翻译)<br>
[paper](https://arxiv.org/abs/2104.05170)<br><br>

[6] ReMix: Towards Image-to-Image Translation with Limited Data(使用有限的数据实现图像到图像的翻译)<br>
[paper](https://arxiv.org/abs/2103.16835)<br><br>

[5] Closing the Loop: Joint Rain Generation and Removal via Disentangled Image Translation(闭环：通过解图像翻译联合产生和去除雨水)<br>
[paper](https://arxiv.org/abs/2103.13660)<br><br>
 
[4] CoMoGAN: continuous model-guided image-to-image translation(连续的模型指导的图像到图像翻译)<br>
[paper](https://arxiv.org/abs/2103.06879) | [code](http://github.com/cv-rits/CoMoGAN)<br><br>

[3] Spatially-Adaptive Pixelwise Networks for Fast Image Translation(空间自适应像素网络，用于快速图像翻译)<br>
[paper](https://arxiv.org/abs/2012.02992) | [project](https://tamarott.github.io/ASAPNet_web/)<br><br>

[2] Image-to-image Translation via Hierarchical Style Disentanglement<br>
[paper](https://arxiv.org/abs/2103.01456) | [code](https://github.com/imlixinyang/HiSD) | [解读-层次风格解耦：人脸多属性篡改终于可控了](https://zhuanlan.zhihu.com/p/354258056)<br><br>

[1] Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation(样式编码：用于图像到图像翻译的StyleGAN编码器)<br>
[paper](https://arxiv.org/abs/2008.00951) | [code](https://github.com/eladrich/pixel2style2pixel) | [project](https://eladrich.github.io/pixel2style2pixel/)<br><br>

<a name="IQA"/> 

### 图像质量评估(Image Quality Assessment)

[1] SDD-FIQA: Unsupervised Face Image Quality Assessment with Similarity Distribution Distance(具有相似分布距离的无监督人脸图像质量评估)<br>
[paper](https://arxiv.org/abs/2103.05977)<br><br>

<a name="StyleTransfer"/> 

### 风格迁移(Style Transfer)

[6] Style-Aware Normalized Loss for Improving Arbitrary Style Transfer(一种改进任意风格转换的风格感知归一化损失算法)<br>
[paper](https://arxiv.org/abs/2104.10064)<br><br>

[5] Instagram Filter Removal on Fashionable Images(删除时尚图片上的Instagram滤镜)<br>
[paper](https://arxiv.org/abs/2104.05072)<br><br>

[4] Drafting and Revision: Laplacian Pyramid Network for Fast High-Quality Artistic Style Transfer(起草和修订：拉普拉斯金字塔网络，用于快速高质量的艺术风格转移)<br>
[paper](https://arxiv.org/abs/2104.05376) | [code](https://github.com/PaddlePaddle/PaddleGAN/)<br><br>

[3] Rethinking and Improving the Robustness of Image Style Transfer(重新思考和改善图像风格迁移的鲁棒性)<br>
[paper](https://arxiv.org/abs/2104.05623)<br><br>

[2] ArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows(通过可逆神经流进行无偏的图像风格迁移)<br>
[paper](https://arxiv.org/abs/2103.16877)<br><br>

[1] Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes(重新考虑风格迁移：从像素到参数化笔触)<br>
[paper](https://arxiv.org/abs/2103.17185)<br><br>

<br>

<a name="Face"/> 

## 人脸(Face)

[6] Continuous Face Aging via Self-estimated Residual Age Embedding(通过自我估计的残差年龄嵌入来实现连续的面部老化)<br>
[paper](https://arxiv.org/abs/2105.00020)<br><br>

[5] Towards High Fidelity Face Relighting with Realistic Shadows(逼真的阴影逼真的高保真面部)<br>
[paper](https://arxiv.org/abs/2104.00825)<br><br>

[4] Unsupervised Disentanglement of Linear-Encoded Facial Semantics(线性编码的面部语义的无监督解缠)<br>
[paper](https://arxiv.org/abs/2103.16605)<br><br>

[3] High-fidelity Face Tracking for AR/VR via Deep Lighting Adaptation(通过深度照明自适应实现AR / VR的高保真人脸跟踪)<br>
[paper](https://arxiv.org/abs/2103.15876) | [project](https://www.cs.rochester.edu/u/lchen63)<br><br>

[2] Structure-Aware Face Clustering on a Large-Scale Graph with 10^7 Nodes(具有10^7个节点的大规模图上的结构感知人脸聚类)<br>
[paper](https://arxiv.org/abs/2103.13225) | [code&project](https://sstzal.github.io/STAR-FC/)<br><br>

[1] SDD-FIQA: Unsupervised Face Image Quality Assessment with Similarity Distribution Distance(具有相似分布距离的无监督人脸图像质量评估)<br>
[paper](https://arxiv.org/abs/2103.05977)<br><br>

<a name="FacialRecognition"/> 

### 人脸识别/检测(Facial Recognition/Detection)

[12] Dynamic Class Queue for Large Scale Face Recognition In the Wild(野外大规模人脸识别的动态类队列)<br>
[paper](https://arxiv.org/abs/2105.11113) | [code](https://github.com/bilylee/DCQ)<br><br>

[11] Feature Decomposition and Reconstruction Learning for Effective Facial Expression Recognition(特征分解与重构学习对有效的面部表情识别)<br>
[paper](https://arxiv.org/abs/2104.05160)<br><br>

[10] FACESEC: A Fine-grained Robustness Evaluation Framework for Face Recognition Systems(FACESEC：用于人脸识别系统的细粒度鲁棒性评估框架)<br>
[paper](https://arxiv.org/abs/2104.04107)<br><br>

[9] IronMask: Modular Architecture for Protecting Deep Face Template(用于保护深脸模板的模块化体系结构)<br>
[paper](https://arxiv.org/abs/2104.02239)<br><br>

[8] HLA-Face: Joint High-Low Adaptation for Low Light Face Detection(用于低光人脸检测的联合高低适应)<br>
[paper](https://arxiv.org/abs/2104.01984)  | [project](https://daooshee.github.io/HLA-Face-Website/)<br><br>

[7] Dive into Ambiguity: Latent Distribution Mining and Pairwise Uncertainty Estimation for Facial Expression Recognition(潜入歧义：面部表情识别的潜在分布挖掘和成对不确定性估计)<br>
[paper](https://arxiv.org/abs/2104.00232)<br><br>

[6] Affective Processes: stochastic modelling of temporal context for emotion and facial expression recognition(情感过程：情感和面部表情识别的时态随机模型)<br>
[paper](https://arxiv.org/abs/2103.13372)<br><br>

[5] Cross-Domain Similarity Learning for Face Recognition in Unseen Domains(跨域相似性学习在未知领域中的人脸识别)<br>
[paper](https://arxiv.org/abs/2103.07503)<br><br>

[4] MagFace: A Universal Representation for Face Recognition and Quality Assessment(MagFace：人脸识别和质量评估的通用表示形式)<br>
[paper](https://arxiv.org/abs/2103.06627) | [code](https://github.com/IrvingMeng/MagFace)<br><br>

[3] CRFace: Confidence Ranker for Model-Agnostic Face Detection Refinement(用于模型不可知的面部检测细化的置信度排名)<br>
[paper](https://arxiv.org/abs/2103.07017)<br><br>

[2] A 3D GAN for Improved Large-pose Facial Recognition(用于改善大姿势面部识别的3D GAN)<br>
[paper](https://arxiv.org/pdf/2012.10545.pdf)<br><br>

[1] WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face Recognition(揭示了百万级深度人脸识别力量的基准测试)<br>
[paper](https://arxiv.org/abs/2103.04098) | [benchmark](https://www.face-benchmark.org/)<br><br>

<a name="FaceSynthesis"/> 

### 人脸生成/合成/重建/编辑(Face Generation/Face Synthesis/Face Reconstruction/Face Editing)

[14] Lifting 2D StyleGAN for 3D-Aware Face Generation(提升 2D StyleGAN 以生成 3D 感知人脸)<br>
[paper](https://arxiv.org/abs/2011.13126)<br><br>

[13] Monocular Real-time Full Body Capture with Inter-part Correlations(具有部分间相关性的单眼实时全身捕获)<br>
[paper](https://arxiv.org/abs/2012.06087)<br><br>

[12] Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation(基于隐式模块化视听表示的姿态可控人脸生成)<br>
[paper](https://arxiv.org/abs/2104.11116) | [code](https://hangz-nju-cuhk.github.io/projects/PC-AVS)<br><br>

[11] Audio-Driven Emotional Video Portraits(音频情感视频肖像)<br>
[paper](https://arxiv.org/abs/2104.07452)<br><br>

[10] Pixel Codec Avatars(像素编解码器头像)<br>
[paper](https://arxiv.org/abs/2104.04638)<br><br>

[9] Riggable 3D Face Reconstruction via In-Network Optimization(通过网络内优化进行可操纵的3D人脸重建)<br>
[paper](https://arxiv.org/abs/2104.03493) | [code](https://github.com/zqbai-jeremy/INORig)<br><br>

[8] Everything's Talkin': Pareidolia Face Reenactment(一切都在说话'：帕累多利亚脸部重现)<br>
[paper](https://arxiv.org/abs/2104.03061) | [project](https://wywu.github.io/projects/ETT/ETT.html)

[7] High-Fidelity and Arbitrary Face Editing(高保真和任意脸部编辑)<br>
[paper](https://arxiv.org/abs/2103.15814)<br><br>

[6] 3DCaricShop: A Dataset and A Baseline Method for Single-view 3D Caricature Face Reconstruction(单视图3D漫画面部重建的数据集和基线方法)<br>
[paper](https://arxiv.org/pdf/2103.08204.pdf) | [project](https://qiuyuda.github.io/3DCaricShop/)<br><br>

[5] ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis(进行全面伪造分析的多功能基准)<br>
[paper](https://arxiv.org/abs/2103.05630) | [code](https://yinanhe.github.io/projects/forgerynet.html)<br><br>

[4] Image-to-image Translation via Hierarchical Style Disentanglement(通过分层样式分解实现图像到图像的翻译)<br>
[paper](https://arxiv.org/abs/2103.01456) | [code](https://github.com/imlixinyang/HiSD)<br><br>

[3] When Age-Invariant Face Recognition Meets Face Age Synthesis: A  Multi-Task Learning Framework(当年龄不变的人脸识别遇到人脸年龄合成时：一个多任务学习框架)<br>
[paper](https://arxiv.org/abs/2103.01520) | [code](https://github.com/Hzzone/MTLFace)<br><br>

[2] PISE: Person Image Synthesis and Editing with Decoupled GAN(使用分离的GAN进行人像合成和编辑)<br>
[paper](https://arxiv.org/abs/2103.04023) | [code](https://github.com/Zhangjinso/PISE)<br><br>

[1] Soft-IntroVAE: Analyzing and Improving Introspective Variational Autoencoders(分析和改进自省变分自动编码器)<br>
[paper](https://arxiv.org/pdf/2012.13253.pdf) | [code](https://github.com/taldatech/soft-intro-vae-pytorch) | [project](https://taldatech.github.io/soft-intro-vae-web/)<br><br>

<a name="FaceAnti-Spoofing"/> 

### 人脸伪造/反欺骗(Face Forgery/Face Anti-Spoofing)

[6] Improving the Efficiency and Robustness of Deepfakes Detection through Precise Geometric Features(通过精确的几何特征提高假脸检测的效率和鲁棒性)<br>
[paper](https://arxiv.org/abs/2104.04480)<br><br>

[5] Face Forensics in the Wild(人脸伪造数据集)<br>
[paper](https://arxiv.org/abs/2103.16076) | [code](https://github.com/tfzhou/FFIW)<br><br>

[4] Frequency-aware Discriminative Feature Learning Supervised by Single-Center Loss for Face Forgery Detection(【人脸伪造检测】由单中心损失监督的频率感知判别特征学习，用于人脸伪造检测)<br>
[paper](https://arxiv.org/abs/2103.09096)<br><br>

[3] MagDR: Mask-guided Detection and Reconstruction for Defending Deepfakes(面罩引导的检测和重建，以防御深造假)<br>
[paper](https://arxiv.org/abs/2103.14211)<br><br>

[2] Cross Modal Focal Loss for RGBD Face Anti-Spoofing(跨模态焦点损失，用于RGBD人脸反欺骗)
[paper](https://arxiv.org/abs/2103.00948)<br><br>

[1] Multi-attentional Deepfake Detection(多注意的Deepfake检测)<br>
[paper](https://arxiv.org/abs/2103.02406)<br><br>



<br>

<a name="ObjectTracking"/> 

## 目标跟踪(Object Tracking)

[18] Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box Estimation(Alpha-Refine：通过精确的边界框估计来提高跟踪性能)<br>
[paper](https://arxiv.org/abs/2012.06815) | [code](https://github.com/MasterBin-IIAU/AlphaRefine)<br><br>

[17] LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search(LightTrack：通过一站式架构搜索找到用于跟踪对象的轻型神经网络)<br>
[paper](https://arxiv.org/abs/2104.14545) ｜ [code](https://github.com/researchmm/LightTrack)<br><br>

[16] Multiple Object Tracking with Correlation Learning(相关学习的多目标跟踪)<br>
[paper](https://arxiv.org/abs/2104.03541)<br><br>

[15] Learning to Track Instances without Video Annotations(学习在没有视频注释的情况下跟踪实例)<br>
[paper](https://arxiv.org/abs/2104.00287)<br><br>

[14] STMTrack: Template-free Visual Tracking with Space-time Memory Networks(具有时空存储网络的无模板视觉跟踪)<br>
[paper](https://arxiv.org/abs/2104.00324) | [code](https://github.com/fzh0917/STMTrack)<br><br>

[13] Online Multiple Object Tracking with Cross-Task Synergy(具有跨任务协同作用的在线多对象跟踪)<br>
[paper](https://arxiv.org/abs/2104.00380) | [code](https://github.com/songguocode/TADAM)<br><br>

[12] Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark(使用自然语言实现更灵活，准确的对象跟踪：算法和基准)<br>
[paper](https://arxiv.org/abs/2103.16746)<br><br>

[11] Learnable Graph Matching: Incorporating Graph Partitioning with Deep Feature Learning for Multiple Object Tracking(可学习的图匹配：将图分区与深度特征学习相结合以实现多对象跟踪)<br>
[paper](https://arxiv.org/abs/2103.16178) | [code](https://github.com/jiaweihe1996/GMTracker)<br><br>

[10] IoU Attack: Towards Temporally Coherent Black-Box Adversarial Attack for Visual Object Tracking(IoU攻击：针对视觉对象跟踪的临时相干黑盒对抗攻击)<br>
[paper](https://arxiv.org/abs/2103.14938) | [code](https://github.com/VISION-SJTU/IoUattack)<br><br>

[9] Transformer Tracking(Transformer跟踪)<br>
[paper](https://arxiv.org/abs/2103.15436) | [code](https://github.com/chenxin-dlut/TransT)<br><br>

[8] Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking(Transformer与追踪器相遇：利用时间上下文进行可靠的视觉追踪)<br>
[paper](https://arxiv.org/pdf/2103.11681)<br><br>

[7] Track to Detect and Segment: An Online Multi-Object Tracker(跟踪检测和分段：在线多目标跟踪器)<br>
[paper](https://arxiv.org/abs/2103.08808) | [code](https://jialianwu.com/projects/TraDeS.html)<br><br>

[6] Learning a Proposal Classifier for Multiple Object Tracking(用于多对象跟踪的分类器)<br>
[paper](https://arxiv.org/abs/2103.07889) | [code](https://github.com/daip13/LPC_MOT.git)<br><br>

[5] Center-based 3D Object Detection and Tracking(基于中心的3D目标检测和跟踪)<br>
[paper](https://arxiv.org/abs/2006.11275) | [code](https://github.com/tianweiy/CenterPoint)<br><br>

[4] HPS: localizing and tracking people in large 3D scenes from wearable sensors(通过可穿戴式传感器对大型3D场景中的人进行定位和跟踪)<br><br>

[3] Track to Detect and Segment: An Online Multi-Object Tracker(跟踪检测和分段：在线多对象跟踪器)<br>
[project](https://jialianwu.com/projects/TraDeS.html) | [video](https://www.youtube.com/watch?v=oGNtSFHRZJA)<br><br>

[2] Probabilistic Tracklet Scoring and Inpainting for Multiple Object Tracking(多目标跟踪的概率小波计分和修复)<br>
[paper](https://arxiv.org/abs/2012.02337)<br><br>

[1] Rotation Equivariant Siamese Networks for Tracking（旋转等距连体网络进行跟踪）<br>
[paper](https://arxiv.org/abs/2012.13078)<br><br>


<br><br>
<a name="ImageRetrieval"/> 

## 图像&视频检索/理解(Image&Video Retrieval/Video Understanding)


[11] VIGOR: Cross-View Image Geo-localization beyond One-to-one Retrieval(VIGOR：超越一对一检索的交叉视图图像地理定位)<br>
[paper](https://arxiv.org/abs/2011.12172) | [dataset&project](https://github.com/Jeff-Zilence/VIGOR)<br><br>

[10] Compatibility-aware Heterogeneous Visual Search(兼容感知的异构视觉搜索)<br>
[paper](https://arxiv.org/abs/2105.06047)<br><br>

[9] 2D or not 2D? Adaptive 3D Convolution Selection for Efficient Video Recognition(2D还是2D？ 自适应3D卷积选择以实现有效的视频识别)<br>
[paper](https://arxiv.org/abs/2012.14950)<br><br>

[8] FrameExit: Conditional Early Exiting for Efficient Video Recognition(【视频理解】帧退出：有条件提前退出以实现有效的视频识别)<br>
[paper](https://arxiv.org/abs/2104.13400)<br><br>

[7] T2VLAD: Global-Local Sequence Alignment for Text-Video Retrieval(T2VLAD：用于文本视频检索的全局局部序列比对)<br>
[paper](https://arxiv.org/abs/2104.10054)<br><br>

[6] Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers(快速思考和缓慢思考：使用变压器进行高效的文本到视觉检索)<br>
[paper](https://arxiv.org/abs/2103.16553)<br><br>

[5] StyleMeUp: Towards Style-Agnostic Sketch-Based Image Retrieval(StyleMeUp：迈向与风格无关的基于草图的图像检索)<br>
[paper](https://arxiv.org/abs/2103.15706)<br><br>

[4] More Photos are All You Need: Semi-Supervised Learning for Fine-Grained Sketch Based Image Retrieval(您只需要更多照片：基于半监督学习的细粒度基于草图的图像检索)<br>
[paper](https://arxiv.org/abs/2103.13990) | [code](https://github.com/AyanKumarBhunia/semisupervised-FGSBIR)<br><br>

[3] Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning(使用分层Transformer和自我监督学习改进跨模态食谱检索)<br>
[paper](https://arxiv.org/pdf/2103.13061.pdf)<br><br>

[2] On Semantic Similarity in Video Retrieval(视频检索中的语义相似度)<br>
[paper](https://arxiv.org/abs/2103.10095) ｜ [code](https://mwray.github.io/SSVR/)<br><br>

[1] QAIR: Practical Query-efficient Black-Box Attacks for Image Retrieval(实用的查询高效的图像检索黑盒攻击)<br>
[paper](https://arxiv.org/abs/2103.02927)<br><br>



<a name="ActionRecognition"/> 

### 行为识别/动作识别/检测/分割/定位(Action/Activity Recognition)

[25] Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization(通过对比交叉视图互信息最大化学习视图解开人体姿势表示)<br>
[paper](https://arxiv.org/abs/2012.01405) | [code](https://github.com/google-research/google-research/tree/master/poem)<br><br>

[24] Anticipating human actions by correlating past with the future with Jaccard similarity measures(用Jaccard相似性测度预测人类行为)<br>
[paper](https://arxiv.org/abs/2105.12414)<br><br>

[23] Collaborative Spatial-Temporal Modeling for Language-Queried Video Actor Segmentation(语言查询视频演员分割的协作时空建模)<br>
[paper](https://arxiv.org/abs/2105.06818)<br>

[22] Home Action Genome: Cooperative Compositional Action Understanding(家庭行动基因组：合作组成行动的理解)<br>
[paper](https://arxiv.org/abs/2105.05226)<br><br>

[21] Weakly Supervised Action Selection Learning in Video(视频中的弱监督动作选择学习)<br>
[paper](https://arxiv.org/abs/2105.02439) | [code](https://github.com/layer6ai-labs/ASL)<br><br>

[20] Global2Local: Efficient Structure Search for Video Action Segmentation()<br>
[paper](https://arxiv.org/abs/2101.00910) | [code](https://github.com/ShangHua-Gao/G2L-search)<br><br>

[19] Self-Supervised Learning for Semi-Supervised Temporal Action Proposal(自我监督学习的半监督时间行动建议)<br>
[paper](https://arxiv.org/abs/2104.03214)<br><br>

[18] Anchor-Constrained Viterbi for Set-Supervised Action Segmentation(锚约束维特比用于集合监督的动作分割)<br>
[paper](https://arxiv.org/abs/2104.02113)<br><br>

[17] Action Shuffle Alternating Learning for Unsupervised Action Segmentation(动作洗牌交替学习，实现无监督动作分割)<br>
[paper](https://arxiv.org/abs/2104.02116)<br><br>

[16] Self-supervised Motion Learning from Static Images(从静态图像进行自我监督的运动学习)<br>
[paper](https://arxiv.org/abs/2104.00240)<br><br>

[15] CoLA: Weakly-Supervised Temporal Action Localization with Snippet Contrastive Learning(带有片段对比学习的弱监督实时动作定位)
[paper](https://arxiv.org/abs/2103.16392)<br><br>

[14] Recognizing Actions in Videos from Unseen Viewpoints(从看不见的角度识别视频中的动作)<br>
[paper](https://arxiv.org/abs/2103.16516)<br><br>

[13] No frame left behind: Full Video Action Recognition(没有残影：完整的视频动作识别)<br>
[paper](https://arxiv.org/abs/2103.15395)<br><br>

[12] Learning Salient Boundary Feature for Anchor-free Temporal Action Localization(学习显着边界特征以实现无锚时间动作定位)<br>
[paper](https://arxiv.org/abs/2103.13137) | [code](https://github.com/TencentYoutuResearch/ActionDetection-AFSD)<br><br>

[11] Temporal Context Aggregation Network for Temporal Action Proposal Refinement(时间上下文聚合网络，用于改进时间行动建议)<br>
[paper](https://arxiv.org/abs/2103.13141)<br><br>

[10] The Blessings of Unlabeled Background in Untrimmed Videos(未修饰视频中未标记背景的祝福)<br>
[paper](https://arxiv.org/abs/2103.13183)<br><br>

[9] Temporally-Weighted Hierarchical Clustering for Unsupervised Action Segmentation(临时加权层次聚类，实现无监督动作分割)<br>
[paper](https://arxiv.org/abs/2103.11264) | [code](https://github.com/ssarfraz/FINCH-Clustering/tree/master/TW-FINCH)<br><br>

[8] Coarse-Fine Networks for Temporal Activity Detection in Videos(粗细网络，用于视频中的时间活动检测)<br>
[paper](https://arxiv.org/abs/2103.01302)<br><br>

[7] Learning Discriminative Prototypes with Dynamic Time Warping(通过动态时间扭曲学习判别性原型)<br>
[paper](https://arxiv.org/pdf/2103.09458.pdf)<br><br>

[6] Temporal Action Segmentation from Timestamp Supervision(时间监督中的时间动作分割)<br>
[paper](https://arxiv.org/abs/2103.06669)<br><br>

[5] ACTION-Net: Multipath Excitation for Action Recognition(用于动作识别的多路径激励)<br>
[paper](https://arxiv.org/abs/2103.07372) ｜ [code](https://github.com/V-Sense/ACTION-Net)<br><br>

[4] BASAR:Black-box Attack on Skeletal Action Recognition(骨骼动作识别的黑匣子攻击)<br>
[paper](https://arxiv.org/abs/2103.05266)<br><br>

[3] Understanding the Robustness of Skeleton-based Action Recognition under Adversarial Attack(了解对抗攻击下基于骨骼的动作识别的鲁棒性)<br>
[paper](https://arxiv.org/pdf/2103.05347.pdf)<br><br>

[2] Temporal Difference Networks for Efficient Action Recognition(用于有效动作识别的时差网络)<br>
[paper](https://arxiv.org/abs/2012.10071) | [code](https://github.com/MCG-NJU/TDN)<br><br>

[1] Behavior-Driven Synthesis of Human Dynamics(行为驱动的人类动力学综合)<br>
[paper](https://arxiv.org/pdf/2103.04677.pdf) | [code](https://compvis.github.io/behavior-driven-video-synthesis/)<>


<a name="Re-Identification"/> 

### 行人重识别/检测(Re-Identification/Detection)

[15] Generalizable Person Re-identification with Relevance-aware Mixture of Experts(具有相关性感知混合专家的可泛化的行人重识别)<br>
[paper](https://arxiv.org/abs/2105.09156)<br><br>

[14] Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification(通过基于记忆的多源元学习来学习概括看不见的域以进行人员重新识别)<br>
[paper](https://arxiv.org/abs/2012.00417)<br><br>

[13] Joint Generative and Contrastive Learning for Unsupervised Person Re-identification(联合生成和对比学习，用于无监督人员重新识别)<br>
[paper](https://arxiv.org/abs/2012.09071) | [code](https://github.com/chenhao2345/GCL)<br><br>

[12] Unsupervised Pre-training for Person Re-identification(对人员进行重新识别的无监督预训练)<br>
[paper](https://arxiv.org/abs/2012.03753)<br><br>

[11] BiCnet-TKS: Learning Efficient Spatial-Temporal Representation for Video Person Re-Identification(BiCnet-TKS：学习有效的时空表示以重新识别视频人)<br>
[paper](https://arxiv.org/abs/2104.14783)<br><br>

[10] Unsupervised Multi-Source Domain Adaptation for Person Re-Identification(用于行人重新识别的无监督多源域适配)<br>
[paper](https://arxiv.org/abs/2104.12961)<br><br>

[9] Combined Depth Space based Architecture Search For Person Re-identification(基于组合深度空间的架构搜索以进行行人重识别)<br>
[paper](https://arxiv.org/abs/2104.04163)<br><br>

[8] Neural Feature Search for RGB-Infrared Person Re-Identification(神经特征搜索以重新识别RGB红外人)<br>
[paper](https://arxiv.org/abs/2104.02366)<br><br>

[7] Group-aware Label Transfer for Domain Adaptive Person Re-identification(组感知标签传输，用于域自适应行人重识别)<br>
[paper](https://arxiv.org/abs/2103.12366)<br><br>

[6] Lifelong Person Re-Identification via Adaptive Knowledge Accumulation(通过自适应知识积累对终身行人重识别)<br>
[paper](https://arxiv.org/abs/2103.12462)<br><br>

[5] Anchor-Free Person Search(Anchor-Free行人搜索)<br>
[paper](https://arxiv.org/abs/2103.11617) | [code](https://github.com/daodaofr/AlignPS)<br><br>

[4] Intra-Inter Camera Similarity for Unsupervised Person Re-Identification(摄像机内部相似度用于无监督人员重新识别)<br>
[paper](https://arxiv.org/abs/2103.11658)<br><br>

[3] Watching You: Global-guided Reciprocal Learning for Video-based Person Re-identification(基于视频的人员重新识别的全球指导对等学习)<br>
[paper](https://arxiv.org/abs/2103.04337)<br><br>

[2] Joint Noise-Tolerant Learning and Meta Camera Shift Adaptation for Unsupervised Person Re-Identification(联合抗噪学习和元相机移位自适应，用于无监督人员的重新识别)<br>
[paper](https://arxiv.org/abs/2103.04618)<br><br>

[1] Meta Batch-Instance Normalization for Generalizable Person Re-Identification(通用批处理人员重新标识的元批实例规范化)<br>
[paper](https://arxiv.org/abs/2011.14670)<br><br>

<a name="VideoCaption"/> 

### 图像/视频字幕(Image/Video Caption)

[7] Towards Accurate Text-based Image Captioning with Content Diversity Exploration(借助内容多样性探索实现精确的基于文本的图像字幕)<br>
[paper](https://arxiv.org/abs/2105.03236)<br><br>

[6] Human-like Controllable Image Captioning with Verb-specific Semantic Roles(具有动词特定语义作用的类人可控图像字幕)<br>
[paper](https://arxiv.org/abs/2103.12204) | [code](https://github.com/mad-red/VSR-guided-CIC)<br><br>

[5] Co-Grounding Networks with Semantic Attention for Referring Expression Comprehension in Videos(语义注意的共同接地网络，用于引用视频中的表达理解)<br>
[paper](https://arxiv.org/pdf/2103.12346.pdf) | [project](https://sijiesong.github.io/co-grounding)<br><br>

[4] Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles(多实例字幕：从组织病理学教科书和文章中学习表示形式)<br>
[paper](https://arxiv.org/pdf/2103.05121.pdf)<br><br>

[3] Open-book Video Captioning with Retrieve-Copy-Generate Network(带有检索复制生成网络的开卷视频字幕)<br>
[paper](https://arxiv.org/pdf/2103.05284.pdf)<br><br>

[2] VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs(基于视频的文本生成的端到端学习来自多模式输入)<br>
[paper](https://arxiv.org/pdf/2101.12059.pdf)<br><br>

[1] Scan2Cap: Context-aware Dense Captioning in RGB-D Scans(：RGB-D扫描中的上下文感知密集字幕)
[paper](https://arxiv.org/abs/2012.02206) | [code](https://github.com/daveredrum/Scan2Cap) | [project](https://daveredrum.github.io/Scan2Cap/) | [video](https://youtu.be/AgmIpDbwTCY)<br><br>


<br>

<a name="MedicalImaging"/> 

## 医学影像(Medical Imaging)

[14] DARCNN: Domain Adaptive Region-based Convolutional Neural Network for Unsupervised Instance Segmentation in Biomedical Images(DARCNN：用于生物医学图像中无监督实例分割的基于域自适应区域的卷积神经网络)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Hsu_DARCNN_Domain_Adaptive_Region-Based_Convolutional_Neural_Network_for_Unsupervised_Instance_CVPR_2021_paper.pdf)<br><br>

[13] Every Annotation Counts: Multi-label Deep Supervision for Medical Image Segmentation(每种注释都至关重要：医学图像分割的多标签深度监管)<br>
[paper](https://arxiv.org/abs/2104.13243)<br><br>

[12] DiNTS: Differentiable Neural Network Topology Search for 3D Medical Image Segmentation(DiNTS：用于3D医学图像分割的可区分神经网络拓扑搜索)<br>
[paper](https://arxiv.org/abs/2103.15954)<br><br>

[11] Confluent Vessel Trees with Accurate Bifurcations(分叉的融合容器树)
[paper](https://arxiv.org/abs/2103.14268)<br><br>

[10] Brain Image Synthesis with Unsupervised Multivariate Canonical CSCℓ4Net(无监督多元规范CSCℓ4Net的脑图像合成)<br>
[paper](https://arxiv.org/pdf/2103.11587.pdf)<br><br>

[9] XProtoNet: Diagnosis in Chest Radiography with Global and Local Explanations(使用全局和局部解释诊断胸部X光片)<br>
[paper](https://arxiv.org/pdf/2103.10663.pdf)<br><br>

[8] FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space(在连续频率空间中通过情景学习进行医学图像分割的联合域泛化)<br>
[paper](https://arxiv.org/abs/2103.06030) | [code](https://github.com/liuquande/FedDG-ELCFS)<br><br>

[7] Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles(多实例字幕：从组织病理学教科书和文章中学习表示形式)<br>
[paper](https://arxiv.org/pdf/2103.05121.pdf)<br><br>

[6] Discovering Hidden Physics Behind Transport Dynamics(在运输动力学背后发现隐藏物理)<br>
[paper](https://arxiv.org/abs/2011.12222)<br><br>

[5] DeepTag: An Unsupervised Deep Learning Method for Motion Tracking on  Cardiac Tagging Magnetic Resonance Images(一种心脏标记磁共振图像运动跟踪的无监督深度学习方法)<br>
[paper](https://arxiv.org/abs/2103.02772)<br><br>

[4] Multi-institutional Collaborations for Improving Deep Learning-based Magnetic Resonance Image Reconstruction Using Federated Learning(多机构协作改进基于深度学习的联合学习磁共振图像重建)<br>
[paper](https://arxiv.org/abs/2103.02148) | [code](https://github.com/guopengf/FLMRCM)<br><br>

[3] 3D Graph Anatomy Geometry-Integrated Network for Pancreatic Mass Segmentation, Diagnosis, and Quantitative Patient Management(用于胰腺肿块分割，诊断和定量患者管理的3D图形解剖学几何集成网络)<br><br>

[2] Deep Lesion Tracker: Monitoring Lesions in 4D Longitudinal Imaging Studies(深部病变追踪器：在4D纵向成像研究中监控病变)<br>
[paper](https://arxiv.org/abs/2012.04872)<br><br>

[1] Automatic Vertebra Localization and Identification in CT by Spine Rectification and Anatomically-constrained Optimization(通过脊柱矫正和解剖学约束优化在CT中自动进行椎骨定位和识别)<br>
[paper](https://arxiv.org/abs/2012.07947)<br><br>

<br>

<a name="TDR"/> 


## 文本检测/识别(Text Detection/Recognition)

[7] TextOCR: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text(TextOCR：对任意形状的场景文本进行大规模的端到端推理)<br>
[paper](https://arxiv.org/abs/2105.05486) | [project](https://textvqa.org/textocr)<br><br>

[6] Fourier Contour Embedding for Arbitrary-Shaped Text Detection(基于Fourier轮廓嵌入的任意形状文本检测)<br>
[paper](https://arxiv.org/abs/2104.10442)<br><br>

[5] Scene Text Retrieval via Joint Text Detection and Similarity Learning(通过联合文本检测和相似性学习检索场景文本)<br>
[paper](https://arxiv.org/abs/2104.01552) | [code](https://github.com/lanfeng4659/STR-TDSL)<br><br>

[4] MetaHTR: Towards Writer-Adaptive Handwritten Text Recognition(迈向写作者自适应的手写文本识别)<br>
[paper](https://arxiv.org/abs/2104.01876)<br><br>

[3] MOST: A Multi-Oriented Scene Text Detector with Localization Refinement(具有本地化优化功能的多方位场景文本检测器)<br>
[paper](https://arxiv.org/abs/2104.01070)<br><br>

[2] Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition(像人类一样阅读：用于场景文本识别的自主，双向和迭代语言建模)<br>
[paper](https://arxiv.org/abs/2103.06495) | [code](https://github.com/FangShancheng/ABINet)<br><br>

[1] What If We Only Use Real Datasets for Scene Text Recognition? Toward Scene Text Recognition With Fewer Labels(如果我们仅将真实数据集用于场景文本识别该怎么办？ 带有较少标签的场景文本识别)<br>
[paepr](https://arxiv.org/abs/2103.04400) | [code](https://github.com/ku21fan/STR-Fewer-Labels)<br><br>

<br>

<a name="RSI"/> 

## 遥感图像(Remote Sensing Image)

[3] SIPSA-Net: Shift-Invariant Pan Sharpening with Moving Object Alignment for Satellite Imagery(SIPSA-Net：带有移动目标对准的卫星图像平移不变锐化)<br>
[paper](https://arxiv.org/abs/2105.02400)<br><br>

[2] PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation(语义流经点以进行航空图像分割)<br>
[paper](https://arxiv.org/pdf/2103.06564.pdf)<br><br>

[1] Deep Gradient Projection Networks for Pan-sharpening(【超分辨率】泛锐化的深梯度投影网络)<br>
[paper](https://arxiv.org/pdf/2103.04584.pdf) | [code](https://github.com/xsxjtu/GPPNN)<br><br>


<br>

<a name="GAN"/> 

## GAN/生成式/对抗式(GAN/Generative/Adversarial)

[27] Enhancing the Transferability of Adversarial Attacks through Variance Tuning(通过方差调整增强对抗性攻击的可转移性)<br>
[paper](https://arxiv.org/abs/2103.15571) | [code](https://github.com/JHL-HUST/VT)<br><br>

[26] HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms(HistoGAN：通过颜色直方图控制 GAN 生成的图像和真实图像的颜色)<br>
[paper](https://arxiv.org/abs/2011.11731)<br><br>

[25] Continuous Face Aging via Self-estimated Residual Age Embedding(通过自我估计的残差年龄嵌入来实现连续的面部老化)<br>
[paper](https://arxiv.org/abs/2105.00020)<br><br>

[24] Unsupervised 3D Shape Completion through GAN Inversion(通过GAN反演实现无监督3D形状补全)<br>
[paper](https://arxiv.org/abs/2104.13366) | [project](https://junzhezhang.github.io/projects/ShapeInversion/)<br><br>

[23] Delving into Data: Effectively Substitute Training for Black-box Attack(深入研究数据：有效替代黑盒攻击的培训)<br>
[paper](https://arxiv.org/abs/2104.12378)<br><br>

[22] LAFEAT: Piercing Through Adversarial Defenses with Latent Features(LAFEAT：通过具有潜在功能的对抗性防御突围)<br>
[paper](https://arxiv.org/abs/2104.09284)<br><br>

[21] Surrogate Gradient Field for Latent Space Manipulation(潜在空间操纵的替代梯度场)<br>
[paper](https://arxiv.org/abs/2104.09065)<br><br>

[20] DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort(DatasetGAN：只需最少的人力即可获得的高效标签数据工厂)<br>
[paper](https://arxiv.org/abs/2104.06490)<br><br>

[19] Regularizing Generative Adversarial Networks under Limited Data(在有限数据下对生成性对抗网络进行正则化)<br>
[paper](https://arxiv.org/abs/2104.03310) | [project](https://hytseng0509.github.io/lecam-gan) | [code](https://github.com/google/lecam-gan)<br><br>

[18] Content-Aware GAN Compression(内容感知GAN压缩)<br>
[paper](https://arxiv.org/abs/2104.02244)<br><br>

[17] Lipstick ain't enough: Beyond Color Matching for In-the-Wild Makeup Transfer(口红还不够：超出配色范围的野外化妆效果)<br>
[paper](https://arxiv.org/abs/2104.01867) | [code](https://github.com/VinAIResearch/CPM)<br><br>

[16] LiBRe: A Practical Bayesian Approach to Adversarial Detection(LiBRe：对抗性检测的实用贝叶斯方法)<br>
[paper](https://arxiv.org/abs/2103.14835)<br><br>

[15] DivCo: Diverse Conditional Image Synthesis via Contrastive Generative Adversarial Network(通过对比生成对抗网络进行多种条件图像合成)<br>
[paper](https://arxiv.org/abs/2103.07893)<br><br>

[14] Diverse Semantic Image Synthesis via Probability Distribution Modeling(基于概率分布建模的多种语义图像合成)<br>
[paper](https://arxiv.org/abs/2103.06878) | [code](https://github.com/tzt101/INADE.git)<br><br>

[13] HumanGAN: A Generative Model of Humans Images(人类图像的生成模型)<br>
[paper](https://arxiv.org/abs/2103.06902)<br><br>

[12] MetaSimulator: Simulating Unknown Target Models for Query-Efficient Black-box Attacks(模拟未知目标模型以提高查询效率的黑盒攻击)<br>
[paper](https://arxiv.org/abs/2009.00960) | [code](https://github.com/machanic/MetaSimulator)<br><br>

[11] Soft-IntroVAE: Analyzing and Improving Introspective Variational Autoencoders(分析和改进自省变分自动编码器)<br>
[paper](https://arxiv.org/pdf/2012.13253.pdf) | [code](https://github.com/taldatech/soft-intro-vae-pytorch) | [project](https://taldatech.github.io/soft-intro-vae-web/)<br><br>

[10] LOHO: Latent Optimization of Hairstyles via Orthogonalization(LOHO：通过正交化潜在地优化发型)<br>
[paper](https://arxiv.org/pdf/2103.03891.pdf)<br><br>

[9] PISE: Person Image Synthesis and Editing with Decoupled GAN(使用分离的GAN进行人像合成和编辑)<br>
[paper](https://arxiv.org/abs/2103.04023) | [code](https://github.com/Zhangjinso/PISE)<br><br>

[8] Closed-Form Factorization of Latent Semantics in GANs(GAN中潜在语义的闭式分解)<br>
[paper](https://arxiv.org/abs/2007.06600) | [code](https://github.com/genforce/sefa)<br><br>

[7] PD-GAN: Probabilistic Diverse GAN for Image Inpainting(用于图像修复的概率多样GAN)<br><br>

[6] Anycost GANs for Interactive Image Synthesis and Editing(用于交互式图像合成和编辑的AnyCost Gans)<br>
[paper](https://arxiv.org/abs/2103.03243) | [code](https://github.com/mit-han-lab/anycost-gan)<br><br>

[5] Efficient Conditional GAN Transfer with Knowledge Propagation across Classes(高效的有条件GAN转移以及跨课程的知识传播)<br>
[paper](https://arxiv.org/pdf/2102.06696.pdf) | [code](http://github.com/mshahbazi72/cGANTransfer)<br><br>

[4] Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing（利用GAN中潜在的空间维度进行实时图像编辑）<br><br>

[3] Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs(Hijack-GAN：意外使用经过预训练的黑匣子GAN)<br>
[paper](https://arxiv.org/pdf/2011.14107.pdf)<br><br>

[2] Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation(样式编码：用于图像到图像翻译的StyleGAN编码器)<br>
[paper](https://arxiv.org/abs/2008.00951) | [code](https://github.com/eladrich/pixel2style2pixel) | [project](https://eladrich.github.io/pixel2style2pixel/)<br><br>

[1] A 3D GAN for Improved Large-pose Facial Recognition(用于改善大姿势面部识别的3D GAN)<br>
[paper](https://arxiv.org/pdf/2012.10545.pdf)<br><br>


<br>

<a name="IGIS"/> 

### 图像生成/图像合成(Image Generation/Image Synthesis)

[26] Self-Supervised Collision Handling via Generative 3D Garment Models for Virtual Try-On(通过虚拟试穿的生成式3D服装模型进行自我监督的碰撞处理)<br>
[paper](https://arxiv.org/abs/2105.06462) ｜ [project](http://mslab.es/projects/SelfSupervisedGarmentCollisions)<br><br>

[25] Roof-GAN: Learning to Generate Roof Geometry and Relations for Residential Houses(Roof-GAN：学习为住宅生成屋顶几何图形和关系)<br>
[paper](https://arxiv.org/abs/2012.09340) | [code](https://github.com/yi-ming-qian/roofgan)<br><br>

[24] TediGAN: Text-Guided Diverse Face Image Generation and Manipulation(TediGAN：文本引导的多样面部图像生成和操纵)<br>
[paper](https://arxiv.org/abs/2012.03308) | [code](https://github.com/weihaox/TediGAN) | [video](https://youtu.be/L8Na2f5viAM)<br><br>

[23] GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving(GeoSim：通过可自动驾驶的几何感知合成进行逼真的视频模拟)<br>
[paper](https://arxiv.org/abs/2101.06543)<br><br>

[22] GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields(GIRAFFE：将场景表示为合成的生成神经特征场)<br>
[paper](https://arxiv.org/abs/2011.12100) | [project](http://bit.ly/giraffe-project)<br><br>

[21] Ensembling with Deep Generative Views(融入深刻的生成观点)<br>
[paper](https://arxiv.org/abs/2104.14551) | [code](https://github.com/chail/gan-ensembling)<br><br>

[20] StylePeople: A Generative Model of Fullbody Human Avatars(StylePeople：全身人类化身的生成模型)<br>
[paper](https://arxiv.org/abs/2104.08363) | [code](http://saic-violet.github.io/style-people)<br>

[19] See through Gradients: Image Batch Recovery via GradInversion(透视渐变：通过GradInversion恢复图像批处理)<br>
[paper](https://arxiv.org/abs/2104.07586)<br><br>

[18] StEP: Style-based Encoder Pre-training for Multi-modal Image Synthesis(StEP：用于多模式图像合成的基于样式的编码器预训练)<br>
[paper](https://arxiv.org/abs/2104.07098)<br><br>

[17] Few-shot Image Generation via Cross-domain Correspondence(通过跨域对应小样本图像生成)<br>
[paper](https://arxiv.org/abs/2104.06820)<br><br>

[16] IMAGINE: Image Synthesis by Image-Guided Model Inversion(想象：通过图像指导模型反演的图像合成)<br>
[paper](https://arxiv.org/abs/2104.05895)<br><br>

[15] Variational Transformer Networks for Layout Generation(用于布局生成的变电站网络)<br>
[paper](https://arxiv.org/abs/2104.02416)<br><br>

[14] VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization(通过未对准感知的归一化进行高分辨率的虚拟试戴)<br>
[paper](https://arxiv.org/abs/2103.16874)<br><br>

[13] A Closer Look at Fourier Spectrum Discrepancies for CNN-generated Images Detection(仔细研究CNN生成图像检测的傅立叶光谱差异)<br>
[paper](https://arxiv.org/abs/2103.17195) | [code](https://keshik6.github.io/Fourier-Discrepancies-CNN-Detection/)<br><br>

[12] Semi-supervised Synthesis of High-Resolution Editable Textures for 3D Humans(用于3D人类的高分辨率可编辑纹理的半监督合成)<br>
[paper](https://arxiv.org/abs/2103.17266)<br><br>

[11] Few-Shot Human Motion Transfer by Personalized Geometry and Texture Modeling(个性化几何和纹理建模的少量人体运动传递)<br>
[paper](https://arxiv.org/abs/2103.14338) | [code](https://github.com/HuangZhiChao95/FewShotMotionTransfer)<br><br>

[10] Brain Image Synthesis with Unsupervised Multivariate Canonical CSCℓ4Net(无监督多元规范CSCℓ4Net的脑图像合成)<br>
[paper](https://arxiv.org/pdf/2103.11587.pdf)<br><br>

[9] Context-Aware Layout to Image Generation with Enhanced Object Appearance(具有增强的对象外观的上下文感知布局到图像生成)<br>
[paper](https://arxiv.org/abs/2103.11897)<br><br>

[8] DivCo: Diverse Conditional Image Synthesis via Contrastive Generative Adversarial Network(通过对比生成对抗网络进行多种条件图像合成)<br>
[paper](https://arxiv.org/abs/2103.07893)<br><br>

[7] HumanGAN: A Generative Model of Humans Images(人类图像的生成模型)<br>
[paper](https://arxiv.org/abs/2103.06902)<br><br>

[6] PISE: Person Image Synthesis and Editing with Decoupled GAN(使用分离的GAN进行人像合成和编辑)<br>
[paper](https://arxiv.org/abs/2103.04023) | [code](https://github.com/Zhangjinso/PISE)<br><br>

[5] SMPLicit: Topology-aware Generative Model for Clothed People(穿衣服的人的拓扑感知生成模型)<br>
[paper](https://arxiv.org/pdf/2103.06871.pdf) | [code](http://www.iri.upc.edu/people/ecorona/smplicit/)<br><br>

[4] Diversifying Sample Generation for Data-Free Quantization（多样化的样本生成，实现无数据量化）<br>
[paper](https://arxiv.org/abs/2103.01049)<br><br>

[3] Diverse Semantic Image Synthesis via Probability Distribution Modeling(基于概率分布建模的多种语义图像合成)<br>
[paper](https://arxiv.org/abs/2103.06878) | [code](https://github.com/tzt101/INADE.git)<br><br>

[2] When Age-Invariant Face Recognition Meets Face Age Synthesis: A  Multi-Task Learning Framework(当年龄不变的人脸识别遇到人脸年龄合成时：一个多任务学习框架)<br>
[paper](https://arxiv.org/abs/2103.01520) | [code](https://github.com/Hzzone/MTLFace)<br><br>

[1] Anycost GANs for Interactive Image Synthesis and Editing(用于交互式图像合成和编辑的AnyCost Gans)<br>
[paper](https://arxiv.org/abs/2103.03243) | [code](https://github.com/mit-han-lab/anycost-gan)<br><br>

<a name="ViewSynthesis"/> 

### 视图合成(View Synthesis)

[7] Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes(用于动态场景时空视图合成的神经场景流场)<br>
[paper](https://arxiv.org/abs/2011.13084) | [project](http://www.cs.cornell.edu/~zl548/NSFF/)<>

[6] Stable View Synthesis(稳定的视图合成)<br>
[paper](https://arxiv.org/abs/2011.07233) | [code](https://github.com/intel-isl/StableViewSynthesis)<br><br>

[5] Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views of Novel Scenes(立体辐射场（SRF）：学习新颖的场景的稀疏视图的视图合成)<br>
[paper](https://arxiv.org/abs/2104.06935) | [project](https://virtualhumans.mpi-inf.mpg.de/srf/)<br><br>

[4] Layout-Guided Novel View Synthesis from a Single Indoor Panorama(单一室内全景的布局引导式新颖视图合成)<br>
[paper](https://arxiv.org/abs/2103.17022) | [project](https://github.com/bluestyle97/PNVS)<br><br>

[3] NeX: Real-time View Synthesis with Neural Basis Expansion(NeX：具有神经基础扩展的实时视图合成)<br>
[paper](https://arxiv.org/abs/2103.05606) | [code](https://nex-mpi.github.io/)<br><br>

[2] ID-Unet: Iterative Soft and Hard Deformation for View Synthesis(视图合成的迭代软硬变形)<br>
[paper](https://arxiv.org/abs/2103.02264)<br><br>

[1] Self-Supervised Visibility Learning for Novel View Synthesis(自我监督的可视性学习，用于新颖的视图合成)<br>
[paper](https://arxiv.org/abs/2103.15407)<br><br>

<br>

<a name="3DVision"/> 

## 三维视觉(3D Vision)

[8] 3D Spatial Recognition without Spatially Labeled 3D(没有空间标记的3D的3D空间识别)<br>
[paper](https://arxiv.org/abs/2105.06461)<br><br>

[7] Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid Representations(等值点：使用混合表示优化神经隐式曲面)<br>
[paper](https://arxiv.org/abs/2012.06434) | [code](https://github.com/yifita/iso-points)<br><br>

[6] Learning Feature Aggregation for Deep 3D Morphable Models(深度3D可变形模型的学习特征聚合)<br>
[paper](https://arxiv.org/abs/2105.02173)<br><br>

[5] Deep Polarization Imaging for 3D shape and SVBRDF Acquisition(用于3D形状和SVBRDF采集的深偏振成像)<br>
[paper](https://arxiv.org/abs/2105.02875)<br><br>

[4] Unsupervised 3D Shape Completion through GAN Inversion(通过GAN反演实现无监督3D形状补全)<br>
[paper](https://arxiv.org/abs/2104.13366) | [project](https://junzhezhang.github.io/projects/ShapeInversion/)<br><br>

[3] KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control(【3D关键点】关键点变形器：用于形状控制的无监督三维关键点发现)<br>
[paper](https://arxiv.org/abs/2104.11224) | [project](http://tomasjakab.github.io/KeypointDeformer)<br><br>

[2] A Deep Emulator for Secondary Motion of 3D Characters(三维角色二次运动的深度仿真器)
[paper](https://arxiv.org/abs/2103.01261)

[1] 3D CNNs with Adaptive Temporal Feature Resolutions(具有自适应时间特征分辨率的3D CNN)<br>
[paper](https://arxiv.org/abs/2011.08652)<br><br>

<a name="3DPC"/> 

### 点云(Point Cloud)

[32] Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR Segmentation([点云分割]用于 LiDAR 分割的圆柱形和非对称 3D 卷积网络)<br>
[paper](https://arxiv.org/abs/2011.10033) ｜ [code](https://github.com/xinge008/Cylinder3D)<br><br>

[31] DyCo3D: Robust Instance Segmentation of 3D Point Clouds through Dynamic Convolution(DyCo3D：通过动态卷积对 3D 点云进行稳健的实例分割)<br>
[paper](https://arxiv.org/abs/2011.13328) | [code](https://git.io/DyCo3D)<br><br>

[30] PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of Point Clouds(PV-RAFT：用于点云场景流估计的点体素相关字段)<br>
[paper](https://arxiv.org/abs/2012.00987)<br><br>

[29] PWCLO-Net: Deep LiDAR Odometry in 3D Point Clouds Using Hierarchical Embedding Mask Optimization(PWCLO-Net：使用分层嵌入掩码优化的 3D 点云中的深度激光雷达测距)<br>
[paper](https://arxiv.org/abs/2012.00972) | [code](https://github.com/IRMVLab/PWCLONet)<br><br>

[28] Omni-supervised Point Cloud Segmentation via Gradual Receptive Field Component Reasoning(通过渐进感受野分量推理的全方位监督点云分割)<br>
[paper](https://arxiv.org/abs/2105.10203) | [code](https://github.com/azuki-miho/RFCR)<br><br>

[27] PMP-Net: Point Cloud Completion by Learning Multi-step Point Moving Paths(PMP-Net：通过学习多步点移动路径来完成点云)<br>
[paper](https://arxiv.org/abs/2012.03408)<br><br>

[26] VoxelContext-Net: An Octree based Framework for Point Cloud Compression(VoxelContext-Net：基于Octree的点云压缩框架)<br>
[paper](https://arxiv.org/abs/2105.02158)<br><br>

[25] Variational Relational Point Completion Network(变分关系点完备网络)<br>
[paper](https://arxiv.org/abs/2104.10154) | [project](https://paul007pl.github.io/projects/VRCNet.html)<br><br>

[24] SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements(规模：使用关节局部元素的表面编解码器模拟穿衣服的人)<br>
[paper](https://arxiv.org/abs/2104.07660) | [code](https://qianlim.github.io/SCALE)<br><br>

[23] RPSRNet: End-to-End Trainable Rigid Point Set Registration Network using Barnes-Hut 2D-Tree Representation(RPSRNet：使用Barnes-Hut二维树表示法的端到端可训练刚性点集配准网络)<br>
[paper](https://arxiv.org/abs/2104.05328)<br><br>

[22] View-Guided Point Cloud Completion(视图引导的点云完成)<br>
[paper](https://arxiv.org/abs/2104.05666)<br><br>

[21] DeepI2P: Image-to-Point Cloud Registration via Deep Classification(通过深度分类的图像到点云配准)<br>
[paper](https://arxiv.org/abs/2104.03501) | [code](https://github.com/lijx10/DeepI2P)<br><br>

[20] FESTA: Flow Estimation via Spatial-Temporal Attention for Scene Point Clouds(FESTA：场景点云通过时空注意进行光流估计)<br>
[paper](https://arxiv.org/abs/2104.00798)<br><br>

[19] Denoise and Contrast for Category Agnostic Shape Completion(类别不可知形状完成的消噪和对比度)<br>
[paper](https://arxiv.org/abs/2103.16671)<br><br>

[18] Panoptic-PolarNet: Proposal-free LiDAR Point Cloud Panoptic Segmentation(无提案的LiDAR点云全景分割)<br>
[paper](https://arxiv.org/pdf/2103.14962.pdf)<br><br>

[17] ReAgent: Point Cloud Registration using Imitation and Reinforcement Learning(ReAgent：使用模仿和强化学习进行点云配准)<br>
[paper](https://arxiv.org/abs/2103.15231)<br><br>

[16] Equivariant Point Network for 3D Point Cloud Analysis(等变点网络进行3D点云分析)<br>
[paper](https://arxiv.org/abs/2103.14147)<br><br> 

[15] PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on Point Clouds(PAConv：点云上具有动态内核组装的位置自适应卷积)<br>
[paper](https://arxiv.org/abs/2103.14635) | [code](https://github.com/CVMI-Lab/PAConv)<br><br> 

[14] Skeleton Merger: an Unsupervised Aligned Keypoint Detector(骨架合并：无监督的对准关键点检测器)<br>
[paper](https://arxiv.org/pdf/2103.10814.pdf) | [code](https://github.com/eliphatfs/SkeletonMerger)<br><br>

[13] Cycle4Completion: Unpaired Point Cloud Completion using Cycle Transformation with Missing Region Coding(使用缺失区域编码的循环变换完成不成对的点云)<br>
[paper](https://arxiv.org/abs/2103.07838)<br><br>

[12] Semantic Segmentation for Real Point Cloud Scenes via Bilateral Augmentation and Adaptive Fusion(通过双边扩充和自适应融合对实点云场景进行语义分割)<br>
[paper](https://arxiv.org/abs/2103.07074)<br><br>

[11] How Privacy-Preserving are Line Clouds? Recovering Scene Details from 3D Lines(线云如何保护隐私？ 从3D线中恢复场景详细信息)<br>
[paper](https://arxiv.org/pdf/2103.05086.pdf) | [code](https://github.com/kunalchelani/Line2Point)<br><br>

[10] PointDSC: Robust Point Cloud Registration using Deep Spatial Consistency(使用深度空间一致性进行稳健的点云配准)<br>
[paper](https://arxiv.org/abs/2103.05465) | [code](https://github.com/XuyangBai/PointDSC)<br><br>

[9] Robust Point Cloud Registration Framework Based on Deep Graph Matching(基于深度图匹配的鲁棒点云配准框架)<br>
[paper](https://arxiv.org/pdf/2103.04256.pdf) | [code](https://github.com/fukexue/RGM)<br><br>

[8] TPCN: Temporal Point Cloud Networks for Motion Forecasting(面向运动预测的时态点云网络)
[paper](https://arxiv.org/abs/2103.03067) | [code]()

[7] PointGuard: Provably Robust 3D Point Cloud Classification(可证明稳健的三维点云分类)<br>
[paper](https://arxiv.org/abs/2103.03046)<br><br>

[6] Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset, Benchmarks and Challenges(走向城市规模3D点云的语义分割：数据集，基准和挑战)<br>
[paper](https://arxiv.org/abs/2009.03137) | [code](https://github.com/QingyongHu/SensatUrban)<br><br>

[5] SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration(SpinNet：学习用于3D点云配准的通用表面描述符)<br>
[paper](https://arxiv.org/abs/2011.12149) | [code](https://github.com/QingyongHu/SpinNet)<br><br>

[4] MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan Synchronization(通过3D扫描同步进行多主体分割和运动估计)<br>
[paper](https://arxiv.org/pdf/2101.06605.pdf) | [code](https://github.com/huangjh-pub/multibody-sync)<br><br>

[3] Diffusion Probabilistic Models for 3D Point Cloud Generation(三维点云生成的扩散概率模型)<br>
[paper](https://arxiv.org/abs/2103.01458) | [code](https://github.com/luost26/diffusion-point-cloud)<br><br>

[2] Style-based Point Generator with Adversarial Rendering for Point Cloud Completion(用于点云补全的对抗性渲染基于样式的点生成器)<br>
[paper](https://arxiv.org/abs/2103.02535)

[1] PREDATOR: Registration of 3D Point Clouds with Low Overlap(预测器：低重叠的3D点云的配准)<br>
[paper](https://arxiv.org/pdf/2011.13005.pdf) | [code](https://github.com/ShengyuH/OverlapPredator) | [project](https://overlappredator.github.io/)<br><br>


<a name="3DReconstruction"/> 

### 三维重建(3D Reconstruction)

[25] 3D-NVS: A 3D Supervision Approach for Next View Selection(3D-NVS：下一个视图选择的 3D 监督方法)<br>
[paper](https://arxiv.org/abs/2012.01743)<br><br>

[24] Multi-view 3D Reconstruction of a Texture-less Smooth Surface of Unknown Generic Reflectance(未知通用反射率的无纹理光滑表面的多视图 3D 重建)<br>
[paper](https://arxiv.org/abs/2105.11599)<br><br>

[23] Sketch2Model: View-Aware 3D Modeling from Single Free-Hand Sketches(Sketch2Model：从单个徒手草图开始的具有视图感知能力的3D建模)<br>
[paper](https://arxiv.org/abs/2105.06663)<br><br>

[22] From Points to Multi-Object 3D Reconstruction(从点到多对象3D重建)<br>
[paper](https://arxiv.org/abs/2012.11575)<br><br>

[21] End-to-End Human Pose and Mesh Reconstruction with Transformers(使用Transformer进行端到端的人体姿势和网格重建)<br>
[paper](https://arxiv.org/abs/2012.09760)<br><br>

[20] DECOR-GAN: 3D Shape Detailization by Conditional Refinement(DECOR-GAN：通过条件细化实现3D形状细化)<br>
[paper](https://arxiv.org/abs/2012.09159) | [code](https://github.com/czq142857/DECOR-GAN)<br><br>

[19] pixelNeRF: Neural Radiance Fields from One or Few Images(pixelNeRF：一幅或几幅图像的神经辐射场)<br>
[paper](https://arxiv.org/abs/2012.02190) | [project](https://alexyu.net/pixelnerf)<br><br>

[18] LASR: Learning Articulated Shape Reconstruction from a Monocular Video(LASR：从单眼视频中学习关节形状的重建)<br>
[paper](https://arxiv.org/abs/2105.02976) | [code](http://lasr-google.github.io/)<br><br>

[17] Cuboids Revisited: Learning Robust 3D Shape Fitting to Single RGB Images(重访长方体：学习适合单个RGB图像的稳健3D形状)<br>
[paper](https://arxiv.org/abs/2105.02047)<br><br>

[16] Multi-person Implicit Reconstruction from a Single Image(从单个图像进行多人隐式重建)<br>
[paper](https://arxiv.org/abs/2104.09283)<br><br>

[15] CodedStereo: Learned Phase Masks for Large Depth-of-field Stereo(CodedStereo：为大景深立体声而设计的相位掩模)<br>
[paper](https://arxiv.org/abs/2104.04641)<br><br>

[14] StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision(通过立体视觉进行深度感知的布景人类数字化)<br>
[paper](https://arxiv.org/abs/2104.05289) | [projec](http://crishy1995.github.io/StereoPIFuProject)<br><br>

[13] Global Transport for Fluid Reconstruction with Learned Self-Supervision(具有自学指导的流体重建的全球运输)<br>
[paper](https://arxiv.org/abs/2104.06031) | [code](https://github.com/tum-pbs/Global-Flow-Transport)<br><br>

[12] Fully Understanding Generic Objects: Modeling, Segmentation, and Reconstruction(全面了解通用对象：建模，分段和重构)<br>
[paper](https://arxiv.org/abs/2104.00858)<br><br>

[11] Reconstructing 3D Human Pose by Watching Humans in the Mirror(通过照镜子中的人来重建3D人的姿势)<br>
[paper](https://arxiv.org/abs/2104.00340) | [project](https://zju3dv.github.io/Mirrored-Human/)<br><br>

[10] Fostering Generalization in Single-view 3D Reconstruction by Learning a Hierarchy of Local and Global Shape Priors(通过学习局部和全局形状先验的层次结构，促进单视图3D重构中的泛化)<br>
[paper](https://arxiv.org/abs/2104.00476)<br><br>

[9] NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video(单目视频的实时相干3D重建)<br>
[paper](https://arxiv.org/abs/2104.00681) | [project](https://zju3dv.github.io/neuralrecon/)<br><br>

[8] Learning Parallel Dense Correspondence from Spatio-Temporal Descriptors for Efficient and Robust 4D Reconstruction(从时空描述符中学习并行密集对应，以进行有效且鲁棒的4D重建)<br>
[paper](https://arxiv.org/abs/2103.16341) | [code](https://github.com/tangjiapeng/LPDC-Net)<br><br>

[7] POSEFusion: Pose-guided Selective Fusion for Single-view Human Volumetric Capture(用于单视图人体体积捕获的姿势引导选择性融合)<br>
[paper](https://arxiv.org/abs/2103.15331) | [project](http://www.liuyebin.com/posefusion/posefusion.html)<br><br>

[6] Deep Implicit Moving Least-Squares Functions for 3D Reconstruction(用于3D重构的深层隐式移动最小二乘函数)<br>
[paper](https://arxiv.org/abs/2103.12266) | [code](https://github.com/Andy97/DeepMLS)<br><br>

[5] Model-based 3D Hand Reconstruction via Self-Supervised Learning(通过自我监督学习进行基于模型的3D手重建)<br>
[paper](https://arxiv.org/pdf/2103.11703)<br><br>

[4] 3DCaricShop: A Dataset and A Baseline Method for Single-view 3D Caricature Face Reconstruction(单视图3D漫画面部重建的数据集和基线方法)<br>
[paper](https://arxiv.org/pdf/2103.08204.pdf) | [project](https://qiuyuda.github.io/3DCaricShop/)<br><br>

[3] Learning Compositional Representation for 4D Captures with Neural ODE(使用神经ODE学习4D捕捉的合成表示)<br>
[paper](https://arxiv.org/pdf/2103.08271.pdf)<br><br>

[2] SMPLicit: Topology-aware Generative Model for Clothed People(穿衣服的人的拓扑感知生成模型)<br>
[paper](https://arxiv.org/pdf/2103.06871.pdf) | [code](http://www.iri.upc.edu/people/ecorona/smplicit/)<br><br>

[1] PCLs: Geometry-aware Neural Reconstruction of 3D Pose with Perspective Crop Layers（具有透视作物层的3D姿势的几何感知神经重建）<br>
[paper](https://arxiv.org/abs/2011.13607)<br><br>

<br>

<a name="ModelCompression"/> 

## 模型压缩(Model Compression)

[6] Towards Compact CNNs via Collaborative Compression(通过协同压缩迈向紧凑型CNN)<br>
[paper](https://arxiv.org/abs/2105.11228)<br><br>

[5] Joint-DetNAS: Upgrade Your Detector with NAS, Pruning and Dynamic Distillation(联合DetNAS：用NAS、剪枝和动态蒸馏升级你的探测器)<br>
[paper](https://arxiv.org/abs/2105.12971)<br><br>

[4] Skip-Convolutions for Efficient Video Processing(跳过卷积以实现高效的视频处理)<br>
[paper](https://arxiv.org/abs/2104.11487)<br><br>

[3] Content-Aware GAN Compression(内容感知GAN压缩)<br>
[paper](https://arxiv.org/abs/2104.02244)<br><br>

[2] Dynamic Slimmable Network(动态可压缩网络)<br>
[paper](https://arxiv.org/abs/2103.13258) | [code](https://github.com/changlin31/DS-Net)<br><br>

[1] Learning Student Networks in the Wild（一种不需要原始训练数据的模型压缩和加速技术）<br>
[paper](https://arxiv.org/pdf/1904.01186.pdf) | [code](https://github.com/huawei-noah/DAFL)<br>
解读：[华为诺亚方舟实验室提出无需数据网络压缩技术](https://zhuanlan.zhihu.com/p/81277796)<br><br>

<a name="KnowledgeDistillation"/> 

### 知识蒸馏(Knowledge Distillation)

[13] Positive-Unlabeled Data Purification in the Wild for Object Detection(野外检测对象的阳性无标签数据提纯)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_Positive-Unlabeled_Data_Purification_in_the_Wild_for_Object_Detection_CVPR_2021_paper.pdf)<br><br>

[12] Wasserstein Contrastive Representation Distillation(Wasserstein对比表示蒸馏)<br>
[paper](https://arxiv.org/abs/2012.08674)<br><br>

[11] Distilling Knowledge via Knowledge Review(通过知识回顾提炼知识)<br>
[paper](https://arxiv.org/abs/2104.09044) | [code](https://github.com/Jia-Research-Lab/ReviewKD)<br><br>

[10] 3D-to-2D Distillation for Indoor Scene Parsing(用于室内场景解析的3D到2D蒸馏)<br>
[paper](https://arxiv.org/abs/2104.02243)<br><br>

[9] Complementary Relation Contrastive Distillation(互补关系对比蒸馏)<br>
[paper](https://arxiv.org/abs/2103.16367)<br><br>

[8] Distilling Object Detectors via Decoupled Features(通过解耦功能蒸馏物体检测器)<br>
[paper](https://arxiv.org/abs/2103.14475) | [code](https://github.com/ggjy/DeFeat.pytorch)<br><br>

[7] Refine Myself by Teaching Myself: Feature Refinement via Self-Knowledge Distillation(通过自学来完善自己：通过自我蒸馏提炼特征)<br>
[paper](https://arxiv.org/pdf/2103.08273.pdf) | [code](https://github.com/MingiJi/FRSKD)<br><br>

[6] Knowledge Evolution in Neural Networks(神经网络中的知识进化)<br>
[paper](https://arxiv.org/pdf/2103.05152.pdf) | [code](https://github.com/ahmdtaha/knowledge_evolution)<br><br>

[5] Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning(少班级增量学习的语义感知知识蒸馏)<br>
[paper](https://arxiv.org/abs/2103.04059)<br><br>

[4] Teachers Do More Than Teach: Compressing Image-to-Image Models(https://arxiv.org/abs/2103.03467)<br>
[paper](https://arxiv.org/abs/2103.03467) | [code](https://github.com/snap-research/CAT)<br><br>

[3] General Instance Distillation for Object Detection(通用实例蒸馏技术在目标检测中的应用)<br>
[paper](https://arxiv.org/abs/2103.02340)<br><br>

[2] Multiresolution Knowledge Distillation for Anomaly Detection(用于异常检测的多分辨率知识蒸馏)<br>
[paper](https://arxiv.org/abs/2011.11108)<br><br>

[1] Distilling Object Detectors via Decoupled Features（前景背景分离的蒸馏技术） <br><br>


<a name="Pruning"/> 

### 剪枝(Pruning)

[3] Convolutional Neural Network Pruning with Structural Redundancy Reduction(减少结构冗余的卷积神经网络修剪)<br>
[paper](https://arxiv.org/abs/2104.03438)<br><br>

[2] Neural Response Interpretation through the Lens of Critical Pathways(关键途径对神经反应的解释)<br>
[paper](https://arxiv.org/abs/2103.16886) | [code1](https://github.com/CAMP-eXplain-AI/PathwayGrad) | [code2](https://github.com/CAMP-eXplain-AI/RoarTorch)<br><br>

[1] Manifold Regularized Dynamic Network Pruning(流形规则化动态网络剪枝)<br>
[paper](https://arxiv.org/pdf/2103.05861.pdf)<br><br>

<a name="Quantization"/> 

### 量化(Quantization)

[3] Network Quantization with Element-wise Gradient Scaling(逐元素梯度缩放的网络量化)<br>
[paper](https://arxiv.org/abs/2104.00903)<br><br>

[2] Zero-shot Adversarial Quantization(零样本对抗量化)<br>
[paper](https://arxiv.org/abs/2103.15263) | [code](https://git.io/Jqc0y)<br><br>

[1] Learnable Companding Quantization for Accurate Low-bit Neural Networks(精确低位神经网络的可学习压扩量化)<br>
[paper](https://arxiv.org/pdf/2103.07156.pdf)<br><br>

<br>

<a name="NNS"/> 

## 神经网络结构设计(Neural Network Structure Design)

[15] Improving Accuracy of Binary Neural Networks using Unbalanced Activation Distribution(使用不平衡激活分布提高二元神经网络的准确性)<br>
[paper](https://arxiv.org/abs/2012.00938)<br><br>

[14] Heterogeneous Grid Convolution for Adaptive, Efficient, and Controllable Computation(用于自适应、高效和可控计算的异构网格卷积)<br>
[paper](https://arxiv.org/abs/2104.11176)<br><br>

[13] AsymmNet: Towards ultralight convolution neural networks using asymmetrical bottlenecks(AsymmNet：利用不对称瓶颈迈向超轻型卷积神经网络)<br>
[paper](https://arxiv.org/abs/2104.07770) | [code](https://github.com/Spark001/AsymmNet)<br><br>

[12] CondenseNet V2: Sparse Feature Reactivation for Deep Networks(CondenseNet V2：深度网络的稀疏功能重新激活)<br>
[paper](https://arxiv.org/abs/2104.04382)<br><br>

[11] Convolutional Hough Matching Networks(卷积霍夫匹配网络)<br>
[paper](https://arxiv.org/abs/2103.16831)<br><br>

[10] Capsule Network is Not More Robust than Convolutional Network(胶囊网络并不比卷积网络更健壮)<br>
[paper](https://arxiv.org/abs/2103.15459)<br><br>

[9] Diverse Branch Block: Building a Convolution as an Inception-like Unit(多元分支块：将卷积构建为类似初始的单位)<br>
[paper](https://arxiv.org/abs/2103.13425) | [code](https://github.com/DingXiaoH/DiverseBranchBlock)<br><br>

[8] Scaling Local Self-Attention For Parameter Efficient Visual Backbones(扩展局部自注意力以获得有效的参数视觉主干)<br>
[paper](https://arxiv.org/pdf/2103.12731.pdf)<br><br>

[7] Fast and Accurate Model Scaling(快速准确的模型缩放)<br>
[paper](https://arxiv.org/abs/2103.06877)<br><br>

[6] Involution: Inverting the Inherence of Convolution for Visual Recognition(反转卷积的固有性以进行视觉识别)<br>
[paper](https://arxiv.org/abs/2103.06255) | [code](https://github.com/d-li14/involution)<br><br>

[5] Inception Convolution with Efficient Dilation Search(具有有效膨胀搜索的初始卷积)<br>
[paper](https://arxiv.org/pdf/2012.13587.pdf) | [code](https://github.com/yifan123/IC-Conv) | [解读-Inception convolution](https://zhuanlan.zhihu.com/p/354194188)
<br><br>

[4] Coordinate Attention for Efficient Mobile Network Design(协调注意力以实现高效的移动网络设计)<br>
[paper](https://arxiv.org/abs/2103.02907)<br><br>

[3] Rethinking Channel Dimensions for Efficient Model Design(重新考虑通道尺寸以进行有效的模型设计)<br>
[paper](https://arxiv.org/abs/2007.00992) | [code](https://github.com/clovaai/rexnet)<br><br>

[2] Inverting the Inherence of Convolution for Visual Recognition（颠倒卷积的固有性以进行视觉识别）<br><br>

[1] RepVGG: Making VGG-style ConvNets Great Again<br>
[paper](https://arxiv.org/abs/2101.03697) | [code](https://github.com/megvii-model/RepVGG)<br>
解读：[RepVGG：极简架构，SOTA性能，让VGG式模型再次伟大](https://zhuanlan.zhihu.com/p/344324470)<br><br>

<a name="Transformer"/> 

### Transformer

[2] Transformer Interpretability Beyond Attention Visualization(注意力可视化之外的Transformer可解释性)<br>
[paper](https://arxiv.org/pdf/2012.09838.pdf) | [code](https://github.com/hila-chefer/Transformer-Explainability)<br><br>

[1] Pre-Trained Image Processing Transformer(底层视觉预训练模型)<br>
[paper](https://arxiv.org/pdf/2012.00364.pdf) | [解读-Transformer再下一城！low-level多个任务榜首被占领，北大华为等联合提出预训练模型IPT](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247524529&idx=1&sn=e39e67981b2afd9a5369cc843ddf28fe&chksm=ec1c8d48db6b045e7cf2d37c5633da8d3caf5e53178fe6df0913f2a04bc143fcc2e504d6f9be&token=2008688100&lang=zh_CN#rd)<br><br>

<a name="GNN"/> 

### 图神经网络(GNN)

[3] A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts(窥探神经网络的推理：解读结构视觉概念)<br>
[paper](https://arxiv.org/abs/2105.00290)<br><br>

[2] Quantifying Explainers of Graph Neural Networks in Computational Pathology(计算病理学中图神经网络的量化解释器)<br>
[paper](https://arxiv.org/pdf/2011.12646.pdf)<br><br>

[1] Sequential Graph Convolutional Network for Active Learning(主动学习的顺序图卷积网络)<br>
[paper](https://arxiv.org/pdf/2006.10219.pdf)<br><br>

<a name="NAS"/> 

### 神经网络架构搜索(NAS)

[16] FP-NAS: Fast Probabilistic Neural Architecture Search(FP-NAS：快速概率神经架构搜索)<br>
[paper](https://arxiv.org/abs/2011.10949)<br><br>

[15] ViPNAS: Efficient Video Pose Estimation via Neural Architecture Search(ViPNAS：通过神经架构搜索进行高效的视频姿态估计)<br>
[paper](https://arxiv.org/abs/2105.10154)<br><br>

[14] BCNet: Searching for Network Width with Bilaterally Coupled Network(BCNet：用双边耦合网络搜索网络宽度)<br>
[paper](https://arxiv.org/abs/2105.10533)<br><br>

[13] TransNAS-Bench-101: Improving Transferability and Generalizability of Cross-Task Neural Architecture Search(TransNAS-Bench-101：提高跨任务神经架构搜索的可转移性和通用性)<br>
[paper](https://arxiv.org/abs/2105.11871)<br><br>

[12] Joint-DetNAS: Upgrade Your Detector with NAS, Pruning and Dynamic Distillation(联合DetNAS：用NAS、剪枝和动态蒸馏升级你的探测器)<br>
[paper](https://arxiv.org/abs/2105.12971)<br><br>

[11] Landmark Regularization: Ranking Guided Super-Net Training in Neural Architecture Search(具有里程碑意义的正则化：神经体系结构搜索中的排名指导超级网络培训)<br>
[paper](https://arxiv.org/abs/2104.05309)<br><br>

[10] NetAdaptV2: Efficient Neural Architecture Search with Fast Super-Network Training and Architecture Optimization(具有快速超级网络培训和架构优化的高效神经架构搜索)<br>
[paper](https://arxiv.org/abs/2104.00031) | [project](http://netadapt.mit.edu/)<br><br>

[9] One-Shot Neural Ensemble Architecture Search by Diversity-Guided Search Space Shrinking(通过分流引导的搜索空间缩小实现一站式神经集成结构搜索)<br>
[paper](https://arxiv.org/abs/2104.00597) | [code](https://github.com/researchmm/NEAS)<br><br>

[8] Dynamic Slimmable Network(动态可压缩网络)<br>
[paper](https://arxiv.org/abs/2103.13258) | [code](https://github.com/changlin31/DS-Net)<br><br>

[7] Prioritized Architecture Sampling with Monto-Carlo Tree Search(蒙特卡洛树搜索的优先架构采样)<br>
[paper](https://arxiv.org/pdf/2103.11922.pdf) | [code](https://github.com/xiusu/NAS-Bench-Macro)<br><br>

[6] Searching by Generating: Flexible and Efficient One-Shot NAS with Architecture Generator(通过生成进行搜索：带有架构生成器的灵活高效的一键式NAS)<br>
[paper](https://arxiv.org/abs/2103.07289) | [code](https://github.com/eric8607242/SGNAS)<br><br>

[5] Contrastive Neural Architecture Search with Neural Architecture Comparators(带有神经结构比较器的对比神经网络架构搜索)<br>
[paper](https://arxiv.org/abs/2103.05471) | [code](https://github.com/chenyaofo/CTNAS)<br><br>

[4] OPANAS: One-Shot Path Aggregation Network Architecture Search for Object(一键式路径聚合网络体系结构搜索对象)<br>
[paper](https://arxiv.org/abs/2103.04507) | [code](https://github.com/VDIGPKU/OPANAS)<br><br>

[3] AttentiveNAS: Improving Neural Architecture Search via Attentive(通过注意力改善神经架构搜索) <br>
[paper](https://arxiv.org/pdf/2011.09011.pdf)<br><br>

[2] ReNAS: Relativistic Evaluation of Neural Architecture Search(NAS predictor当中ranking loss的重要性)<br>
[paper](https://arxiv.org/pdf/1910.01523.pdf)<br><br>

[1] HourNAS: Extremely Fast Neural Architecture（降低NAS的成本）<br>
[paper](https://arxiv.org/pdf/2005.14446.pdf)<br><br>


<br>

<a name="DataProcessing"/> 

## 数据处理(Data Processing)

<a name="DataAugmentation"/> 

### 数据增广(Data Augmentation)

[3] A Fourier-based Framework for Domain Generalization(基于傅立叶的域泛化框架)<br>
[paper](https://arxiv.org/abs/2105.11120)<br><br>

[2] AutoDO: Robust AutoAugment for Biased Data with Label Noise via Scalable Probabilistic Implicit Differentiation(通过可扩展的概率隐式微分对带有标签噪声的有偏数据进行鲁棒的自动增强)<br>
[paper](https://arxiv.org/abs/2103.05863)<br><br>

[1] KeepAugment: A Simple Information-Preserving Data Augmentation(一种简单的保存信息的数据扩充)<br>
[paper](https://arxiv.org/pdf/2011.11778.pdf)<br><br>

<a name="RepresentationLearning"/> 

### 表征学习(Representation Learning)

[19] Task Programming: Learning Data Efficient Behavior Representations(任务编程：学习数据高效行为表征)<br>
[paper](https://arxiv.org/abs/2011.13917) ｜ [code](https://github.com/neuroethology/TREBA) | [project](https://sites.google.com/view/task-programming)<br><br>

[18] Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization(通过对比交叉视图互信息最大化学习视图解开人体姿势表示)<br>
[paper](https://arxiv.org/abs/2012.01405) | [code](https://github.com/google-research/google-research/tree/master/poem)<br><br>

[17] SSAN: Separable Self-Attention Network for Video Representation Learning(SSAN：用于视频表示学习的可分离自注意力网络)<br>
[paper](https://arxiv.org/abs/2105.13033)<br><br>

[16] Generative Interventions for Causal Learning(因果学习的生成性干预)<br>
[paper](https://arxiv.org/abs/2012.12265)<br><br>

[15] Representation Learning via Global Temporal Alignment and Cycle-Consistency(通过全局时间对齐和周期一致性进行表示学习)<br>
[paper](https://arxiv.org/abs/2105.05217)<br><br>

[14] Multi-Perspective LSTM for Joint Visual Representation Learning(用于联合视觉表示学习的多视角LSTM)<br>
[paper](https://arxiv.org/abs/2105.02802) | [code](https://github.com/arsm/MPLSTM)<br><br>

[13] Unsupervised Visual Representation Learning by Tracking Patches in Video(通过跟踪视频中的补丁来进行无监督的视觉表示学习)<br>
[paper](https://arxiv.org/abs/2105.02545) | [code](http://github.com/microsoft/CtP)<br><br>

[12] A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning(无监督时空表示学习的大规模研究)<br>
[paper](https://arxiv.org/abs/2104.14558)<br><br>

[11] Where and What? Examining Interpretable Disentangled Representations(在哪里和什么？ 检查可解释的纠缠表示)<br>
[paper](https://arxiv.org/abs/2104.05622)<br><br>

[10] Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning(眼见为实：视觉语言表示学习的端到端预训练)<br>
[paper](https://arxiv.org/abs/2104.03135)<br><br>

[9] Self-supervised Video Representation Learning by Context and Motion Decoupling(通过上下文和运动解耦进行自我监督的视频表示学习)<br>
[paper](https://arxiv.org/abs/2104.00862)<br><br>

[8] Jigsaw Clustering for Unsupervised Visual Representation Learning(拼图聚类的无监督视觉表示学习)<br>
[paper](https://arxiv.org/abs/2104.00323) | [code](https://github.com/Jia-Research-Lab/JigsawClustering)<br><br>

[7] Learning by Aligning Videos in Time(【视频表征】通过时间对齐视频进行学习)<br>
[paper](https://arxiv.org/abs/2103.17260)<br><br>

[6] Vectorization and Rasterization: Self-Supervised Learning for Sketch and Handwriting(矢量化和光栅化：素描和手写的自我指导学习)<br>
[paper](https://arxiv.org/abs/2103.13716) | [code](https://github.com/AyanKumarBhunia/Self-Supervised-Learning-for-Sketch)<br><br>

[5] Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks(神经零件：使用可逆神经网络学习富有表现力的3D形状提取)<br>
[paper](https://arxiv.org/pdf/2103.10429.pdf)<br><br>

[4] VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples(对比视频表示学习和临时对抗示例)<br>
[paper](https://arxiv.org/abs/2103.05905)<br><br>

[3] Spatially Consistent Representation Learning(空间一致表示学习)<br>
[paper](https://arxiv.org/abs/2103.06122)<br><br>

[2] Removing the Background by Adding the Background: Towards Background Robust Self-supervised Video Representation Learning(通过添加背景来删除背景：朝着背景进行鲁棒的自我监督视频表示学习)<br>
[paper](https://arxiv.org/pdf/2009.05769.pdf) | [code](https://github.com/FingerRec/BE) | [project](https://fingerrec.github.io/index_files/jinpeng/papers/CVPR2021/project_website.html) | [解读](https://zhuanlan.zhihu.com/p/355232006)<br><br>

[1] VirTex: Learning Visual Representations from Textual Annotations（从文本注释中学习视觉表示）<br>
[paper](https://arxiv.org/abs/2006.06666) | [code](https://github.com/kdexd/virtex)<br><br>

<a name="BatchNormalization"/> 

### 归一化/正则化(Batch Normalization)

[3] Adaptive Consistency Regularization for Semi-Supervised Transfer Learning(半监督转移学习的自适应一致性正则化)<br>
[paper](https://arxiv.org/abs/2103.02193) | [code](https://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning)

[2] Meta Batch-Instance Normalization for Generalizable Person Re-Identification(通用批处理人员重新标识的元批实例规范化)<br>
[paper](https://arxiv.org/abs/2011.14670)<br><br>

[1] Representative Batch Normalization with Feature Calibration（具有特征校准功能的代表性批量归一化）<br><br>

<a name="ImageClustering"/> 

### 图像聚类(Image Clustering)

[4] Structure-Aware Face Clustering on a Large-Scale Graph with 10^7 Nodes(具有10^7个节点的大规模图上的结构感知人脸聚类)<br>
[paper](https://arxiv.org/abs/2103.13225) | [code&project](https://sstzal.github.io/STAR-FC/)<br><br>

[3] COMPLETER: Incomplete Multi-view Clustering via Contrastive Prediction(通过对比预测的不完整多视图聚类)<br>
[paper](http://pengxi.me/wp-content/uploads/2021/03/2021CVPR-completer.pdf) | [code](https://github.com/XLearning-SCU/2021-CVPR-Completer)<br><br>

[2] Improving Unsupervised Image Clustering With Robust Learning（通过鲁棒学习改善无监督图像聚类）<br>
[paper](https://arxiv.org/abs/2012.11150) | [code](https://github.com/deu30303/RUC)<br><br>

[1] Reconsidering Representation Alignment for Multi-view Clustering(重新考虑多视图聚类的表示对齐方式)<br>
[paper](https://arxiv.org/abs/2103.07738) | [code](https://github.com/DanielTrosten/mvc)<br><br>


<a name="ImageCompression"/> 

### 图像压缩(Image Compression)

[4] Learning Scalable ℓ∞-constrained Near-lossless Image Compression via Joint Lossy Image and Residual Compression(通过联合有损图像和残差压缩学习可伸缩ℓ∞约束的近无损图像压缩)<br>
[paper](https://arxiv.org/abs/2103.17015) | [code](https://github.com/BYchao100/Scalable-Near-lossless-Image-Compression)<br><br>

[3] Checkerboard Context Model for Efficient Learned Image Compression(高效学习图像压缩的棋盘上下文模型)<br>
[paper](https://arxiv.org/abs/2103.15306)<br><br>

[2] Slimmable Compressive Autoencoders for Practical Neural Image Compression(实用神经图像压缩的可压缩压缩自动编码器)<br>
[paper](https://arxiv.org/abs/2103.15726)<br><br>

[1] Attention-guided Image Compression by Deep Reconstruction of Compressive Sensed Saliency Skeleton(通过压缩感知显着性骨架的深度重构来进行注意力引导的图像压缩)<br>
[paper](https://arxiv.org/abs/2103.15368)<br><br>

<a name="AnomalyDetection"/> 

### 异常检测(Anomaly Detection)

[3] MOS: Towards Scaling Out-of-distribution Detection for Large Semantic Space(MOS：面向大型语义空间的规模化异常样本检测)<br>
[paper](https://arxiv.org/abs/2105.01879)<br><br>

[2] MOOD: Multi-level Out-of-distribution Detection(MOOD：多级异常样本检测)<br>
[paper](https://arxiv.org/abs/2104.14726)<br><br>

[1] Learning Placeholders for Open-Set Recognition(学习占位符以进行开放式识别)<br>
[paper](https://arxiv.org/abs/2103.15086)<br><br>


<br>

<a name="ModelTraining"/> 

## 模型训练/泛化(Model Training/Generalization)

[8] A Bop and Beyond: A Second Order Optimizer for Binarized Neural Networks(【优化算法】Bop和超越：二值神经网络的二阶优化器)<br>
[paper](https://arxiv.org/abs/2104.05124)<br><br>

[7] Simpler Certified Radius Maximization by Propagating Covariances(通过传播协方差简化认证半径最大化)<br>
[paper](https://arxiv.org/abs/2104.05888) | [video](https://youtu.be/m1ya2oNf5iE)<br><br>

[6] Differentiable Patch Selection for Image Recognition(用于图像识别的差异化补丁选择)<br>
[paper](https://arxiv.org/abs/2104.03059) | [code](https://github.com/google-research/google-research/tree/master/ptopk_patch_selection/)<br><br>

[5] Towards Evaluating and Training Verifiably Robust Neural Networks(评估和训练可验证的稳健神经网络)<br>
[paper](https://arxiv.org/abs/2104.00447) | [code](https://github.com/ZhaoyangLyu/VerifiablyRobustNN)<br><br>

[4] Student-Teacher Learning from Clean Inputs to Noisy Inputs(从纯净输入到噪音输入的师生学习)<br>
[paper](https://arxiv.org/pdf/2103.07600.pdf)<br><br>

[3] Uncertainty-guided Model Generalization to Unseen Domains(不确定性指导的模型泛化)<br>
[paper](https://arxiv.org/abs/2103.07531)<br><br>

[2] Knowledge Evolution in Neural Networks(神经网络中的知识进化)<br>
[paper](https://arxiv.org/pdf/2103.05152.pdf) | [code](https://github.com/ahmdtaha/knowledge_evolution)<br><br>

[1] PGT: A Progressive Method for Training Models on Long Videos(一种在长视频上训练模型的渐进方法)<br>
[paper](https://arxiv.org/pdf/2103.11313.pdf) | [code](https://github.com/BoPang1996/PGT)<br><br>

<a name="NoisyLabel"/> 

### 噪声标签(Noisy Label)

[3] Correlated Input-Dependent Label Noise in Large-Scale Image Classification(大规模图像分类中的关联输入相关标签噪声)<br>
[paper](https://arxiv.org/abs/2105.10305)<br><br>

[2] A Second-Order Approach to Learning with Instance-Dependent Label Noise(与实例相关的标签噪声的二阶学习方法)<br>
[paper](https://arxiv.org/abs/2012.11854) | [code](https://github.com/UCSC-REAL/CAL)<br><br>

[1] Partially View-aligned Representation Learning with Noise-robust Contrastive Loss(面向部分视图对齐表示学习的噪声鲁棒对比损失函数)<br>
[paper](http://pengxi.me/wp-content/uploads/2021/03/2021CVPR-MvCLNwith-supp.pdf) | [code](https://github.com/XLearning-SCU/2021-CVPR-MvCLN)<br><br>

<a name="Long-Tailed"/> 

### 长尾分布(Long-Tailed Distribution)

[8] Disentangling Label Distribution for Long-tailed Visual Recognition(用于长尾视觉识别的解开标签分布)<br>
[paper](https://arxiv.org/abs/2012.00321)<br><br>

[7] Adversarial Robustness under Long-Tailed Distribution(长尾分布下的对抗鲁棒性)<br>
[paper](https://arxiv.org/abs/2104.02703) | [code](https://github.com/wutong16/Adversarial_Long-Tail)<br><br>

[6] Adaptive Class Suppression Loss for Long-Tail Object Detection(长尾目标检测的自适应类抑制损失)<br>
[paper](https://arxiv.org/abs/2104.00885) | [code](https://github.com/CASIA-IVA-Lab/ACSL)<br><br>

[5] Improving Calibration for Long-Tailed Recognition(改善长尾识别的校准)<br>
[paper](https://arxiv.org/abs/2104.00466) | [code](https://github.com/Jia-Research-Lab/MiSLAS)<br><br>

[4] Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification(基于对比学习的混合网络的长尾图像分类)<br>
[paper](https://arxiv.org/abs/2103.14267)<br><br>

[3] PML: Progressive Margin Loss for Long-tailed Age Classification(长尾年龄分类的累进边际损失)<br>
[paper](https://arxiv.org/abs/2103.02140)<br><br>

[2] MetaSAug: Meta Semantic Augmentation for Long-Tailed Visual Recognition(MetaSAug：用于长尾视觉识别的元语义增强)<br>
[paper](https://arxiv.org/pdf/2103.12579.pdf)<br><br>

[1] Distribution Alignment: A Unified Framework for Long-tail Visual Recognition(分布对齐：长尾视觉识别的统一框架)<br>
[paper](https://arxiv.org/abs/2103.16370) | [code](https://github.com/Megvii-BaseDetection/DisAlign)<br><br>
<br>

<a name="ModelEvaluation"/> 

## 模型评估(Model Evaluation)

[1] Are Labels Necessary for Classifier Accuracy Evaluation?(测试集没有标签，我们可以拿来测试模型吗？)<br>
[paper](https://arxiv.org/abs/2007.02915) | [解读](https://zhuanlan.zhihu.com/p/328686799)<br><br>

<br>

<a name="MMLearning"/> 

## 多模态学习(Multi-Modal Learning)

[8] Distilling Audio-Visual Knowledge by Compositional Contrastive Learning(运用组合对比学习提取视听知识)<br>
[paper](https://arxiv.org/abs/2104.10955) | [code](https://github.com/yanbeic/CCL)<br><br>

[7] Cross-Modal Center Loss for 3D Cross-Modal Retrieval(用于3D跨模态检索的跨模态中心损失)<br>
[paper](https://arxiv.org/abs/2008.03561) | [code](https://github.com/LongLong-Jing/Cross-Modal-Center-Loss)<br><br>

[6] Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion(具有深度敏感注意力和自动多模态融合的深度RGB-D显著性检测)<br>
[paper](https://arxiv.org/abs/2103.11832)<br><br>

[5] There is More than Meets the Eye: Self-Supervised Multi-Object Detection  and Tracking with Sound by Distilling Multimodal Knowledge(多模态知识提取的自监督多目标检测与有声跟踪)<br>
[paper](https://arxiv.org/abs/2103.01353) | [video](https://www.youtube.com/channel/UCRpM8k1GY3kD2TqCo_yKN3g) | [project](http://rl.uni-freiburg.de/research/multimodal-distill)<br><br>

[4] Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion(具有深度敏感注意力和自动多模态融合的深度RGB-D显著性检测)<br>
[paper](https://arxiv.org/abs/2103.11832)<br><br>

[3] LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of Dynamic Agents(动态代理的多模态未来轨迹的车道感知预测)<br>
[paper](https://arxiv.org/abs/2104.00249)<br><br>

[2] Multimodal Motion Prediction with Stacked Transformers(堆叠式Transformer的多模态运动预测)<br>
[paper](https://arxiv.org/pdf/2103.11624.pdf) | [code](https://decisionforce.github.io/mmTransformer)<br><br>

[1] Multi-Modal Fusion Transformer for End-to-End Autonomous Driving(用于端到端自动驾驶的多模态融合Transformer)<br>
[paper](https://arxiv.org/abs/2104.09224)<br><br>

<a name="Audio-VisualLearning"/> 

### 视听学习(Audio-visual Learning)

[7] Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions(语音时刻：从视频描述中学习联合视听表示)<br>
[paper](https://arxiv.org/abs/2105.04489)<br><br>

[6] Visually Informed Binaural Audio Generation without Binaural Audios(无需双耳音频的可视化双耳音频生成)<br>
[paper](https://arxiv.org/abs/2104.06162) | [project](https://sheldontsui.github.io/projects/PseudoBinaural)<br><br>

[5] Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation(观察您的语音：学习跨模态亲和力以进行视听语音分离)<br>
[paper](https://arxiv.org/abs/2104.02775) | [project](https://caffnet.github.io/)<br><br>

[4] Localizing Visual Sounds the Hard Way(视觉声音定位的困难方法)<br>
[paper](https://arxiv.org/abs/2104.02691)<br><br>

[3] Can audio-visual integration strengthen robustness under multimodal attacks?(视听集成能否增强多模式攻击下的鲁棒性？)<br>
[paper](https://arxiv.org/abs/2104.02000)<br><br>

[2] Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation(探测对象视觉接地与声音分离的循环共同学习)<br>
[paper](https://arxiv.org/abs/2104.02026)<br><br>

[1] Positive Sample Propagation along the Audio-Visual Event Line(沿视听事件线的正样本传播)<br>
[paper](https://arxiv.org/abs/2104.00239) | [code](https://github.com/jasongief/PSP_CVPR_2021)<br><br>


<br>
<a name="Vision-basedPrediction"/> 

## 视觉预测(Vision-based Prediction)

[12] Shared Cross-Modal Trajectory Prediction for Autonomous Driving(自动驾驶的共享跨模态轨迹预测)<br>
[paper](https://arxiv.org/abs/2011.08436)<br><br>

[11] We are More than Our Joints: Predicting how 3D Bodies Move()<br>
[paper](https://arxiv.org/abs/2012.00619) ｜ [code](https://github.com/yz-cnsdqz/MOJO-release) | [project](https://yz-cnsdqz.github.io/MOJO/MOJO.html)<br><br>

[10] Interpretable Social Anchors for Human Trajectory Forecasting in Crowds(人群中人类轨迹预测的可解释社会锚点)<br>
[paper](https://arxiv.org/abs/2105.03136)<br><br>

[9] DriveGAN: Towards a Controllable High-Quality Neural Simulation(DriveGAN：迈向可控的高质量神经仿真)<br>
[paper](https://arxiv.org/abs/2104.15060)<br><br>

[8] Learning Semantic-Aware Dynamics for Video Prediction(视频预测中的语义感知动态学习)<br>
[paper](https://arxiv.org/abs/2104.09762)<br><br>

[7] Divide-and-Conquer for Lane-Aware Diverse Trajectory Prediction(车道感知不同轨迹预测的分而治之)<br>
[paper](https://arxiv.org/abs/2104.08277)<br><br>

[6] GATSBI: Generative Agent-centric Spatio-temporal Object Interaction(GATSBI：以生成代理为中心的时空对象交互)<br>
[paper](https://arxiv.org/abs/2104.04275)<br><br>

[5] SGCN:Sparse Graph Convolution Network for Pedestrian Trajectory Prediction(SGCN：行人轨迹预测的稀疏图卷积网络)<br>
[paper](https://arxiv.org/abs/2104.01528)<br><br>

[4] LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of Dynamic Agents(动态代理的多模态未来轨迹的车道感知预测)<br>
[paper](https://arxiv.org/abs/2104.00249)<br><br>

[3] Multimodal Motion Prediction with Stacked Transformers(堆叠式Transformer的多模态运动预测)<br>
[paper](https://arxiv.org/pdf/2103.11624.pdf) | [code](https://decisionforce.github.io/mmTransformer)<br><br>

[2] Video Prediction Recalling Long-term Motion Context via Memory Alignment Learning(通过记忆对准学习的视频预测调用长期运动环境)<br>
[paper](https://arxiv.org/abs/2104.00924)<br><br>

[1] MotionRNN: A Flexible Model for Video Prediction with Spacetime-Varying Motions(针对复杂时空运动的通用视频预测模型)<br>
[paper](https://arxiv.org/abs/2103.02243) | [解读](https://zhuanlan.zhihu.com/p/355703957)<br><br>


<br>

<a name="Dataset"/> 

## 数据集(Dataset)

[18] VIGOR: Cross-View Image Geo-localization beyond One-to-one Retrieval(VIGOR：超越一对一检索的交叉视图图像地理定位)<br>
[paper](https://arxiv.org/abs/2011.12172) | [dataset&project](https://github.com/Jeff-Zilence/VIGOR)<br><br>

[17] Multi-shot Temporal Event Localization: a Benchmark(多镜头时间事件本地化：基准)<br>
[paper](https://arxiv.org/abs/2012.09434) | [dataset&project](https://songbai.site/muses/)<br><br>

[16] Detection, Tracking, and Counting Meets Drones in Crowds: A Benchmark(检测，跟踪和计数遇到人群中的无人机：基准)<br>
[paper](https://arxiv.org/abs/2105.02440) | [dataset&code](https://github.com/VisDrone/DroneCrowd)<br><br>

[15] AGORA: Avatars in Geography Optimized for Regression Analysis(AGORA：针对回归分析进行了优化的地理头像)<br>
[paper](https://arxiv.org/abs/2104.14643) | [project](https://agora.is.tue.mpg.de)<br><br>

[14] Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets(【数据集标注】寻求有效注释大型图像分类数据集的良好做法)<br>
[paper](https://arxiv.org/abs/2104.12690) | [project](https://fidler-lab.github.io/efficient-annotation-cookbook)<br><br>

[13] Learning To Count Everything(【视觉计数】学习计算一切)<br>
[paper](https://arxiv.org/abs/2104.08391) | [dataset&code](https://github.com/cvlab-stonybrook/LearningToCountEverything)<br><br>

[12] DexYCB: A Benchmark for Capturing Hand Grasping of Objects(DexYCB：捕获对象的手抓握的基准)<br>
[paper](https://arxiv.org/abs/2104.04631) |[dataset&code](https://dex-ycb.github.io/)

[11] The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions(多智能体行为数据集：鼠标二元社交互动)<br>
[paper](https://arxiv.org/abs/2104.02710) | [dataset](https://www.aicrowd.com/challenges/multi-agent-behavior-representation-modeling-measurement-and-applications)<br><br>

[10] Deep Animation Video Interpolation in the Wild(野外深度动画视频插帧)<br>
[paper](https://arxiv.org/abs/2104.02495) | [dataset&code](https://github.com/lisiyao21/AnimeInterp/)<br><br>

[9] Towards Rolling Shutter Correction and Deblurring in Dynamic Scenes(在动态场景中实现卷帘快门校正和去模糊)<br>
[paper](https://arxiv.org/abs/2104.01601) | [dataset&code](https://github.com/zzh-tech/RSCD)<br><br>

[8] UAV-Human: A Large Benchmark for Human Behavior Understanding with Unmanned Aerial Vehicles(无人机-人类：了解无人机行为的大型基准)<br>
[paper](https://arxiv.org/abs/2104.00946)<br><br>

[7] Visual Semantic Role Labeling for Video Understanding(【视频理解】用于视频理解的视觉语义角色标签)<br>
[paper](https://arxiv.org/abs/2104.00990) | [dataset&code](http://vidsitu.org/)<br><br>

[6] Face Forensics in the Wild(人脸伪造数据集)<br>
[paper](https://arxiv.org/abs/2103.16076) | [dataset&code](https://github.com/tfzhou/FFIW)<br><br>

[5] Benchmarking Representation Learning for Natural World Image Collections(【自然图像分类】自然世界影像收藏的基准表示学习)<br>
[paper](https://arxiv.org/abs/2103.16483)<br><br>

[4] Sewer-ML: A Multi-Label Sewer Defect Classification Dataset and Benchmark(多标签下水道缺陷分类数据集和基准)<br>
[paper](https://arxiv.org/abs/2103.10895) | [project&dataset](https://vap.aau.dk/sewer-ml/)<br><br>

[3] 3DCaricShop: A Dataset and A Baseline Method for Single-view 3D Caricature Face Reconstruction(单视图3D漫画面部重建的数据集和基线方法)<br>
[paper](https://arxiv.org/pdf/2103.08204.pdf) | [project](https://qiuyuda.github.io/3DCaricShop/)<br><br>

[2] Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset, Benchmarks and Challenges(走向城市规模3D点云的语义分割：数据集，基准和挑战)<br>
[paper](https://arxiv.org/abs/2009.03137) | [code](https://github.com/QingyongHu/SensatUrban)<br><br>

[1] Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels（重新标记ImageNet：从单标签到多标签，从全局标签到本地标签）<br>
[paper](https://arxiv.org/abs/2101.05022) | [code](https://github.com/naver-ai/relabel_imagenet)<br><br>

<br>

<a name="ActiveLearning"/> 

## 主动学习(Active Learning)


[3] Vab-AL: Incorporating Class Imbalance and Difficulty with Variational Bayes for Active Learning<br>
[paper](https://arxiv.org/abs/2003.11249)<br><br>

[2] Multiple Instance Active Learning for Object Detection（用于对象检测的多实例主动学习）<br>
[paper](https://github.com/yuantn/MIAL/raw/master/paper.pdf) | [code](https://github.com/yuantn/MIAL)<br><br>

[1] Sequential Graph Convolutional Network for Active Learning(主动学习的顺序图卷积网络)<br>
[paper](https://arxiv.org/pdf/2006.10219.pdf)<br><br>

<br>

<a name="Few-shotLearning"/> 

## 小样本学习/零样本学习(Few-shot Learning/Zero-shot Learning)

[11] Few-Shot Classification with Feature Map Reconstruction Networks(使用特征映射重建网络的少样本分类)<br>
[paper](https://arxiv.org/abs/2012.01506) | [code](https://github.com/Tsingularity/FRN)<br><br>

[10] Learning Graph Embeddings for Compositional Zero-shot Learning(组成零样本学习的学习图嵌入)<br>
[paper](https://arxiv.org/abs/2102.01987) | [code](https://github.com/ExplainableML/czsl)<br><br>

[9] Self-Guided and Cross-Guided Learning for Few-Shot Segmentation(自我指导和交叉指导学习，用于小样本分割)<br>
[paper](https://arxiv.org/abs/2103.16129)<br><br>

[8] Contrastive Embedding for Generalized Zero-Shot Learning(广义零样本学习的对比嵌入)<br>
[paper](https://arxiv.org/abs/2103.16173) | [code](https://github.com/Hanzy1996/CE-GZSL)<br><br>

[7] Learning Dynamic Alignment via Meta-filter for Few-shot Learning(通过元过滤器学习动态对齐，以进行小样本学习)<br>
[paper](https://arxiv.org/abs/2103.13582)<br><br>

[6] Goal-Oriented Gaze Estimation for Zero-Shot Learning(零样本学习的目标导向注视估计)<br>
[paper](https://arxiv.org/abs/2103.03433) | [code](https://github.com/osierboy/GEM-ZSL)<br><br>

[5] Few-Shot Segmentation Without Meta-Learning: A Good Transductive Inference Is All You Need?<br>
[paper](https://arxiv.org/abs/2012.06166) | [code](https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation)<br><br>

[4] Counterfactual Zero-Shot and Open-Set Visual Recognition(反事实零样本和开集视觉识别)<br>
[paper](https://arxiv.org/abs/2103.00887) | [code](https://github.com/yue-zhongqi/gcm-cf)<br><br>

[3] Semantic Relation Reasoning for Shot-Stable Few-Shot Object Detection(小样本目标检测的语义关系推理)<br>
[paper](https://arxiv.org/abs/2103.01903)<br><br>

[2] Few-shot Open-set Recognition by Transformation Consistency(转换一致性的小样本开放集识别)<br><br>

[1] Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning(探索小样本学习的不变表示形式和等变表示形式的互补强度)<br>
[paper](https://arxiv.org/abs/2103.01315)<br><br>

<br>

<a name="ContinualLearning"/> 

## 持续学习(Continual Learning/Life-long Learning)

[5] Rectification-based Knowledge Retention for Continual Learning(基于矫正的知识保留用于持续学习)<br>
[paper](https://arxiv.org/abs/2103.16597)<br><br>

[4] Rainbow Memory: Continual Learning with a Memory of Diverse Samples(彩虹记忆：持续学习与多种样本的记忆)<br>
[paper](https://arxiv.org/abs/2103.17230) | [code](https://github.com/clovaai/rainbow-memory)<br><br>

[3] Efficient Feature Transformations for Discriminative and Generative Continual Learning(区分性和生成性持续学习的有效特征转换)<br>
[paper](https://arxiv.org/abs/2103.13558)<br><br>

[2] Rainbow Memory: Continual Learning with a Memory of Diverse Samples（不断学习与多样本的记忆）<br><br>

[1] Learning the Superpixel in a Non-iterative and Lifelong Manner(以非迭代和终身的方式学习超像素)<br>
[paper](https://arxiv.org/pdf/2103.10681.pdf)<br><br>

<br>

<a name="SG"/> 

## 场景图(Scene Graph)

<a name="SGG"/> 

### 场景图生成(Scene Graph Generation)

[4] Bipartite Graph Network with Adaptive Message Passing for Unbiased Scene Graph Generation(具有自适应消息传递功能的二分图网络，用于无偏场景图的生成)<br>
[paper](https://arxiv.org/abs/2104.00308)<br><br>

[3] Fully Convolutional Scene Graph Generation(全卷积场景图生成)<br>
[paper](https://arxiv.org/abs/2103.16083)<br><br>

[2] Probabilistic Modeling of Semantic Ambiguity for Scene Graph Generation(场景图生成的语义歧义概率建模)<br>
[paper](https://arxiv.org/abs/2103.05271)<br><br>

[1] Exploiting Edge-Oriented Reasoning for 3D Point-based Scene Graph Analysis(利用基于边缘的推理进行基于3D点的场景图分析)<br>
[paper](https://arxiv.org/pdf/2103.05558.pdf)<br><br>

<a name="SGP"/> 

### 场景图预测(Scene Graph Prediction)

[1] SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences(基于RGB-D序列的增量3D场景图预测)<br>
[paper](https://arxiv.org/abs/2103.14898)<br><br>

<a name="SGU"/> 

### 场景图理解(Scene Graph Understanding)

[4] Semantic Scene Completion via Integrating Instances and Scene in-the-Loop(通过集成实例和场景在环来完成语义场景)<br>
[paper](https://arxiv.org/abs/2104.03640) | [code](https://github.com/yjcaimeow/SISNet)<br><br>

[3] 3D-to-2D Distillation for Indoor Scene Parsing(用于室内场景解析的3D到2D蒸馏)<br>
[paper](https://arxiv.org/abs/2104.02243)<br><br>

[2] Bidirectional Projection Network for Cross Dimension Scene Understanding(双向投影网络，用于跨维度场景理解)<br>
[paper](https://arxiv.org/abs/2103.14326) | [code](https://github.com/wbhu/BPNet)<br><br>

[1] Monte Carlo Scene Search for 3D Scene Understanding(蒙特卡洛场景搜索以了解3D场景)<br>
[paper](https://arxiv.org/pdf/2103.07969.pdf)<br><br>

<br><br>

<a name="VisualLocalization"/> 

## 视觉定位(Visual Localization)

[1] LoFTR: Detector-Free Local Feature Matching with Transformers(【图像特征匹配】LoFTR：与变压器互不影响的无检测器局部特征)<br>
[paper](https://arxiv.org/abs/2104.00680) | [project](https://zju3dv.github.io/loftr/)<br><br>


<br><br>

<a name="VisualReasoning"/> 

## 视觉推理/视觉问答(Visual Reasoning/VQA)

[9] Found a Reason for me? Weakly-supervised Grounded Visual Question Answering using Capsules(找到了我的理由？ 使用胶囊进行弱监督的地面视觉问答)<br>
[paper](https://arxiv.org/abs/2105.04836)<br><br>

[8] Bridge to Answer: Structure-aware Graph Interaction Network for Video Question Answering(通往答案的桥梁：用于视频问答的结构感知图交互网络)<br>
[paper](https://arxiv.org/abs/2104.14085)<br><br>

[7] PQA: Perceptual Question Answering(感性问题解答)<br>
[paper](https://arxiv.org/abs/2104.03589)<br><br>

[6] Domain-robust VQA with diverse datasets and methods but no target labels(具有各种数据集和方法，但没有目标标签的领域稳健的VQA)<br>
[paper](https://arxiv.org/abs/2103.15974)<br><br>

[5] AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning(AGQA：组成时空推理的基准)<br>
[paper](https://arxiv.org/abs/2103.16002)<br><br>

[4] Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution(通过概率绑架和执行进行抽象时空推理)
[paper](https://arxiv.org/abs/2103.14230) | [project](http://wellyzhang.github.io/project/prae.html) | [supplementary](http://wellyzhang.github.io/attach/cvpr21zhang_prae_supp.pdf)<br><br>

[3] ACRE: Abstract Causal REasoning Beyond Covariation(ACRE：超越协方差的抽象因果推理)<br>
[paper](https://arxiv.org/abs/2103.14232) | [project](http://wellyzhang.github.io/project/acre.html) | [Supplementary](http://wellyzhang.github.io/attach/cvpr21zhang_acre_supp.pdf)<br><br>

[2] TrafficQA: A Question Answering Benchmark and an Efficient Network for Video Reasoning over Traffic Events(问题解答基准和有效的交通事件视频推理网络)<br>
[paper](https://arxiv.org/abs/2103.15538) | [project](https://github.com/SUTDCV/SUTD-TrafficQA)<br>

[1] Transformation Driven Visual Reasoning(转型驱动的视觉推理)<br>
[paper](https://arxiv.org/pdf/2011.13160.pdf) | [code](https://github.com/hughplay/TVR) | [project](https://hongxin2019.github.io/TVR/)<br>


<br><br>

<a name="ImageClassification"/> 

## 图像分类(Image Classification)

[5] Benchmarking Representation Learning for Natural World Image Collections(自然世界影像收藏的基准表示学习)<br>
[paper](https://arxiv.org/abs/2103.16483)<br><br>

[4] Sewer-ML: A Multi-Label Sewer Defect Classification Dataset and Benchmark(多标签下水道缺陷分类数据集和基准)<br>
[paper](https://arxiv.org/abs/2103.10895) | [project&dataset](https://vap.aau.dk/sewer-ml/)<br><br>

[3] Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification(基于对比学习的混合网络的长尾图像分类)<br>
[paper](https://arxiv.org/abs/2103.14267)<br><br>

[2] PML: Progressive Margin Loss for Long-tailed Age Classification(长尾年龄分类的累进边际损失)<br>
[paper](https://arxiv.org/abs/2103.02140)<br><br>

[1] A Realistic Evaluation of Semi-Supervised Learning for Fine-Grained Classification(细粒度分类的半监督学习的现实评估)<br>
[paper](https://arxiv.org/abs/2104.00679)<br><br>


<br><br>

<a name="domain"/> 

## 迁移学习/domain/自适应(Transfer Learning/Domain Adaptation)

[24] FixBi: Bridging Domain Spaces for Unsupervised Domain Adaptation(FixBi：无监督域适应的桥接域空间)<br>
[paper](https://arxiv.org/abs/2011.09230)<br><br>

[23] Ranking Neural Checkpoints(对神经检查点进行排名)<br>
[paper](https://arxiv.org/abs/2011.11200)<br><br>

[22] How Well Do Self-Supervised Models Transfer?(自监督模型的迁移效果如何？)<br>
[paper](https://arxiv.org/abs/2011.13377) | [code](https://github.com/linusericsson/ssl-transfer)<br><br>

[21] Adversarially Adaptive Normalization for Single Domain Generalization(单域泛化的对抗性自适应归一化)<br>
[paper](https://arxiv.org/abs/2106.01899)<br><br>

[20] Visualizing Adapted Knowledge in Domain Transfer(领域转移中适应性知识的可视化)<br>
[paper](https://arxiv.org/abs/2104.10602) | [code](https://github.com/hou-yz/DA_visualization)<br><br>

[19] Instance Level Affinity-Based Transfer for Unsupervised Domain Adaptation(基于实例级亲和力的无监督域自适应传输)<br>
[paper](https://arxiv.org/abs/2104.01286) | [code](https://github.com/astuti/ILA-DA)<br><br>

[18] Unsupervised Multi-source Domain Adaptation Without Access to Source Data(无需访问源数据的无监督多源域适配)<br>
[paper](https://arxiv.org/abs/2104.01845)<br><br>

[17] Curriculum Graph Co-Teaching for Multi-Target Domain Adaptation(多目标领域适应的课程图协同教学)<br>
[paper](https://arxiv.org/abs/2104.00808)<br><br>

[16] Divergence Optimization for Noisy Universal Domain Adaptation(噪声通用域自适应的发散优化)<br>
[paper](https://arxiv.org/abs/2104.00246)<br><br>

[15] Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation(典型的跨域自我监督学习，适用于少拍无监督领域自适应)<br>
[paper](https://arxiv.org/abs/2103.16765) | [project](http://xyue.io/pcs-fuda/index.html)<br><br>

[14] Progressive Domain Expansion Network for Single Domain Generalization(用于单域泛化的渐进域扩展网络)<br>
[paper](https://arxiv.org/abs/2103.16050)<br><br>

[13] Dynamic Domain Adaptation for Efficient Inference(动态域自适应以实现高效推理)<br>
[paper](https://arxiv.org/abs/2103.16403)<br><br>

[12] Adaptive Methods for Real-World Domain Generalization(真实世界域自适应的自适应方法)<br>
[paper](https://arxiv.org/abs/2103.15796)<br><br>

[11] OTCE: A Transferability Metric for Cross-Domain Cross-Task Representations(跨域跨任务表示的可传递性度量标准)<br>
[paper](https://arxiv.org/abs/2103.13843)<br><br>

[10] DRANet: Disentangling Representation and Adaptation Networks for Unsupervised Cross-Domain Adaptation(分解表示和自适应网络以实现无监督的跨域自适应)<br>
[paper](https://arxiv.org/abs/2103.13447)<br><br>

[9] MetaAlign: Coordinating Domain Alignment and Classification for Unsupervised Domain Adaptation(无监督域自适应的协调域对齐和分类)<br>
[paper](https://arxiv.org/pdf/2103.13575.pdf)<br><br>

[8] Transferable Semantic Augmentation for Domain Adaptation(可转移的语义增强以适应领域)<br>
[paper](https://arxiv.org/abs/2103.12562) | [code](https://github.com/BIT-DA/TSA)<br><br>

[7] Dynamic Transfer for Multi-Source Domain Adaptation(多源域自适应的动态传输)<br>
[paper](https://arxiv.org/abs/2103.10583)<br><br>

[6] Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation(基于双层域混合的半监督域自适应语义分割)<br>
[paper](https://arxiv.org/pdf/2103.04705.pdf)<br><br>

[5] Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation(多源领域自适应与协作学习的语义分割)<br>
[paper](https://arxiv.org/abs/2103.04717)<br><br>

[4] Continual Adaptation of Visual Representations via Domain Randomization and Meta-learning(通过域随机化和元学习对视觉表示进行连续调整)<br>
[paper](https://arxiv.org/abs/2012.04324)<br><br>

[3] Domain Generalization via Inference-time Label-Preserving Target Projections(基于推理时间保标目标投影的区域泛化)<br>
[paper](https://arxiv.org/abs/2103.01134)<br><br>

[2] MetaSCI: Scalable and Adaptive Reconstruction for Video Compressive  Sensing(可伸缩的自适应视频压缩传感重建)<br>
[paper](https://arxiv.org/abs/2103.01786) | [code](https://github.com/xyvirtualgroup/MetaSCI-CVPR2021)<br><br>

[1] FSDR: Frequency Space Domain Randomization for Domain Generalization(用于域推广的频域随机化)<br>
[paper](https://arxiv.org/abs/2103.02370)<br><br>


<br><br>

<a name="MetricLearning"/> 

### 度量学习(Metric Learning)

[5] SLADE: A Self-Training Framework For Distance Metric Learning(SLADE：远程度量学习的自训练框架)<br>
[paper](https://arxiv.org/abs/2011.10269)<br><br>

[4] MetricOpt: Learning to Optimize Black-Box Evaluation Metrics(MetricOpt：学习优化黑盒评估指标)<br>
[paper](https://arxiv.org/abs/2104.10631)<br><br>

[3] Noise-resistant Deep Metric Learning with Ranking-based Instance Selection(具有基于排名的实例选择的抗噪深度度量学习)<br>
[paper](https://arxiv.org/abs/2103.16047)<br><br>

[2] Embedding Transfer with Label Relaxation for Improved Metric Learning(嵌入转移与标签松弛功能以改善度量学习)<br>
[paper](https://arxiv.org/abs/2103.14908)<br><br>

[1] Dynamic Metric Learning: Towards a Scalable Metric Space to Accommodate Multiple Semantic Scales(动态度量学习：迈向可扩展的度量空间以适应多个语义尺度)<br>
[paper](https://arxiv.org/pdf/2103.11781.pdf) | [code](https://github.com/SupetZYK/DynamicMetricLearning)<br><br>

<br><br>

<a name="ContrastiveLearning"/> 

## 对比学习(Contrastive Learning)

[4] Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning(具有自监督对比学习的全幻灯片图像分类的双流多实例学习网络)<br>
[paper](https://arxiv.org/abs/2011.08939) | [code](https://github.com/binli123/dsmil-wsi)<br><br>

[3] Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification(基于对比学习的混合网络的长尾图像分类)<br>
[paper](https://arxiv.org/abs/2103.14267)<br><br>

[2] AdCo: Adversarial Contrast for Efficient Learning of Unsupervised Representations from Self-Trained Negative Adversaries(有效对比自我训练的负面对抗无监督表示的对抗性对比)<br>
[paper](https://arxiv.org/abs/2011.08435) | [code](https://github.com/maple-research-lab/AdCo) | [解读-AdCo基于对抗的对比学习](https://mp.weixin.qq.com/s/u7Lhzh8uYEEHfWiM32-4yQ)]
<br><br>

[1] Fine-grained Angular Contrastive Learning with Coarse Labels(粗标签的细粒度角度对比学习)<br>
[paper](https://arxiv.org/abs/2012.03515)<br><br>


<br><br>

<a name="IncrementalLearning"/> 

## 增量学习(Incremental Learning)



[4] Few-Shot Incremental Learning with Continually Evolved Classifiers(借助不断发展的分类器进行少量增量学习)<br>
[paper](https://arxiv.org/abs/2104.03047)<br><br>

[3] DER: Dynamically Expandable Representation for Class Incremental Learning(于类增量学习的动态可扩展表示形式)<br>
[paper](https://arxiv.org/abs/2103.16788)<br><br>

[2] Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning(少类别增量学习的语义感知知识蒸馏)<br>
[paper](https://arxiv.org/abs/2103.04059)<br><br>

[1] On Learning the Geodesic Path for Incremental Learning(关于学习增量学习的测地线路径)<br>
[paper](https://arxiv.org/abs/2104.08572)<br><br>


<br><br>
<a name="RL"/> 

## 强化学习(Reinforcement Learning)

[2] Unsupervised Visual Attention and Invariance for Reinforcement Learning(强化学习的无监督视觉注意和不变性)<br>
[paper](https://arxiv.org/abs/2104.02921)<br><br>

[1] Unsupervised Learning for Robust Fitting:A Reinforcement Learning Approach(无监督学习以进行稳健拟合：一种强化学习方法)<br>
[paper](https://arxiv.org/abs/2103.03501)<br><br>

<br><br>
<a name="MetaLearning"/> 

## 元学习(Meta Learning)

[4] Cluster, Split, Fuse, and Update: Meta-Learning for Open Compound Domain Adaptive Semantic Segmentation(集群、拆分、融合和更新：开放复合域自适应语义分割的元学习)<br>
[paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Gong_Cluster_Split_Fuse_and_Update_Meta-Learning_for_Open_Compound_Domain_CVPR_2021_paper.pdf)<br><br>

[3] Faster Meta Update Strategy for Noise-Robust Deep Learning(更快的元更新策略，适用于杂乱无章的深度学习)<br>
[paper](https://arxiv.org/abs/2104.15092)<br><br>

[2] Meta-Mining Discriminative Samples for Kinship Verification(进行亲缘关系验证的元挖掘歧视性样本)<br>
[paper](https://arxiv.org/pdf/2103.15108.pdf)<br><br>

[1] MetaSAug: Meta Semantic Augmentation for Long-Tailed Visual Recognition(MetaSAug：用于长尾视觉识别的元语义增强)<br>
[paper](https://arxiv.org/pdf/2103.12579.pdf)<br><br>

<br><br>

<a name="100"/> 

## 暂无分类

Rotation-Only Bundle Adjustment(仅旋转束调整)<br>
[paper](https://arxiv.org/abs/2011.11724)<br><br>

HDR Environment Map Estimation for Real-Time Augmented Reality(用于实时增强现实的 HDR 环境地图估计)<br>
[paper](https://arxiv.org/abs/2011.10687) | [video](https://docs-assets.developer.apple.com/ml-research/papers/hdr-environment-map.mp4)<br><br>

Magic Layouts: Structural Prior for Component Detection in User Interface Designs(魔术布局：用户界面设计中组件检测的结构先验)<br>
[paper](https://arxiv.org/abs/2106.07615)<br><br>

Unsupervised Part Discovery via Feature Alignment(通过特征对齐的无监督零件发现)<br>
[paper](https://arxiv.org/abs/2012.00313)<br><br>

Neural Prototype Trees for Interpretable Fine-grained Image Recognition(【可解释性】用于可解释细粒度图像识别的神经原型树)<br>
[paper](https://arxiv.org/abs/2012.02046) | [code](https://github.com/M-Nauta/ProtoTree)<br><br>

How Robust are Randomized Smoothing based Defenses to Data Poisoning?(基于随机平滑的数据中毒防御有多强健？)<br>
[paper](https://arxiv.org/abs/2012.01274)<br><br>

Uncalibrated Neural Inverse Rendering for Photometric Stereo of General Surfaces(一般表面光度立体的未校准神经逆渲染)<br>
[paper](https://arxiv.org/abs/2012.06777)<br><br>

The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models(计算机视觉模型中监督和自我监督预训练的彩票假设)<br>
[paper](https://arxiv.org/abs/2012.06908) | [code](https://github.com/VITA-Group/CV_LTH_Pre-training)<br><br>

The Lottery Ticket Hypothesis for Object Recognition(用于对象识别的彩票假设)<br>
[paper](https://arxiv.org/abs/2012.04643)<br><br>

One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing(【视频合成】用于视频会议的一次性自由视角神经谈话头合成)<br>
[paper](https://arxiv.org/abs/2011.15126) | [project](https://nvlabs.github.io/face-vid2vid)<br><br>

Stochastic Image-to-Video Synthesis using cINNs(【视频合成】使用cINN的随机图像到视频合成)<br>
[paper](https://arxiv.org/abs/2105.04551) | [project](https://bit.ly/3t66bnU)<br>

NeRD: Neural 3D Reflection Symmetry Detector(NeRD：神经3D反射对称检测器)<br>
[paper](https://arxiv.org/abs/2105.03211) | [code](https://github.com/zhou13/nerd)<br><br>

Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer RGBD Sensors(【人体体积捕获】Function4D：从非常稀疏的消费类RGBD传感器实时采集人体体积)<br>
[paper](https://arxiv.org/abs/2105.01859) | [project](http://www.liuyebin.com/Function4D/Function4D.html) | [video](https://www.youtube.com/watch?v=-rWUn4fEQNU&t=126s)<br>

AutoFlow: Learning a Better Training Set for Optical Flow(AutoFlow：学习更好的光流训练集)<br>
[paper](https://arxiv.org/abs/2104.14544) | [code](https://autoflow-google.github.io/)<br><br>

Shot Contrastive Self-Supervised Learning for Scene Boundary Detection(【场景边界检测】【对比学习】用于场景边界检测的镜头对比自我监督学习)<br>
[paper](https://arxiv.org/abs/2104.13537)<br><br>

Practical Wide-Angle Portraits Correction with Deep Structured Models(【人像校正】深度结构模型的实用广角人像校正)<br>
[paper](https://arxiv.org/abs/2104.12464)<br><br>

Deep Lucas-Kanade Homography for Multimodal Image Alignment(【图像对齐】用于多模态图像对齐的Deep-Lucas-Kanade单应)
[paper](https://arxiv.org/pdf/2104.11693.pdf) | [code](https://github.com/placeforyiming/CVPR21-Deep-Lucas-Kanade-Homography)<br><br>

Hierarchical Motion Understanding via Motion Programs(【人体动作理解】基于运动程序的分层运动理解)<br>
[paper](https://arxiv.org/abs/2104.11216) | [project](https://sumith1896.github.io/motion2prog/)<br><br>

ManipulaTHOR: A Framework for Visual Object Manipulation(操纵器：一个视觉对象操纵的框架)<br>
[paper](https://arxiv.org/abs/2104.11213)<br><br>

Learning To Count Everything(【视觉计数】学习计算一切)<br>
[paper](https://arxiv.org/abs/2104.08391) | [dataset&code](https://github.com/cvlab-stonybrook/LearningToCountEverything)<br><br>

Ego-Exo: Transferring Visual Representations from Third-person to First-person Videos(Ego-Exo：将视觉表示从第三人称视频转移到第一人称视频)<br>
[paper](https://arxiv.org/pdf/2104.07905.pdf)<br><br>

Harmonious Semantic Line Detection via Maximal Weight Clique Selection(【语义线检测】通过最大权重集团选择进行和谐的语义线检测)<br>
[paper](https://arxiv.org/abs/2104.06903) | [code](https://github.com/dongkwonjin/Semantic-Line-MWCS)<br><br>

Neural Camera Simulators(神经相机模拟器)<br>
[paper](https://arxiv.org/abs/2104.05237)<br><br>

All Labels Are Not Created Equal: Enhancing Semi-supervision via Label Grouping and Co-training(并非所有标签都相等：通过标签分组和共同训练增强半监督)<br>
[paper](https://arxiv.org/abs/2104.05248) | [code](https://github.com/islam-nassar/semco)<br><br>

Shape and Material Capture at Home(在家中进行形状和材料捕获)<br>
[paper](https://arxiv.org/abs/2104.06397) ｜ [project](http://dlichy.github.io/ShapeAndMaterialAtHome/)<br><br>

SOLD2: Self-supervised Occlusion-aware Line Description and Detection(【图像匹配】自我监督的遮挡感知线描述和检测)<br>
[paper](https://arxiv.org/abs/2104.03362) | [code](https://github.com/cvg/SOLD2)<br><br>

Progressive Temporal Feature Alignment Network for Video Inpainting(【视频修复】用于视频修复的渐进时间特征对齐网络)<br>
[paper](https://arxiv.org/abs/2104.03507) | [code](https://github.com/MaureenZOU/TSAM)<br><br>

A Decomposition Model for Stereo Matching(【立体声匹配】立体匹配的分解模型)<br>
[paper](https://arxiv.org/abs/2104.07516)<br><br>

CFNet: Cascade and Fused Cost Volume for Robust Stereo Matching(【立体声匹配】CFNet：稳健的立体声匹配的级联和融合成本)<br>
[paper](https://arxiv.org/abs/2104.04314) | [code](https://github.com/gallenszl/CFNet)<br><br>

SMD-Nets: Stereo Mixture Density Networks(【立体声匹配】立体声混合密度网络)<br>
[paper](https://arxiv.org/abs/2104.03866) | [project](https://github.com/fabiotosi92/SMD-Nets)<br><br>

De-rendering the World's Revolutionary Artefacts(渲染世界革命文物)<br>
[paper](https://arxiv.org/abs/2104.03954) | [project](https://sorderender.github.io/)<br><br>

Learning Triadic Belief Dynamics in Nonverbal Communication from Videos(【视频摘要】从视频中学习非语言交流中的三重性信念动力学)<br>
[paper](https://arxiv.org/abs/2104.02841)<br><br>

Beyond Short Clips: End-to-End Video-Level Learning with Collaborative Memories(超越短片：具有协作记忆的端到端视频级学习)<br>
[paper](https://arxiv.org/abs/2104.01198)<br><br>

Passive Inter-Photon Imaging(被动光子间成像)<br>
[paper](https://arxiv.org/abs/2104.00059)<br><br>

PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting(PhySG：球形高斯逆渲染，用于基于物理的材质编辑和重新照明)<br>
[paper](https://arxiv.org/abs/2104.00674) | [project](https://kai-46.github.io/PhySG-website/)<br><br>

Learning Camera Localization via Dense Scene Matching(【密集场景匹配】通过密集场景匹配学习相机定位)<br>
[paper](https://arxiv.org/abs/2103.16792) | [code](https://github.com/Tangshitao/Dense-Scene-Matching)<br><br>



SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification(半监督分类的类似伪标签开发)<br>
[paper](https://arxiv.org/abs/2103.16725) | [code](http://github.com/zijian-hu/SimPLE)<br><br>

Online Learning of a Probabilistic and Adaptive Scene Representation(概率自适应场景表示的在线学习)<br>
[paper](https://arxiv.org/abs/2103.16832)<br><br>

Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding(拥抱不确定性：去耦和去偏置以实现可靠的实时落地)<br>
[paper](https://arxiv.org/abs/2103.16848)<br><br>

Model-Contrastive Federated Learning(模型对比联合学习)<br>
[paper](https://arxiv.org/abs/2103.16257)<br><br>

Repopulating Street Scenes(重新填充街景)<br>
[paper](https://arxiv.org/abs/2103.16183)<br><br>

Visual Room Rearrangement(视觉室重新布置)<br>
[paper](https://arxiv.org/abs/2103.16544)<br><br>

Tuning IR-cut Filter for Illumination-aware Spectral Reconstruction from RGB(可调红外截止滤光片，用于从RGB感知照明的光谱重建)<br>
[paper](https://arxiv.org/abs/2103.14708)<br><br>

Video Rescaling Networks with Joint Optimization Strategies for Downscaling and Upscaling(具有联合优化策略的视频缩放网络，用于缩小和放大)<br>
[paper](https://arxiv.org/abs/2103.14858)<br><br>

Bilevel Online Adaptation for Out-of-Domain Human Mesh Reconstruction(用于域外人网格重构的双层在线适应)<br>
[paper](https://arxiv.org/abs/2103.16449) | [project](https://sites.google.com/view/humanmeshboa)<br><br>

Picasso: A CUDA-based Library for Deep Learning over 3D Meshes(【网格简化】毕加索：基于CUDA的3D网格深度学习库)<br>
[paper](https://arxiv.org/abs/2103.15076) | [library](https://github.com/hlei-ziyan/Picasso)<br><br>

Cloud2Curve: Generation and Vectorization of Parametric Sketches(参数草图的生成和矢量化)<br>
[paper](https://arxiv.org/abs/2103.15536)<br><br>

Learning Probabilistic Ordinal Embeddings for Uncertainty-Aware Regression(【不确定性学习】学习概率序数嵌入以进行不确定性感知回归)<br>
[paper](https://arxiv.org/abs/2103.13629)<br><br>

SSLayout360: Semi-Supervised Indoor Layout Estimation from 360◦ Panorama(【布局估计】360°全景图的半监督室内布局估计)<br>
[paper](https://arxiv.org/abs/2103.13696)<br><br>

Convex Online Video Frame Subset Selection using Multiple Criteria for Data Efficient Autonomous Driving(使用多种标准的凸面在线视频帧子集选择，以实现数据高效自动驾驶)<br>
[paper](https://arxiv.org/pdf/2103.13021.pdf)<br><br>

Scene-Intuitive Agent for Remote Embodied Visual Grounding(场景直观的代理，用于远程实现可视化接地)<br>
[paper](https://arxiv.org/abs/2103.12944)<br><br>

Relation-aware Instance Refinement for Weakly Supervised Visual Grounding(【visual grounding】弱监督视觉接地的关系感知实例细化)<br>
[paper](https://arxiv.org/pdf/2103.12989.pdf) | [code](https://github.com/youngfly11/ReIR-WeaklyGrounding.pytorch.git)<br><br>

Context-aware Biaffine Localizing Network for Temporal Sentence Grounding(上下文感知的Biaffine本地化网络，用于临时Sentence Grounding)<br>
[paper](https://arxiv.org/pdf/2103.11555.pdf)<br><br>

Dynamic Face Video Segmentation via Reinforcement Learning(通过强化学习进行动态人脸视频分割)<br>
[paper](https://arxiv.org/pdf/1907.01296) | [code](https://github.com/mapleandfire/300VW-Mask)<br><br>

Back to the Feature: Learning Robust Camera Localization from Pixels to Pose(从像素到姿势学习可靠的相机定位)<br>
[paper](https://arxiv.org/pdf/2103.09213.pdf) | [code](https://github.com/cvg/pixloc)<br><br>


Rotation Coordinate Descent for Fast Globally Optimal Rotation Averaging(【优化】旋转坐标下降用于快速全局最优旋转平均)<br>
[paper](https://arxiv.org/pdf/2103.08292.pdf)<br><br>

Affect2MM: Affective Analysis of Multimedia Content Using Emotion Causality(使用情感因果关系对多媒体内容进行情感分析)<br>
[paper](https://arxiv.org/pdf/2103.06541.pdf)<br><br>

Deep Graph Matching under Quadratic Constraint(【图匹配】二次约束下的深度图匹配)<br>
[paper](https://arxiv.org/pdf/2103.06643.pdf)<br><br>

Deep Gaussian Scale Mixture Prior for Spectral Compressive Imaging(用于光谱压缩成像的深高斯比例混合气)<br>
[paper](https://arxiv.org/pdf/2103.07152.pdf) | [code](https://see.xidian.edu.cn/faculty/wsdong/Projects/DGSM-SCI.htm)<br><br>

Limitations of Post-Hoc Feature Alignment for Robustness(健壮性的赛后特征对齐的局限性)<br>
[paper](https://arxiv.org/abs/2103.05898)<br><br>

Consensus Maximisation Using Influences of Monotone Boolean Functions(利用单调布尔函数的影响实现共识最大化)<br>
[paper](https://arxiv.org/pdf/2103.04200.pdf)<br><br>

Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food(实现对通用食品的自动营养理解)<br>
[paper](https://arxiv.org/abs/2103.03375)<br><br>

Structured Scene Memory for Vision-Language Navigation(用于视觉语言导航的结构化场景存储器)<br>
[paper](https://arxiv.org/abs/2103.03454) | [code](https://github.com/HanqingWangAI/SSM-VLN)<br><br>

Learning Asynchronous and Sparse Human-Object Interaction in Videos(视频中异步稀疏人-物交互的学习)<br>
[paper](https://arxiv.org/abs/2103.02758)<br><br>

Self-supervised Geometric Perception(自我监督的几何知觉)<br>
[paper](https://arxiv.org/abs/2103.03114)<br><br>

Quantifying Explainers of Graph Neural Networks in Computational Pathology(计算病理学中图神经网络的量化解释器)<br>
[paper](https://arxiv.org/pdf/2011.12646.pdf)<br><br>

Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts(探索具有对比场景上下文的数据高效3D场景理解)<br>
[paper](http://arxiv.org/abs/2012.09165) | [project](http://sekunde.github.io/project_efficient) | [video](http://youtu.be/E70xToZLgs4)<br><br>

Data-Free Model Extraction(无数据模型提取)<br>
[paper](https://arxiv.org/abs/2011.14779)<br>

Patch-NetVLAD: Multi-Scale Fusion of Locally-Global Descriptors for Place Recognition(用于【位置识别】的局部全局描述符的【多尺度融合】)<br>
[paper](https://arxiv.org/pdf/2103.01486.pdf) | [code](https://github.com/QVPR/Patch-NetVLAD)<br><br>

Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations(适用于正确概念的权利：通过可解释性来修正神经符号概念)<br>
[paper](https://arxiv.org/abs/2011.12854)<br><br>

Hierarchical and Partially Observable Goal-driven Policy Learning with  Goals Relational Graph(基于目标关系图的分层部分可观测目标驱动策略学习)<br>
[paper](https://arxiv.org/abs/2103.01350)<br><br>

Domain Generalization via Inference-time Label-Preserving Target Projections（通过保留推理时间的目标投影进行域泛化）<br>
[paper](https://arxiv.org/pdf/2103.01134.pdf)<br><br>

DeRF: Decomposed Radiance Fields（分解的辐射场）<br>
[project](https://ubc-vision.github.io/derf/)<br><br>

Multi-Objective Interpolation Training for Robustness to Label Noise(多目标插值训练的鲁棒性)<br>
[paper](https://arxiv.org/abs/2012.04462) | [code](https://git.io/JI40X)<br><br>

CDFI: Compression-Driven Network Design for Frame Interpolation(用于帧插值的压缩驱动网络设计)<br>
[paper](https://arxiv.org/pdf/2103.10559.pdf) | [code](https://github.com/tding1/CDFI)<br><br>

FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation（【视频插帧】FLAVR：用于快速帧插值的与流无关的视频表示）<br>
[paper](https://arxiv.org/pdf/2012.08512.pdf) | [code](https://tarun005.github.io/FLAVR/Code) | [project](https://tarun005.github.io/FLAVR/)<br><br>

Deep Animation Video Interpolation in the Wild(【视频插帧】野外深度动画视频插帧)<br>
[paper](https://arxiv.org/abs/2104.02495) | [code&dataset](https://github.com/lisiyao21/AnimeInterp/)<br><br>

Probabilistic Embeddings for Cross-Modal Retrieval（跨模态检索的概率嵌入）<br>
[paper](https://arxiv.org/abs/2101.05068)<br><br>

Self-supervised Simultaneous Multi-Step Prediction of Road Dynamics and Cost Map(道路动力学和成本图的自监督式多步同时预测)<br><br>

IIRC: Incremental Implicitly-Refined Classification(增量式隐式定义的分类)<br>
[paper](https://arxiv.org/abs/2012.12477) | [project](https://chandar-lab.github.io/IIRC/)<br><br>

Fair Attribute Classification through Latent Space De-biasing(通过潜在空间去偏的公平属性分类)<br>
[paper](https://arxiv.org/abs/2012.01469) | [code](https://github.com/princetonvisualai/gan-debiasing) | [project](https://princetonvisualai.github.io/gan-debiasing/)<br><br>

Information-Theoretic Segmentation by Inpainting Error Maximization(修复误差最大化的信息理论分割)<br>
[paper](https://arxiv.org/abs/2012.07287)<br><br>

Kaleido-BERT: Vision-Language Pre-training on Fashion Domain(Kaleido-BERT：时尚领域的视觉语言预训练)<br>
[paper](https://arxiv.org/abs/2103.16110) | [code](https://github.com/mczhuge/Kaleido-Bert)<br><br>

UC2: Universal Cross-lingual Cross-modal Vision-and-Language Pretraining(【视频语言学习】UC2：通用跨语言跨模态视觉和语言预培训)<br><br>

Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling(通过稀疏采样进行视频和语言学习)<br>
[paper](https://arxiv.org/pdf/2102.06183.pdf) | [code](https://github.com/jayleicn/ClipBERT)<br><br>

D-NeRF: Neural Radiance Fields for Dynamic Scenes(D-NeRF：动态场景的神经辐射场)<br>
[paper](https://arxiv.org/abs/2011.13961) | [project](https://www.albertpumarola.com/research/D-NeRF/index.html)<br><br>

FlowStep3D: Model Unrolling for Self-Supervised Scene Flow Estimation(FlowStep3D：用于自监督场景流估计的模型展开)<br>
[paper](https://arxiv.org/abs/2011.10147)<br><br>

Weakly Supervised Learning of Rigid 3D Scene Flow(刚性3D【场景流】的弱监督学习)<br>
[paper](https://arxiv.org/pdf/2102.08945.pdf) | [code](https://arxiv.org/pdf/2102.08945.pdf) | [project](https://3dsceneflow.github.io/)<br><br>

<br>

<a name="2"/> 


# 2. CVPR2021 Oral

[106] Fully Convolutional Networks for Panoptic Segmentation(Oral | 用于全景分割的全卷积网络)<br>
[paper](https://arxiv.org/abs/2012.00720) | [paper](https://github.com/Jia-Research-Lab/PanopticFCN)<br><br>

[105] SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation(SSTVOS：用于视频对象分割的稀疏时空变换器)<br>
[paper](https://arxiv.org/abs/2101.08833) ｜ [code](https://github.com/dukebw/SSTVOS)<br><br>

[104] Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning(具有自监督对比学习的全幻灯片图像分类的双流多实例学习网络)<br>
[paper](https://arxiv.org/abs/2011.08939) | [code](https://github.com/binli123/dsmil-wsi)<br><br>

[103] Shared Cross-Modal Trajectory Prediction for Autonomous Driving(自动驾驶的共享跨模态轨迹预测)<br>
[paper](https://arxiv.org/abs/2011.08436)<br><br>

[102] Open-Vocabulary Object Detection Using Captions(使用字幕的开放词汇对象检测)<br>
[paper](https://arxiv.org/abs/2011.10678)<br><br>

[101] Task Programming: Learning Data Efficient Behavior Representations(任务编程：学习数据高效行为表征)<br>
[paper](https://arxiv.org/abs/2011.13917) ｜ [code](https://github.com/neuroethology/TREBA) | [project](https://sites.google.com/view/task-programming)<br><br>

[100] One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing(用于视频会议的一次性自由视角神经谈话头合成)<br>
[paper](https://arxiv.org/abs/2011.15126) | [project](https://nvlabs.github.io/face-vid2vid)<br><br>

[99] Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization(通过对比交叉视图互信息最大化学习视图解开人体姿势表示)<br>
[paper](https://arxiv.org/abs/2012.01405) | [code](https://github.com/google-research/google-research/tree/master/poem)<br><br>

[98] Correlated Input-Dependent Label Noise in Large-Scale Image Classification(大规模图像分类中的关联输入相关标签噪声)<br>
[paper](https://arxiv.org/abs/2105.10305)<br><br>

[97] A Fourier-based Framework for Domain Generalization(基于傅立叶的域泛化框架)<br>
[paper](https://arxiv.org/abs/2105.11120)<br><br>

[96] Omnimatte: Associating Objects and Their Effects in Video(Omnimatte：在视频中关联对象及其效果)<br>
[paper](https://arxiv.org/abs/2105.06993) | [project](https://omnimatte.github.io/)<br><br>

[95] CoCosNet v2: Full-Resolution Correspondence Learning for Image Translation(CoCosNet v2：用于图像翻译的全分辨率函授学习)<br>
[paper](https://arxiv.org/abs/2012.02047)<br><br>

[94] MOS: Towards Scaling Out-of-distribution Detection for Large Semantic Space(MOS：面向大型语义空间的规模化异常样本检测)<br>
[paper](https://arxiv.org/abs/2105.01879)<br><br>

[93] Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer RGBD Sensors(【人体体积捕获】Function4D：从非常稀疏的消费类RGBD传感器实时采集人体体积)<br>
[paper](https://arxiv.org/abs/2105.01859) | [project](http://www.liuyebin.com/Function4D/Function4D.html) | [video](https://www.youtube.com/watch?v=-rWUn4fEQNU&t=126s)<br>

[92] Deep Polarization Imaging for 3D shape and SVBRDF Acquisition(用于3D形状和SVBRDF采集的深偏振成像)<br>
[paper](https://arxiv.org/abs/2105.02875)<br><br>

[91] GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving(GeoSim：通过可自动驾驶的几何感知合成进行逼真的视频模拟)<br>
[paper](https://arxiv.org/abs/2101.06543)<br><br>

[90] GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields(GIRAFFE：将场景表示为合成的生成神经特征场)<br>
[paper](https://arxiv.org/abs/2011.12100) | [project](http://bit.ly/giraffe-project)<br><br>

[89] DriveGAN: Towards a Controllable High-Quality Neural Simulation(DriveGAN：迈向可控的高质量神经仿真)<br>
[paper](https://arxiv.org/abs/2104.15060)<br><br>

[88] HOTR: End-to-End Human-Object Interaction Detection with Transformers(HOTR：使用变压器进行端到端的人与对象交互检测)<br>
[paper](https://arxiv.org/abs/2104.13682)<br><br>

[87] FrameExit: Conditional Early Exiting for Efficient Video Recognition(【视频理解】帧退出：有条件提前退出以实现有效的视频识别)<br>
[paper](https://arxiv.org/abs/2104.13400)<br><br>

[86] Unsupervised Multi-Source Domain Adaptation for Person Re-Identification(用于行人重新识别的无监督多源域适配)<br>
[paper](https://arxiv.org/abs/2104.12961)<br><br>

[85] Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets(寻求有效注释大型图像分类数据集的良好做法)<br>
[paper](https://arxiv.org/abs/2104.12690) | [project](https://fidler-lab.github.io/efficient-annotation-cookbook)<br><br>

[84] KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control(【3D关键点】关键点变形器：用于形状控制的无监督三维关键点发现)<br>
[paper](https://arxiv.org/abs/2104.11224) | [project](http://tomasjakab.github.io/KeypointDeformer)<br><br>

[85] ManipulaTHOR: A Framework for Visual Object Manipulation(操纵器：一个视觉对象操纵的框架)<br>
[paper](https://arxiv.org/abs/2104.11213)<br><br>

[84] Variational Relational Point Completion Network(变分关系点完备网络)<br>
[paper](https://arxiv.org/abs/2104.10154) | [project](https://paul007pl.github.io/projects/VRCNet.html)<br><br>

[83] Style-Aware Normalized Loss for Improving Arbitrary Style Transfer(一种改进任意风格转换的风格感知归一化损失算法)<br>
[paper](https://arxiv.org/abs/2104.10064)<br><br>

[82] Guided Interactive Video Object Segmentation Using Reliability-Based Attention Maps(基于可靠性的注意映射引导交互式视频对象分割)<br>
[paper](https://arxiv.org/abs/2104.10386) | [code](https://github.com/yuk6heo/GIS-RAmap)<br><br>

[81] MetricOpt: Learning to Optimize Black-Box Evaluation Metrics(MetricOpt：学习优化黑盒评估指标)<br>
[paper](https://arxiv.org/abs/2104.10631)<br><br>

[80] LAFEAT: Piercing Through Adversarial Defenses with Latent Features(LAFEAT：通过具有潜在功能的对抗性防御突围)<br>
[paper](https://arxiv.org/abs/2104.09284)<br><br>

[79] Single-view robot pose and joint angle estimation via render & compare(通过渲染和比较进行单视图机器人姿态和关节角度估计)<br>
[paper](https://arxiv.org/abs/2104.09359) | [code](https://www.di.ens.fr/willow/research/robopose/)<br><br>

[78] Temporal Query Networks for Fine-grained Video Understanding(时间查询网络，用于细粒度的视频理解)<br>
[paper](https://arxiv.org/abs/2104.09496) | [project](http://www.robots.ox.ac.uk/~vgg/research/tqn/)<br><br>

[77] Divide-and-Conquer for Lane-Aware Diverse Trajectory Prediction(车道感知不同轨迹预测的分而治之)<br>
[paper](https://arxiv.org/abs/2104.08277)<br><br>

[76] Fusing the Old with the New: Learning Relative Camera Pose with Geometry-Guided Uncertainty(新旧融合：通过几何引导的不确定性学习相对相机姿势)<br>
[paper](https://arxiv.org/abs/2104.08278)<br><br>

[75] DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort(DatasetGAN：只需最少的人力即可获得的高效标签数据工厂)<br>
[paper](https://arxiv.org/abs/2104.06490)<br><br>

[74] Pixel Codec Avatars(像素编解码器头像)<br>
[paper](https://arxiv.org/abs/2104.04638)<br><br>

[73] CodedStereo: Learned Phase Masks for Large Depth-of-field Stereo(CodedStereo：为大景深立体声而设计的相位掩模)<br>
[paper](https://arxiv.org/abs/2104.04641)<br><br>

[72] Rethinking and Improving the Robustness of Image Style Transfer(重新思考和改善图像风格迁移的鲁棒性)<br>
[paper](https://arxiv.org/abs/2104.05623)<br><br>

[71] Simpler Certified Radius Maximization by Propagating Covariances(通过传播协方差简化认证半径最大化)<br>
[paper](https://arxiv.org/abs/2104.05888) | [video](https://youtu.be/m1ya2oNf5iE)<br><br>

[70] Global Transport for Fluid Reconstruction with Learned Self-Supervision(具有自学指导的流体重建的全球运输)<br>
[paper](https://arxiv.org/abs/2104.06031) | [code](https://github.com/tum-pbs/Global-Flow-Transport)<br><br>

[69] SOLD2: Self-supervised Occlusion-aware Line Description and Detection(【图像匹配】自我监督的遮挡感知线描述和检测)<br>
[paper](https://arxiv.org/abs/2104.03362) | [code](https://github.com/cvg/SOLD2)<br><br>

[68] InverseForm: A Loss Function for Structured Boundary-Aware Segmentation(结构化边界感知分割的损失函数)<br>
[paper](https://arxiv.org/abs/2104.02745)<br><br>

[67] Learning Triadic Belief Dynamics in Nonverbal Communication from Videos(【视频摘要】从视频中学习非语言交流中的三重性信念动力学)<br>
[paper](https://arxiv.org/abs/2104.02841)<br><br>

[66] Adversarial Robustness under Long-Tailed Distribution(长尾分布下的对抗鲁棒性)<br>
[paper](https://arxiv.org/abs/2104.02703) | [code](https://github.com/wutong16/Adversarial_Long-Tail)<br><br>

[65] S2R-DepthNet: Learning a Generalizable Depth-specific Structural Representation(学习通用的深度特定的结构表示)<br>
[paper](https://arxiv.org/abs/2104.00877)<br><br>

[64] Video Prediction Recalling Long-term Motion Context via Memory Alignment Learning(【视频预测】通过记忆对准学习的视频预测调用长期运动环境)<br>
[paper](https://arxiv.org/abs/2104.00924)<br><br>

[63] Passive Inter-Photon Imaging(被动光子间成像)<br>
[paper](https://arxiv.org/abs/2104.00059)<br><br>

[62] Jigsaw Clustering for Unsupervised Visual Representation Learning(拼图聚类的无监督视觉表示学习)<br>
[paper](https://arxiv.org/abs/2104.00323) | [code](https://github.com/Jia-Research-Lab/JigsawClustering)<br><br>

[61] Reconstructing 3D Human Pose by Watching Humans in the Mirror(通过照镜子中的人来重建3D人的姿势)<br>
[paper](https://arxiv.org/abs/2104.00340) | [project](https://zju3dv.github.io/Mirrored-Human/)<br><br>

[60] Towards Evaluating and Training Verifiably Robust Neural Networks(评估和训练可验证的稳健神经网络)<br>
[paper](https://arxiv.org/abs/2104.00447) | [code](https://github.com/ZhaoyangLyu/VerifiablyRobustNN)<br><br>

[59] LED2-Net: Monocular 360 Layout Estimation via Differentiable Depth Rendering(通过可分辨深度渲染进行单眼360布局估算)<br>
[paper](https://arxiv.org/abs/2104.00568) | [project](https://fuenwang.ml/project/led2net)<br><br>

[58] A Realistic Evaluation of Semi-Supervised Learning for Fine-Grained Classification(细粒度分类的半监督学习的现实评估)<br>
[paper](https://arxiv.org/abs/2104.00679)<br><br>

[57] SimPoE: Simulated Character Control for 3D Human Pose Estimation(用于3D人体姿势估计的模拟角色控制)<br>
[paper](https://arxiv.org/abs/2104.00683) | [project](https://www.ye-yuan.com/simpoe/)<br><br>

[56] DER: Dynamically Expandable Representation for Class Incremental Learning(【增量学习】用于类增量学习的动态可扩展表示形式)<br>
[paper](https://arxiv.org/abs/2103.16788)<br><br>

[55] Convolutional Hough Matching Networks(卷积霍夫匹配网络)<br>
[paper](https://arxiv.org/abs/2103.16831)<br><br>

[54] A Closer Look at Fourier Spectrum Discrepancies for CNN-generated Images Detection(仔细研究CNN生成图像检测的傅立叶光谱差异)<br>
[paper](https://arxiv.org/abs/2103.17195) | [code](https://keshik6.github.io/Fourier-Discrepancies-CNN-Detection/)<br><br>

[53] DiNTS: Differentiable Neural Network Topology Search for 3D Medical Image Segmentation(DiNTS：用于3D医学图像分割的可区分神经网络拓扑搜索)<br>
[paper](https://arxiv.org/abs/2103.15954)<br><br>

[52] Face Forensics in the Wild(人脸伪造数据集)<br>
[paper](https://arxiv.org/abs/2103.16076) | [paper](https://github.com/tfzhou/FFIW)<br><br>

[51] Fully Convolutional Scene Graph Generation(全卷积场景图生成)<br>
[paper](https://arxiv.org/abs/2103.16083)<br><br>

[50] Visual Room Rearrangement(视觉室重新布置)<br>
[paper](https://arxiv.org/abs/2103.16544)<br><br>

[49] Adaptive Methods for Real-World Domain Generalization(真实世界域自适应的自适应方法)<br>
[paper](https://arxiv.org/abs/2103.15796)<br><br>

[48] Tuning IR-cut Filter for Illumination-aware Spectral Reconstruction from RGB(可调红外截止滤光片，用于从RGB感知照明的光谱重建)<br>
[paper](https://arxiv.org/abs/2103.14708)<br><br>

[47] Learning Placeholders for Open-Set Recognition(学习占位符以进行开放式识别)<br>
[paper](https://arxiv.org/abs/2103.15086)<br><br>

[46] Zero-shot Adversarial Quantization(零样本对抗量化)<br>
[paper](https://arxiv.org/abs/2103.15263) | [code](https://git.io/Jqc0y)<br><br>

[45] POSEFusion: Pose-guided Selective Fusion for Single-view Human Volumetric Capture(用于单视图人体体积捕获的姿势引导选择性融合)<br>
[paper](https://arxiv.org/abs/2103.15331) | [project](http://www.liuyebin.com/posefusion/posefusion.html)<br><br>

[44] RobustNet: Improving Domain Generalization in Urban-Scene Segmentation via Instance Selective Whitening(通过实例选择性增白提高城市场景分割中的域泛化)<br>
[paper](https://arxiv.org/abs/2103.15597) | [code](https://github.com/shachoi/RobustNet)<br><br>

[43] Bidirectional Projection Network for Cross Dimension Scene Understanding(【场景理解】双向投影网络，用于跨维度场景理解)<br>
[paper](https://arxiv.org/abs/2103.14326) | [code](https://github.com/wbhu/BPNet)<br><br>

[42] Dynamic Slimmable Network(动态可压缩网络)<br>
[paper](https://arxiv.org/abs/2103.13258) | [code](https://github.com/changlin31/DS-Net)<br><br>

[41] Scaling Local Self-Attention For Parameter Efficient Visual Backbones(扩展局部自注意力以获得有效的参数视觉主干)<br>
[paper](https://arxiv.org/pdf/2103.12731.pdf)<br><br>

[40] PGT: A Progressive Method for Training Models on Long Videos(一种在长视频上训练模型的渐进方法)<br>
[paper](https://arxiv.org/pdf/2103.11313.pdf) | [code](https://github.com/BoPang1996/PGT)<br><br>

[39] Brain Image Synthesis with Unsupervised Multivariate Canonical CSCℓ4Net(无监督多元规范CSCℓ4Net的脑图像合成)<br>
[paper](https://arxiv.org/pdf/2103.11587.pdf)<br><br>

[38] Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking(Transformer与追踪器相遇：利用时间上下文进行可靠的视觉追踪)<br>
[paper](https://arxiv.org/pdf/2103.11681)<br><br>

[37] Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion(具有深度敏感注意力和自动多模态融合的深度RGB-D显著性检测)<br>
[paper](https://arxiv.org/abs/2103.11832)<br><br>

[36] Rotation Coordinate Descent for Fast Globally Optimal Rotation Averaging(【优化】旋转坐标下降用于快速全局最优旋转平均)<br>
[paper](https://arxiv.org/pdf/2103.08292.pdf)<br><br>

[35] MagFace: A Universal Representation for Face Recognition and Quality Assessment(MagFace：人脸识别和质量评估的通用表示形式)<br>
[paper](https://arxiv.org/abs/2103.06627) | [code](https://github.com/IrvingMeng/MagFace)<br><br>

[34] CoMoGAN: continuous model-guided image-to-image translation(连续的模型指导的图像到图像翻译)<br>
[paper](https://arxiv.org/abs/2103.06879) | [code](http://github.com/cv-rits/CoMoGAN)<br><br>

[33] FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism(具有分离旋转机制的类别级6D对象姿势估计的快速基于形状的网络)<br>
[paper](https://arxiv.org/abs/2103.07054)<br><br>

[32] Knowledge Evolution in Neural Networks(神经网络中的知识进化)<br>
[paper](https://arxiv.org/pdf/2103.05152.pdf) | [code](https://github.com/ahmdtaha/knowledge_evolution)<br><br>

[31] NeX: Real-time View Synthesis with Neural Basis Expansion(NeX：具有神经基础扩展的实时视图合成)<br>
[paper](https://arxiv.org/abs/2103.05606) | [code](https://nex-mpi.github.io/)<br><br>

[30] ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis(进行全面伪造分析的多功能基准)<br>
[paper](https://arxiv.org/abs/2103.05630) | [code](https://yinanhe.github.io/projects/forgerynet.html)<br><br>

[29] Dense Contrastive Learning for Self-Supervised Visual Pre-Training(自监督视觉预训练的密集对比学习)<br>
[paper](https://arxiv.org/abs/2011.09157) | [code](https://github.com/WXinlong/DenseCL)<br><br>

[28] Consensus Maximisation Using Influences of Monotone Boolean Functions(利用单调布尔函数的影响实现共识最大化)<br>
[paper](https://arxiv.org/pdf/2103.04200.pdf)<br><br>

[27] Differentiable Multi-Granularity Human Representation Learning for Instance-Aware Human Semantic Parsing(用于实例感知人类语义解析的可微分多粒度人类表示学习)<br>
[paper](https://arxiv.org/pdf/2103.04570.pdf) | [code](https://github.com/tfzhou/MG-HumanParsing)<br><br>

[26] Discovering Hidden Physics Behind Transport Dynamics(在运输动力学背后发现隐藏的物理)<br>
[paper](https://arxiv.org/pdf/2011.12222v1.pdf)<br><br>

[25] Learning Continuous Image Representation with Local Implicit Image Function(通过局部隐含图像功能学习连续图像表示)<br>
[paepr](https://arxiv.org/abs/2012.09161) | [code](https://github.com/yinboc/liif) | [video](https://youtu.be/6f2roieSY_8) | [project](https://yinboc.github.io/liif/) | [解读-真正的无极放大！30x插值效果惊艳，英伟达等开源LIIF：巧妙的图像超分新思路](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247529081&idx=1&sn=2f6fa79081a85b21bbdb10ace3852f78&chksm=ec1c9f80db6b16969f049f6c389dc9250ba661c31bad69f3490eb0b9561f264f19434a65c458&token=2008688100&lang=zh_CN#rd)<br><br>

[24] UP-DETR: Unsupervised Pre-training for Object Detection with Transformers<br>
[paper](https://arxiv.org/pdf/2011.09094.pdf) | [code](https://github.com/dddzg/up-detr)<br>
解读：[无监督预训练检测器](https://www.zhihu.com/question/432321109/answer/1606004872)<br><br>

[23] Self-supervised Geometric Perception(自我监督的几何知觉)<br>
[paper](https://arxiv.org/abs/2103.03114)<br><br>

[22] DeepTag: An Unsupervised Deep Learning Method for Motion Tracking on  Cardiac Tagging Magnetic Resonance Images(一种心脏标记磁共振图像运动跟踪的无监督深度学习方法)<br>
[paper](https://arxiv.org/abs/2103.02772)<br><br>

[21] Modeling Multi-Label Action Dependencies for Temporal Action Localization(为时间动作定位建模多标签动作相关性)<br>
[paper](https://arxiv.org/pdf/2103.03027.pdf)<br><br>

[20] HPS: localizing and tracking people in large 3D scenes from wearable sensors(通过可穿戴式传感器对大型3D场景中的人进行定位和跟踪)<br><br>

[19] Real-Time High Resolution Background Matting(实时高分辨率背景抠像)<br>
[paper](https://arxiv.org/abs/2012.07810) | [code](https://github.com/PeterL1n/BackgroundMattingV2) | [project](https://grail.cs.washington.edu/projects/background-matting-v2/) | [video](https://youtu.be/oMfPTeYDF9g)<br><br>

[18] Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts(探索具有对比场景上下文的数据高效3D场景理解)<br>
[paper](http://arxiv.org/abs/2012.09165) | [project](http://sekunde.github.io/project_efficient) | [video](http://youtu.be/E70xToZLgs4)<br><br>

[17] Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments(在动态室内环境中，通过空间划分的鲁棒神经路由可实现摄像机的重新定位)<br>
[paper](https://arxiv.org/abs/2012.04746) | [project](https://ai.stanford.edu/~hewang/)<br><br>

[16] MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan Synchronization(通过3D扫描同步进行多主体分割和运动估计)<br>
[paper](https://arxiv.org/pdf/2101.06605.pdf) | [code](https://github.com/huangjh-pub/multibody-sync)<br><br>

[15] Categorical Depth Distribution Network for Monocular 3D Object Detection(用于单目三维目标检测的分类深度分布网络)<br>
[paper](https://arxiv.org/abs/2103.01100)<br><br>

[14] PatchmatchNet: Learned Multi-View Patchmatch Stereo(学习多视图立体声)<br>
[paper](https://arxiv.org/abs/2012.01411) | [code](https://github.com/FangjinhuaWang/PatchmatchNet)

[13] Continual Adaptation of Visual Representations via Domain Randomization and Meta-learning(通过域随机化和元学习对视觉表示进行连续调整)<br>
[paper](https://arxiv.org/abs/2012.04324)<br><br>

[12] Single-Stage Instance Shadow Detection with Bidirectional Relation Learning(具有双向关系学习的单阶段实例阴影检测)<br><br>

[11] Neural Geometric Level of Detail:Real-time Rendering with Implicit 3D Surfaces(神经几何细节水平：隐式3D曲面的实时渲染)<br>
[paper](https://arxiv.org/abs/2101.10994) | [code](https://github.com/nv-tlabs/nglod) | [project](https://nv-tlabs.github.io/nglod/)<br><br>

[9] PREDATOR: Registration of 3D Point Clouds with Low Overlap(预测器：低重叠的3D点云的配准)<br>
[paper](https://arxiv.org/pdf/2011.13005.pdf) | [code](https://github.com/ShengyuH/OverlapPredator) | [project](https://overlappredator.github.io/)<br><br>

[8] Domain Generalization via Inference-time Label-Preserving Target Projections(通过保留推理时间的目标投影进行域泛化)<br>
[paper](https://arxiv.org/abs/2103.01134)<br><br>

[7] Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction(全局一致的非刚性重建的神经变形图)<br>
[paper](https://arxiv.org/abs/2012.01451) | [project](https://aljazbozic.github.io/neural_deformation_graphs/) | [video](https://youtu.be/vyq36eFkdWo)<br><br>

[6] Fine-grained Angular Contrastive Learning with Coarse Labels(粗标签的细粒度角度对比学习)<br>
[paper](https://arxiv.org/abs/2012.03515)<br><br>

[5] Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling(通过稀疏采样进行视频和语言学习)<br>
[paper](https://arxiv.org/pdf/2102.06183.pdf) | [code](https://github.com/jayleicn/ClipBERT)<br><br>

[4] Cross-View Regularization for Domain Adaptive Panoptic Segmentation(用于域自适应全景分割的跨视图正则化)<br>
[paper](https://arxiv.org/abs/2103.02584)<br><br>

[3] Image-to-image Translation via Hierarchical Style Disentanglement(通过分层样式分解实现图像到图像的翻译)<br>
[paper](https://arxiv.org/abs/2103.01456) | [code](https://github.com/imlixinyang/HiSD)<br><br>

[2] Towards Open World Object Detection(开放世界中的目标检测)<br>
[paper](https://arxiv.org/abs/2103.02603) | [code](https://github.com/JosephKJ/OWOD)<br><br>

[1] End-to-End Video Instance Segmentation with Transformers(使用Transformer的端到端视频实例分割) <br>
[paper](https://arxiv.org/abs/2011.14503)<br><br>

<br>

<a name="3"/> 

# CVPR2021 论文解读汇总

【13】[无监督预训练检测器(CVPR2021 Oral)](https://zhuanlan.zhihu.com/p/313684358)<br>
无监督预训练模型无论是在nlp(BERT,GPT,XLNet)还是在cv(MoCo,SimCLR,BYOL)上都取得了突破性的进展。而对于无监督（自监督）预训练而言，最重要的就是设计一个合理的pretext，典型的像BERT的masked language model，MoCo的instance discrimination。他们都通过一定的方式，从样本中无监督的构造了一个"label"，从而对模型进行预训练，提高下游任务的表现。那么，对于DETR而言，既然CNN可以是无监督预训练的，那么transformer能不能也无监督预训练一下？<br>
[paper](https://arxiv.org/abs/2011.09094) | [code](https://github.com/dddzg/up-detr)
<br><br>

【12】[GFLV2：目标检测良心技术，无Cost涨点！](https://zhuanlan.zhihu.com/p/313684358)<br>
本文是检测领域首次引入用边界框的不确定性的统计量来高效地指导定位质量估计，从而基本无cost（包括在训练和测试阶段）地提升one-stage的检测器性能，涨幅在1~2个点AP。<br>
[paper](https://arxiv.org/abs/2011.12885) | [code](https://github.com/implus/GFocalV2)
<br><br>

【11】[DCL：旋转目标检测新方法](https://zhuanlan.zhihu.com/p/354373013)<br>
Densely Coded Labels (DCL)是 Circular Smooth Label (CSL)的优化版本。DCL主要从两方面进行了优化：过于厚重的预测层以及对类正方形目标检测的不友好。<br>
[paper](https://arxiv.org/abs/2011.09670) | [code](https://github.com/yangxue0827/RotationDetection)
<br><br>

【10】[层次风格解耦：人脸多属性篡改终于可控了(CVPR2021 Oral)](https://zhuanlan.zhihu.com/p/354258056)<br>
从CycleGAN提出后，图像翻译面临的最大的两个问题就是扩展性（同时处理多种篡改）和多样性（生成不同的结果），然而，一直没有一个很好的方法，可以兼顾扩展性和多样性的同时，又能使得这种篡改满足预期。例如，对于人脸属性篡改任务，我们想要给人脸加上刘海，可是却改变了发色或是背景，再例如，我们想要给人脸加上眼睛，结果竟然性别和年龄也改变了。HiSD就是为了解决这些问题，并且还同时支持从噪声中生成或者从图像中提取这样的风格。<br>
[paper](https://arxiv.org/abs/2103.01456) | [code](https://github.com/imlixinyang/HiSD)
<br><br>

【9】[Transformer再下一城！low-level多个任务榜首被占领，北大华为等联合提出预训练模型IPT](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247524529&idx=1&sn=e39e67981b2afd9a5369cc843ddf28fe&chksm=ec1c8d48db6b045e7cf2d37c5633da8d3caf5e53178fe6df0913f2a04bc143fcc2e504d6f9be&token=2008688100&lang=zh_CN#rd)<br>
对low-level计算机视觉任务（比如降噪、超分、去雨）进行了研究并提出了一种新的预训练模型：IPT(image processing transformer)。为最大挖掘transformer的能力，作者采用知名的ImageNet制作了大量的退化图像数据对，然后采用这些训练数据对对所提IPT(它具有多头、多尾以适配多种退化降质模型)模型进行训练。此外，作者还引入了对比学习以更好的适配不同的图像处理任务。经过微调后，预训练模型可以有效的应用不到的任务中。仅仅需要一个预训练模型，IPT即可在多个low-level基准上取得优于SOTA方案的性能。<br>
[paper](https://arxiv.org/abs/2012.00364)
<br><br>

【8】[真正的无极放大！30x插值效果惊艳，英伟达等开源LIIF：巧妙的图像超分新思路](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247529081&idx=1&sn=2f6fa79081a85b21bbdb10ace3852f78&chksm=ec1c9f80db6b16969f049f6c389dc9250ba661c31bad69f3490eb0b9561f264f19434a65c458&token=2008688100&lang=zh_CN#rd)<br>
一种新颖的连续图像表达方案。它在离散2D图像与连续2D图像之间构建了一种巧妙的连接。受益于所提方法的“连续表达”，它能够对图像进行分辨率调整，做到了真正意义上的“无极放大”，甚至可以进行30x的放大处理。<br>
[paper](https://arxiv.org/abs/2012.09161) | [code](https://github.com/yinboc/liif) | [video](https://youtu.be/6f2roieSY_8) | [project](https://yinboc.github.io/liif/)
<br><br>

【7】[AdCo基于对抗的对比学习](https://mp.weixin.qq.com/s/u7Lhzh8uYEEHfWiM32-4yQ)<br>
自监督学习领域，基于contrastive learning（对比学习）的思路已经在下游分类检测和任务中取得了明显的优势。其中如何充分利用负样本提高学习效率和学习效果一直是一个值得探索的方向，本文第一次全新提出了用对抗的思路end-to-end来直接学习负样本，在ImageNet和下游任务均达到SOTA。AdCo仅仅用8196个负样本（八分之一的MoCo v2的负样本量），就能达到与之相同的精度。同时，这些可直接训练的负样本在和BYOL中Prediction MLP参数量相同的情况下依然能够取得相似的效果。这说明了在自监督学习时代，通过将负样本可学习化，对比学习仍然具有学习效率高、训练稳定和精度高等一系列优势。<br>
[paper](https://arxiv.org/abs/2011.08435) | [code](https://github.com/maple-research-lab/AdCo)
<br><br>

【6】[超分性能不降低，计算量降低50%：加速图像超分的ClassSR](https://zhuanlan.zhihu.com/p/355873199)<br>
本文是在low-level领域关于超分网络加速的一次探索。它创新性的将分类与超分进行了融合，根据不同子块的复原难度自适应选择合适的超分分支以降低整体计算复杂度：复原难度低的平坦区域选择复杂度低的超分分支，复原难度高的纹理区域选择复杂度高的超分分支。在不降低超分性能的情况下，该方法可以最高可以节省50%的计算量。<br>
[paper](https://arxiv.org/abs/2103.04039)
<br><br>

【5】[ MotionRNN：针对复杂时空运动的通用视频预测模型](https://zhuanlan.zhihu.com/p/355703957)<br>
视频预测方法被广泛应用于降水预报（Precipitation Nowcasting）、交通流预测（Traffic Flow Prediction）、机器人视觉规划（Visual Planning）等众多任务中。然而现实世界的运动极其复杂，且往往处于不断变化中，比如人体运动中的变向、变速、肢体运动，雷达回波中的云团产生、消散、位移、形变等等。这种复杂的时空变化使得准确预测未来的运动极具挑战性。<br>
针对复杂时空运动，我们关注到现实世界的运动在时空上可以分解为整体运动趋势（motion trend）与瞬时变化（transient variation），并基于此提出了名为MotionRNN的模型，对运动趋势与瞬时变化进行统一建模。同时，作为一个通用的视频预测模型，MotionRNN具有很好的灵活性，可以结合众多的基于RNN的时空预测模型，稳定提升它们应对复杂时空运动的能力。<br>
[paper](https://arxiv.org/abs/2103.02243)
<br><br>

【4】[Statistical Texture Learning](https://zhuanlan.zhihu.com/p/354501947)<br>
从底层细节纹理分析与增强优化视觉学习问题，并在分割任务上得到了验证，直观、合理且有效涨点。我们从传统图像分析领域获得灵感，构建了这样一套Statistical Texture Learning框架，有效的在CNN架构中学习底层纹理（分析+增强），从而获得了非常有效的性能涨点。<br>
[paper](https://arxiv.org/abs/2103.04133)
<br><br>

【3】[二次元妹子五官画风都能改，周博磊团队用无监督方法控制GAN(CVPR2021 Oral)](https://mp.weixin.qq.com/s/JmGCwi4QF5VVzujj7KIBwg)<br>
现在，GAN不仅能画出二次元妹子，还能精准调节五官、表情、姿势和绘画风格。而且在调控某个因素的时候，其他条件能尽量保持不变。SeFa适用于PGGAN、StyleGAN、BigGAN和StyleGAN2等常见GAN模型，不仅对二次元妹子有效，甚至还能调控猫咪上下左右不同方向。<br>
[paper](https://arxiv.org/abs/2007.06600) | [code](https://github.com/genforce/sefa) | [Colab](https://colab.research.google.com/github/genforce/sefa/blob/master/docs/SeFa.ipynb)
<br><br>

【2】[Inception convolution](https://zhuanlan.zhihu.com/p/354194188)<br>
我们最近被CVPR2021接受的工作，主要使用一些优化手段来找到新的卷积模式，目标是能够找到一个部署友好简单的卷积来帮助下游各个任务更好的提升baseline。<br>
[paper](https://arxiv.org/pdf/2012.13587.pdf) | [code](https://github.com/yifan123/IC-Conv)
<br><br>

【1】[RepVGG：极简架构，SOTA性能，让VGG式模型再次伟大（CVPR-2021）](https://zhuanlan.zhihu.com/p/344324470)<br>
我们最近的工作RepVGG，用结构重参数化（structural re-parameterization）实现VGG式单路极简架构，一路3x3卷到底，在速度和性能上达到SOTA水平，在ImageNet上超过80%正确率。已经被CVPR-2021接收。不用NAS，不用attention，不用各种新颖的激活函数，甚至不用分支结构，只用3x3卷积和ReLU，也能达到SOTA性能。<br>
[paper](https://arxiv.org/abs/2101.03697) | [开源预训练模型和代码（PyTorch版）](https://github.com/DingXiaoH/RepVGG) | [MegEngine版](https://github.com/megvii-model/RepVGG)


<br>

<a name="4"/> 

# 4. CVPR2021 Workshop

[99] Learning from Incomplete Features by Simultaneous Training of Neural Networks and Sparse Coding(通过同时训练神经网络和稀疏编码从不完整特征中学习)<br>
[paper](https://arxiv.org/abs/2011.14047)<br><br>

[98] Anticipative Video Transformer(预期视频转换器)<br>
[paper](https://arxiv.org/abs/2106.02036) | [project](http://facebookresearch.github.io/AVT)<br><br>

[97] Occlusion Guided Scene Flow Estimation on 3D Point Clouds(3D 点云上的遮挡引导场景流估计)<br>
[paper](https://arxiv.org/abs/2011.14880)<br><br>

[96] ideo-based Person Re-identification without Bells and Whistles(基于 ideo 的人员重新识别)<br>
[paper](https://arxiv.org/abs/2105.10678) | [code](https://github.com/jackie840129/CF-AAN)<br><br>

[95] ADNet: Attention-guided Deformable Convolutional Network for High Dynamic Range Imaging(ADNet：用于高动态范围成像的注意力引导可变形卷积网络)<br>
[paper](https://arxiv.org/abs/2105.10697)<br><br>

[94] PAL: Intelligence Augmentation using Egocentric Visual Context Detection(PAL：使用以自我为中心的视觉上下文检测的智能增强)<br>
[paper](https://arxiv.org/abs/2105.10735)<br><br>

[93] PLM: Partial Label Masking for Imbalanced Multi-label Classification(PLM：不平衡多标签分类的部分标签屏蔽)<br>
[paper](https://arxiv.org/abs/2105.10782)<br><br>

[92] GOO: A Dataset for Gaze Object Prediction in Retail Environments(GOO：零售环境中注视对象预测的数据集)<br>
[paper](https://arxiv.org/abs/2105.10793)<br><br>

[91] Wisdom for the Crowd: Discoursive Power in Annotation Instructions for Computer Vision(大众的智慧：计算机视觉注释指令中的话语权)<br>
[paper](https://arxiv.org/abs/2105.10990)<br><br>

[90] High-level camera-LiDAR fusion for 3D object detection with machine learning(使用机器学习进行 3D 物体检测的高级相机-LiDAR 融合)<br>
[paper](https://arxiv.org/abs/2105.11060)<br><br>

[89] Real-time Monocular Depth Estimation with Sparse Supervision on Mobile(移动端稀疏监督下的实时单目深度估计)<br>
[paper](https://arxiv.org/abs/2105.12053)<br><br>

[88] RSCA: Real-time Segmentation-based Context-Aware Scene Text Detection(RSCA：基于实时分割的上下文感知场景文本检测)<br>
[paper](https://arxiv.org/abs/2105.12789)<br><br>

[87] Automatic Non-Linear Video Editing Transfer(自动非线性视频编辑传输)<br>
[paper](https://arxiv.org/abs/2105.06988)<br>

[86] City-Scale Multi-Camera Vehicle Tracking Guided by Crossroad Zones(十字路口区域引导的城市规模多摄像机车辆跟踪)<br>
[paper](https://arxiv.org/abs/2105.06623) | [code](https://github.com/LCFractal/AIC21-MTMC)<br><br>

[85] High-Resolution Complex Scene Synthesis with Transformers(Transformer的高分辨率复杂场景综合)<br>
[paper](https://arxiv.org/abs/2105.06458)<br><br>

[84] Deep Graphics Encoder for Real-Time Video Makeup Synthesis from Example(示例中的用于实时视频合成的Deep Graphics编码器)<br>
[paper](https://arxiv.org/abs/2105.06407)<br><br>

[83] Learning to Generate Novel Scene Compositions from Single Images and Videos(学习从单个图像和视频生成新颖的场景构图)<br>
[paper](https://arxiv.org/abs/2105.05847)<br><br>

[82] Directional GAN: A Novel Conditioning Strategy for Generative Networks(定向GAN：面向生成网络的新型调节策略)<br>
[paper](https://arxiv.org/abs/2105.05712)<br><br>

[81] EDPN: Enhanced Deep Pyramid Network for Blurry Image Restoration(ESPN：用于模糊图像恢复的增强型深金字塔网络)<br>
[paper](https://arxiv.org/abs/2105.04872)<br><br>

[80] ChaLearn LAP Large Scale Signer Independent Isolated Sign Language Recognition Challenge: Design, Results and Future Research(ChaLearn LAP大规模签名人独立的隔离手语识别挑战：设计，结果和未来研究)<br>
[paper](https://arxiv.org/abs/2105.05066)<br><br>

[79] Rethinking of Radar's Role: A Camera-Radar Dataset and Systematic Annotator via Coordinate Alignment(重新考虑雷达的作用：通过坐标对齐的摄像机-雷达数据集和系统注释器)<br>
[paper](https://arxiv.org/abs/2105.05207)<br><br>

[78] Good Practices and A Strong Baseline for Traffic Anomaly Detection(【AI CITY第一名】良好做法和强大的交通异常检测基准)<br>
[paper](https://arxiv.org/abs/2105.03827) | [code](https://github.com/Endeavour10020/AICity2021-Anomaly-Detection)<br>

[77] Dynamic-OFA: Runtime DNN Architecture Switching for Performance Scaling on Heterogeneous Embedded Platforms(Dynamic-OFA：用于在异构嵌入式平台上进行性能扩展的运行时DNN架构切换)<br>
[paper](https://arxiv.org/abs/2105.03596)<br><br>

[76] The iWildCam 2021 Competition Dataset(iWildCam 2021竞赛数据集)<br>
[paper](https://arxiv.org/abs/2105.03494)<br><br>

[75] Pareto-Optimal Quantized ResNet Is Mostly 4-bit(帕累托最优量化ResNet主要为4位)<br>
[paper](https://arxiv.org/abs/2105.03536) | [code](https://github.com/google-research/google-research/tree/master/aqt)<br><br>

[74] Neural 3D Scene Compression via Model Compression(通过模型压缩进行神经3D场景压缩)<br>
[paper](https://arxiv.org/abs/2105.03120)<br><br>

[73] BasisNet: Two-stage Model Synthesis for Efficient Inference(BasisNet：有效推理的两阶段模型综合)<br>
[paper](https://arxiv.org/abs/2105.03014)<br><br>

[72] Effectively Leveraging Attributes for Visual Similarity(有效地利用属性实现视觉相似性)<br>
[paper](https://arxiv.org/abs/2105.01695)<br><br>

[71] Physically Inspired Dense Fusion Networks for Relighting(灵感来自于物理的密集融合网络)<br>
[paper](https://arxiv.org/abs/2105.02209)<br><br>

[70] Feedback control of event cameras(事件摄像机的反馈控制)<br>
[paper](https://arxiv.org/abs/2105.00409)<br><br>

[69] EQFace: A Simple Explicit Quality Network for Face Recognition(EQFace：用于面部识别的简单显式质量网络)<br>
[paper](https://arxiv.org/abs/2105.00634) | [code](https://github.com/deepcam-cn/facequality)<br><br>

[68] S3Net: A Single Stream Structure for Depth Guided Image Relighting(S3Net：用于深度引导图像重新照明的单一流结构)<br>
[paper](https://arxiv.org/abs/2105.00681)<br><br>

[67] Multi-modal Bifurcated Network for Depth Guided Image Relighting(用于深度引导图像重新照明的多模式分叉网络)<br>
[paper](https://arxiv.org/abs/2105.00690)<br><br>

[66] Renofeation: A Simple Transfer Learning Method for Improved Adversarial Robustness(Renofeation：一种简单的转移学习方法，以提高对抗性的鲁棒性)<br>
[paper](https://arxiv.org/abs/2002.02998)<br><br>

[65] D-LEMA: Deep Learning Ensembles from Multiple Annotations -- Application to Skin Lesion Segmentation(D-LEMA：来自多种注释的深度学习集合-在皮肤病变分割中的应用)<br>
[paper](https://arxiv.org/abs/2012.07206)<br><br>

[64] Pseudo-IoU: Improving Label Assignment in Anchor-Free Object Detection(伪IoU：改进无锚对象检测中的标签分配)<br>
[paper](https://arxiv.org/abs/2104.14082) | [code](https://github.com/SHI-Labs/Pseudo-IoU-for-Anchor-Free-Object-Detection)<br><br>

[63] Cluster-driven Graph Federated Learning over Multiple Domains(跨域的集群驱动图联合学习)<br>
[paper](https://arxiv.org/abs/2104.14628)<br><br>

[62] Perceptual Image Quality Assessment with Transformers(Transformer的感知图像质量评估)<br>
[paper](https://arxiv.org/abs/2104.14730) | [code](https://github.com/manricheon/IQT)<br><br>

[61] CASSOD-Net: Cascaded and Separable Structures of Dilated Convolution for Embedded Vision Systems and Applications(CASSOD-Net：扩展的卷积的层叠和可分离结构，用于嵌入式视觉系统和应用)<br>
[paper](https://arxiv.org/abs/2104.14126)<br><br>

[60] NTIRE 2021 Challenge on Video Super-Resolution(NTIRE 2021视频超分辨率挑战)<br>
[paper](https://arxiv.org/abs/2104.14852)<br><br>

[59] NTIRE 2021 Challenge on Image Deblurring(NTIRE 2021图像去模糊挑战)<br>
[paper](https://arxiv.org/abs/2104.14854)<br><br>

[58] Differentiable Event Stream Simulator for Non-Rigid 3D Tracking(用于非刚性3D跟踪的可区分事件流模拟器)<br>
[paper](https://arxiv.org/abs/2104.15139) ｜ [code](http://gvv.mpi-inf.mpg.de/projects/Event-based_Non-rigid_3D_Tracking)

[57] Sign Segmentation with Changepoint-Modulated Pseudo-Labelling(具有Changepoint调制伪标签的符号分割)<br>
[paper](https://arxiv.org/abs/2104.13817)<br><br>

[56] Boosting Co-teaching with Compression Regularization for Label Noise(通过压缩正则化促进共教学以消除标签噪声)<br>
[paper](https://arxiv.org/abs/2104.13766) | [project](https://github.com/yingyichen-cyy/Nested-Co-teaching)<br><br>

[55] Towards Fair Federated Learning with Zero-Shot Data Augmentation(【数据增广】借助零散散数据增强实现公平的联合学习)<br>
[paper](https://arxiv.org/abs/2104.13417)<br><br>

[54] Unsupervised Detection of Cancerous Regions in Histology Imagery using Image-to-Image Translation(【图像翻译】【医学影像】使用图像到图像翻译的组织学图像中癌区域的无监督检测)<br>
[paper](https://arxiv.org/abs/2104.13786)<br><br>

[53] Width Transfer: On the (In)variance of Width Optimization(宽度传递：关于宽度优化的（输入）方差)<br>
[paper](https://arxiv.org/abs/2104.13255)<br><br>

[52] Three-stream network for enriched Action Recognition(【动作识别】三流网络，用于丰富动作识别)<br>
[paper](https://arxiv.org/abs/2104.13051)<br><br>

[51] Detecting and Matching Related Objects with One Proposal Multiple Predictions(用一个多预测提案检测和匹配相关对象)<br>
[paper](https://arxiv.org/abs/2104.12574) | [code](https://github.com/foreverYoungGitHub/detect-and-match-related-objects)<br><br>

[50] The 5th AI City Challenge(第五届AI城市挑战赛)<br>
[paper](https://arxiv.org/abs/2104.12233)<br><br>

[49] Do All MobileNets Quantize Poorly? Gaining Insights into the Effect of Quantization on Depthwise Separable Convolutional Networks Through the Eyes of Multi-scale Distributional Dynamics(【模型压缩】【移动端】所有MobileNets量化效果不佳吗？ 通过多尺度分布动力学了解量化对深度可分离卷积网络的影响)<br>
[paper](https://arxiv.org/abs/2104.11849)<br><br>

[48] Multi-Scale Hourglass Hierarchical Fusion Network for Single Image Deraining(【图像去雨】用于单图像去雨的多尺度沙漏分层融合网络)<br>
[paper](https://arxiv.org/abs/2104.12100) | [code](https://github.com/cxtalk/MH2F-Net)<br><br>

[47] Class-Incremental Experience Replay for Continual Learning under Concept Drift(【增量学习】在概念漂移下继续学习的班级增量体验重播)<br>
[paper](https://arxiv.org/abs/2104.11861)<br><br>

[46] SBNet: Segmentation-based Network for Natural Language-based Vehicle Search(SBNet：基于分段的自然语言车辆搜索网络)<br>
[paper](https://arxiv.org/abs/2104.11589)<br><br>

[45] Patch Shortcuts: Interpretable Proxy Models Efficiently Find Black-Box Vulnerabilities(补丁快捷方式：可解释的代理模型有效地发现黑盒漏洞)<br>
[paper](https://arxiv.org/abs/2104.11691)<br><br>
 
[44] Region-Adaptive Deformable Network for Image Quality Assessment(用于图像质量评价的区域自适应变形网络)<br>
[paper](https://arxiv.org/abs/2104.11599) | [code](https://github.com/IIGROUP/RADN)<br><br>

[43] Network Space Search for Pareto-Efficient Spaces(【深度学习训练】Pareto有效空间的网络空间搜索)<br>
[paper](https://arxiv.org/abs/2104.11014)<br><br>

[42] A Strong Baseline for Vehicle Re-Identification(【AI城市】车辆重新识别的强大基线)<br>
[paper](https://arxiv.org/abs/2104.10850) | [code](https://github.com/cybercore-co-ltd/track2_aicity_2021)<br><br>

[41] Multi-task Learning with Attention for End-to-end Autonomous Driving(【自动驾驶】端到端自主驾驶的多任务注意力学习)<br>
[paper](https://arxiv.org/abs/2104.10753)<br><br>

[40] Perceptual Loss for Robust Unsupervised Homography Estimation(鲁棒无监督单应估计的感知损失)<br>
[paper](https://arxiv.org/abs/2104.10011)<br><br>

[39] Table Tennis Stroke Recognition Using Two-Dimensional Human Pose Estimation(【人体姿态估计】基于二维人体姿态估计的乒乓球笔划识别)<br>
[paper](https://arxiv.org/abs/2104.09907)<br><br>

[38] Comparing Representations in Tracking for Event Camera-based SLAM(基于SLAM的事件摄像机跟踪中的比较表示)<br>
[paper](https://arxiv.org/abs/2104.09887)<br><br>

[37] Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry(【遥感图像】多视点卫星摄影测量中的阴影神经辐射场)<br>
[paper](https://arxiv.org/abs/2104.09877)<br><br>

[36] Distill on the Go: Online knowledge distillation in self-supervised learning(【知识蒸馏】在线提取：自我监督学习中的在线知识提取)<br>
[paper](https://arxiv.org/abs/2104.09866)<br><br>

[35] An Efficient Approach for Anomaly Detection in Traffic Videos(【视频异常检测】一种有效的交通视频异常检测方法)<br>
[paper](https://arxiv.org/abs/2104.09758)<br><br>

[34] Class-Incremental Learning with Generative Classifiers(【增量学习】基于生成分类器的课堂增量学习)<br>
[paper](https://arxiv.org/abs/2104.10093)<br><br>

[33] Engineering Sketch Generation for Computer-Aided Design(面向计算机辅助设计的工程草图生成)<br>
[paper](https://arxiv.org/abs/2104.09621)<br><br>

[32] IB-DRR: Incremental Learning with Information-Back Discrete Representation Replay(【增量学习】IB-DRR：基于信息反馈的增量学习)<br>
[paper](https://arxiv.org/abs/2104.10588)<br><br>

[31] Revisiting The Evaluation of Class Activation Mapping for Explainability: A Novel Metric and Experimental Analysis(【可解释性】重新审视类激活映射的可解释性评价：一个新的度量和实验分析)<br>
[paper](https://arxiv.org/abs/2104.10252)<br><br>

[30] GAN-Based Data Augmentation and Anonymization for Skin-Lesion Analysis: A Critical Review(基于GAN的皮肤损伤分析的数据增强和匿名化：一项重要的评论)<br>
[paper](https://arxiv.org/abs/2104.10603)<br><br>

[29] Brittle Features May Help Anomaly Detection(脆弱的功能可能有助于异常检测)<br>
[paper](https://arxiv.org/abs/2104.10453)<br><br>

[28] I Only Have Eyes for You: The Impact of Masks On Convolutional-Based Facial Expression Recognition(我只有你的眼睛：口罩对基于卷积的面部表情识别的影响)<br>
[paper](https://arxiv.org/abs/2104.08353)<br><br>

[27] Assessment of deep learning based blood pressure prediction from PPG and rPPG signals(从PPG和rPPG信号评估基于深度学习的血压预测)<br>
[paper](https://arxiv.org/abs/2104.09313)<br><br>

[26] A Two-branch Neural Network for Non-homogeneous Dehazing via Ensemble Learning(通过集合学习进行非均匀去雾的两分支神经网络)<br>
[paper](https://arxiv.org/abs/2104.08902)<br><br>

[25] End-to-End Interactive Prediction and Planning with Optical Flow Distillation for Autonomous Driving(用于自动驾驶的带有光流蒸馏的端到端交互式预测和计划)<br>
[paper](https://arxiv.org/abs/2104.08862) | [project](http://sites.google.com/view/inmp-ofd)<br><br>

[24] Reconsidering CO2 emissions from Computer Vision(考虑计算机视觉产生的二氧化碳排放量)<br>
[paper](https://arxiv.org/abs/2104.08702)<br><br>

[23] On Training Sketch Recognizers for New Domains(关于新领域的训练草图识别器)<br>
[paper](https://arxiv.org/abs/2104.08850)<br><br>

[22] Filtering Empty Camera Trap Images in Embedded Systems(过滤嵌入式系统中的空相机陷阱图像)<br>
[paper](https://arxiv.org/abs/2104.08859)<br><br>

[21] Contrastive Learning Improves Model Robustness Under Label Noise(对比学习提高了标签噪声下的模型鲁棒性)<br>
[paper](https://arxiv.org/abs/2104.08984) | [code](https://github.com/arghosh/noisy_label_pretrain)<br><br>

[20] Restoration of Video Frames from a Single Blurred Image with Motion Understanding(通过运动理解从单个模糊图像恢复视频帧)<br>
[paper](https://arxiv.org/abs/2104.09134)<br><br>

[19] LSPnet: A 2D Localization-oriented Spacecraft Pose Estimation Neural Network(LSPnet：面向二维本地化的航天器姿态估计神经网络)<br>
[paper](https://arxiv.org/abs/2104.09248)<br><br>

[18] Plants Don't Walk on the Street: Common-Sense Reasoning for Reliable Semantic Segmentation(植物不在大街上行走：可靠语义分割的常识推理)<br>
[paper](https://arxiv.org/abs/2104.09254)<br><br>

[17] Temporal Consistency Loss for High Resolution Textured and Clothed 3DHuman Reconstruction from Monocular Video(从单眼视频的高分辨率纹理化和布料化3D人体重建的时间一致性损失)<br>
[paper](https://arxiv.org/abs/2104.09259)<br><br>

[16] A Mathematical Analysis of Learning Loss for Active Learning in Regression(回归中主动学习的学习损失的数学分析)<br>
[paper](https://arxiv.org/abs/2104.09315)<br><br>

[15] Camera Calibration and Player Localization in SoccerNet-v2 and Investigation of their Representations for Action Spotting(SoccerNet-v2中的摄像机校准和球员本地化以及用于动作识别的研究)<br>
[paper](https://arxiv.org/abs/2104.09333)<br><br>

[14] DANICE: Domain adaptation without forgetting in neural image compression(DANICE：在不忘记神经图像压缩的情况下进行域自适应)<br>
[paper](https://arxiv.org/abs/2104.09370)

[13] OmniLayout: Room Layout Reconstruction from Indoor Spherical Panoramas(OmniLayout：从室内球形全景图进行房间布局重建)<br>
[paper](https://arxiv.org/abs/2104.09403) | [code](https://github.com/rshivansh/OmniLayout)<br><br>

[12] Dual Contrastive Learning for Unsupervised Image-to-Image Translation(【图像翻译】双重对比学习，实现无监督的图像到图像翻译)<br>
[paper](https://arxiv.org/abs/2104.07689) | [code](https://github.com/JunlinHan/DCLGAN)<br><br>

[11] OmniFlow: Human Omnidirectional Optical Flow(OmniFlow：人类全向光流)<br>
[paper](https://arxiv.org/abs/2104.07960)<br><br>

[10] I Find Your Lack of Uncertainty in Computer Vision Disturbing(在计算机视觉干扰方面缺乏不确定性)<br>
[paper](https://arxiv.org/abs/2104.08188)<br><br>

[9] Fast Walsh-Hadamard Transform and Smooth-Thresholding Based Binary Layers in Deep Neural Networks(神经网络中快速Walsh-Hadamard变换和基于平滑阈值的二进制层)<br>
[paper](https://arxiv.org/abs/2104.07085)<br><br>

[8] Machine-learned 3D Building Vectorization from Satellite Imagery(【遥感图像】通过卫星图像进行机器学习的3D建筑矢量化)<br>
[paper](https://arxiv.org/abs/2104.06485)<br><br>

[7] Graph-based Person Signature for Person Re-Identifications(【行人重识别】用于行人重识别的基于图的人员签名)<br>
[paper](https://arxiv.org/abs/2104.06770)<br><br>

[6] Temporally-Aware Feature Pooling for Action Spotting in Soccer Broadcasts(【动作识别】用于足球广播中动作识别的临时感知功能池)<br>
[paper](https://arxiv.org/abs/2104.06779)<br><br>

[5] Continual learning in cross-modal retrieval(【持续学习】跨模式检索中的持续学习)<br>
[paper](https://arxiv.org/abs/2104.06806)<br><br>

[4] Towards Automated and Marker-less Parkinson Disease Assessment: Predicting UPDRS Scores using Sit-stand videos(迈向自动无标记帕金森病评估：使用站立式视频预测UPDRS得分)<br>
[paper](https://arxiv.org/abs/2104.04650)<br><br>

[3] Efficient Space-time Video Super Resolution using Low-Resolution Flow and Mask Upsampling(【图像超分】使用低分辨率流和遮罩上采样的高效时空视频超分辨率)<br>
[paper](https://arxiv.org/abs/2104.05778) | [project](https://github.com/saikatdutta/FMU_STSR)<br><br>

[2] Generalizable Multi-Camera 3D Pedestrian Detection(【行人检测】通用多摄像机3D行人检测)<br>
[paper](https://arxiv.org/abs/2104.05813)<br><br>

[1] Dealing with Missing Modalities in the Visual Question Answer-Difference Prediction Task through Knowledge Distillation(【知识蒸馏】【视觉问答】通过知识蒸馏处理视觉问题回答差异预测任务中的缺失模态)<br>
[paper](https://arxiv.org/abs/2104.05965)<br><br>

<br>

<a name="5"/> 

# 5. To do list

* CVPR2021论文分享
